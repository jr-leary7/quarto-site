---
title: "IN PROGRESS - Interpreting Intercepts in Linear Regression"
author:
  name: Jack Leary
  email: j.leary@ufl.edu
  affiliations:
    - name: University of Florida
      department: Biostatistics 
      city: Gainesville
      state: FL
date: "`r Sys.Date()`"
format:
  html:
    code-fold: show
    code-copy: true
    code-tools: true
    toc: true
    self-contained: true
    fig-format: retina
    df-print: kable
    link-external-newwindow: true
execute: 
  cache: true
  freeze: auto
---

# Introduction 

One concept I struggled with a lot in early statistics courses was what the intercept meant in linear regression models. I tended to just ignore it unless questions specifically pertained to it, and the vast majority of homework questions focused on interpreting the effects of covariates instead. I saw many of my master's-level students struggle with in the SAS computing course I taught during Fall 2022 as well, with confusion about the effect of centering, the difference between centering and standardizing, and intercept interpretation in mixed models being common pain points on homeworks. As such, I thought it might be useful - for myself and others - to jot down some notes on how the intercept is estimated and what it means under a variety of regression modelling frameworks. 

# Matrix Algebra Review 

We're going to start from first principles here with a quick review on matrix algebra. Linear regression is, after, just multiplying matrices in a clever way. 

## Multiplication 

### Theory 

First define two matrices $A$ and $B$, each with 2 rows and 2 columns:

$$
\begin{aligned}

\mathbf{A} &= 
  \begin{bmatrix} 
    a_{11} & a_{21} \\
    a_{12} & a_{22} \\
  \end{bmatrix} \\
  
& \\
  
\mathbf{B} &= 
  \begin{bmatrix} 
    b_{11} & b_{21} \\
    b_{12} & b_{22} \\
  \end{bmatrix}

\end{aligned}
$$

Their product, another matrix $C$, also has 2 rows and 2 columns, and its elements are defined like so, with $i$ specifying the row and $j$ the column of each element. What we're doing is finding the dot product of the $i^{\text{th}}$ row of $\mathbf{A}$ and the $i^{\text{th}}$ column of $\mathbf{B}$, the expanded definition of which is below. 

$$
\begin{aligned}

c_{i,j} &= \mathbf{A}_{i*} \cdot \mathbf{B}_{*j} \\
c_{i,j} &= \sum_{k=1}^n a_{ik}b_{kj} \\
c_{i,j} &= a_{i1}b_{1j} + \dots + a_{n1}b_{nj} \\

\end{aligned}
$$

As such, we can define the product of $\mathbf{A}$ and $\mathbf{B}$ like so:

$$
\begin{aligned}

\mathbf{C} &=  \mathbf{A} \mathbf{B} \\

& \\

\mathbf{C} &= 
  \begin{bmatrix} 
    \mathbf{A}_{1*} \cdot \mathbf{B}_{*1} & \mathbf{A}_{2*} \cdot \mathbf{B}_{*1} \\
    \mathbf{A}_{2*} \cdot \mathbf{B}_{*1} & \mathbf{A}_{2*} \cdot \mathbf{B}_{*2} \\
  \end{bmatrix} \\

& \\

\mathbf{C} &= 
  \begin{bmatrix} 
    a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
    a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\
  \end{bmatrix} \\

\end{aligned}
$$

**Important Note**: To multiply two matrices $\mathbf{A}$ and $\mathbf{B}$ together, the number of rows of $\mathbf{B}$ must be equal to the number of columns in $\mathbf{A}$. To generalize: 

$$
\mathbf{A}_{mn} \cdot \mathbf{B}_{np} = \mathbf{C}_{mp}
$$

### Example 

Let's define two matrices:

$$
\begin{aligned}

\mathbf{A} &= 
  \begin{bmatrix} 
    3 & 2 \\
    0 & 7 \\
  \end{bmatrix} \\
  
& \\
  
\mathbf{B} &= 
  \begin{bmatrix} 
    1 & 4 \\
    1 & 2 \\
  \end{bmatrix}

\end{aligned}
$$

Their product $\mathbf{C}$ is defined as:

$$
\begin{aligned}

\mathbf{C} &= 
  \begin{bmatrix} 
    3 \times 1 + 2 \times 1 & 3 \times 4 + 2 \times 2 \\
    0 \times 1 + 7 \times 1 & 0 \times 4 + 7 \times 2 \\
  \end{bmatrix} \\

& \\

\mathbf{C} &= 
  \begin{bmatrix} 
    5 & 16 \\
    7 & 14 \\
  \end{bmatrix} \\

\end{aligned}
$$

We can check this using R:

```{r}
A_mat <- matrix(c(3, 2, 0, 7), 
                nrow = 2, 
                ncol = 2, 
                byrow = TRUE)
B_mat <- matrix(c(1, 4, 1, 2), 
                nrow = 2, 
                ncol = 2, 
                byrow = TRUE)
C_mat <- A_mat %*% B_mat
C_mat
```

## Transposition 

### Theory 

### Example 

## Inversion 

### Theory 

### Example 


## The Identity Matrix 

### Theory 

The identity matrix $\mathbf{I}_{n}$ is a square matrix composed entirely of zeroes *except* along the diagonal, which is composed of ones. This matrix carries some unique properties (which are listed below) that will be helpful to us later on.  

$$
\begin{aligned}

\mathbf{I}_{n} &= 
  \begin{bmatrix} 
    1 & 0 & \cdots & 0 \\ 
    0 & 1 & \cdots & 0 \\ 
    \vdots & \vdots & \ddots & 0 \\ 
    0 & 0 & 0 & 1 \\
  \end{bmatrix}
  
& \\

\mathbf{I}_{n}^\prime &= \mathbf{I}_{n} \\

& \\

\mathbf{I}_{n}^{-1} &= \mathbf{I}_{n} \\

\end{aligned}
$$

### Example 

We can set up a $3 \times 3$ identity matrix $\mathbf{I}_{3}$ in R using the `diag()` function:

```{r}
ident_mat <- diag(nrow = 3)
ident_mat
```

The transpose is also equal to $\mathbf{I}_{3}$:

```{r}
t(ident_mat)
```

As is the inverse: 

```{r}
solve(ident_mat)
```

# Linear Regression 

## Setup 

For now, we'll take it for granted that the solution to a linear regression problem is defined as follows:

$$
\widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^\prime \mathbf{X} \right)^{-1} \mathbf{X}^\prime \mathbf{y}
$$

## The Intercept-only Model 

The intercept-only model (also sometimes called the null model) is defined as linear regression when $\mathbf{X}$ is simply a column vector of ones:

$$
\mathbf{X} = 
  \begin{bmatrix} 
    1 \\
    \vdots \\
    1 \\
  \end{bmatrix}
$$


# References 



# Session Info 

```{r}
sessioninfo::session_info()
```
