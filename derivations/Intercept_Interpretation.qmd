---
title: "IN PROGRESS - Interpreting Intercepts in Linear Regression"
author:
  name: Jack Leary
  email: j.leary@ufl.edu
  affiliations:
    - name: University of Florida
      department: Biostatistics 
      city: Gainesville
      state: FL
date: "`r Sys.Date()`"
format:
  html:
    code-fold: show
    code-copy: true
    code-tools: true
    toc: true
    self-contained: true
    fig-format: retina
    df-print: kable
    link-external-newwindow: true
execute: 
  cache: true
  freeze: auto
---

# Introduction 

One concept I struggled with a lot in early statistics courses was what the intercept meant in linear regression models. I tended to just ignore it unless questions specifically pertained to it, and the vast majority of homework questions focused on interpreting the effects of covariates instead. I saw many of my master's-level students struggle with in the SAS computing course I taught during Fall 2022 as well, with confusion about the effect of centering, the difference between centering and standardizing, and intercept interpretation in the different types of (generalized) linear (mixed) models being common pain points on homeworks. As such, I thought it might be useful - for myself and others - to jot down some notes on how the intercept is estimated and what it means under a variety of regression modelling frameworks. 

# Matrix Algebra Review 

We're going to start from first principles here with a quick review on matrix algebra. Linear regression is, after, just multiplying matrices in a clever way. 

## Multiplication 

### Theory 

First define two matrices $\mathbf{A}$ and $\mathbf{B}$, each with 2 rows and 2 columns:

$$
\begin{aligned}
\mathbf{A} &= 
    \begin{bmatrix} 
      a_{11} & a_{21} \\
      a_{12} & a_{22} \\
    \end{bmatrix} \\
\mathbf{B} &= 
  \begin{bmatrix} 
    b_{11} & b_{21} \\
    b_{12} & b_{22} \\
  \end{bmatrix} \\
\end{aligned}
$$

Their product, another matrix $C$, also has 2 rows and 2 columns, and its elements are defined like so, with $i$ specifying the row and $j$ the column of each element. What we're doing is finding the dot product of the $i^{\text{th}}$ row of $\mathbf{A}$ and the $j^{\text{th}}$ column of $\mathbf{B}$, the expanded definition of which is below. 

$$
\begin{aligned}
c_{ij} &= \mathbf{A}_{i*} \cdot \mathbf{B}_{*j} \\
c_{ij} &= \sum_{k=1}^n a_{ik}b_{kj} \\
c_{ij} &= a_{i1}b_{1j} + \dots + a_{n1}b_{nj} \\
\end{aligned}
$$

As such, we can define the product of $\mathbf{A}$ and $\mathbf{B}$ like so:

$$
\begin{aligned}
\mathbf{C} &=  \mathbf{A} \mathbf{B} \\
\mathbf{C} &= 
  \begin{bmatrix} 
    \mathbf{A}_{1*} \cdot \mathbf{B}_{*1} & \mathbf{A}_{2*} \cdot \mathbf{B}_{*1} \\
    \mathbf{A}_{2*} \cdot \mathbf{B}_{*1} & \mathbf{A}_{2*} \cdot \mathbf{B}_{*2} \\
  \end{bmatrix} \\
\mathbf{C} &= 
  \begin{bmatrix} 
    a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
    a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\
  \end{bmatrix} \\
\end{aligned}
$$

**Important Note**: To multiply two matrices $\mathbf{A}$ and $\mathbf{B}$ together, the number of rows of $\mathbf{B}$ must be equal to the number of columns in $\mathbf{A}$. To generalize: 

$$
\mathbf{A}_{mn} \cdot \mathbf{B}_{np} = \mathbf{C}_{mp}
$$

### Example 

Let's define two matrices:

$$
\begin{aligned}
\mathbf{A} &= 
  \begin{bmatrix} 
    3 & 2 \\
    0 & 7 \\
  \end{bmatrix} \\
\mathbf{B} &= 
  \begin{bmatrix} 
    1 & 4 \\
    1 & 2 \\
  \end{bmatrix} \\
\end{aligned}
$$

Their product $\mathbf{C}$ is defined as:

$$
\begin{aligned}
\mathbf{C} &= 
  \begin{bmatrix} 
    3 \times 1 + 2 \times 1 & 3 \times 4 + 2 \times 2 \\
    0 \times 1 + 7 \times 1 & 0 \times 4 + 7 \times 2 \\
  \end{bmatrix} \\
\mathbf{C} &= 
  \begin{bmatrix} 
    5 & 16 \\
    7 & 14 \\
  \end{bmatrix} \\
\end{aligned}
$$

We can check this using R:

```{r}
A_mat <- matrix(c(3, 2, 0, 7), 
                nrow = 2, 
                ncol = 2, 
                byrow = TRUE)
B_mat <- matrix(c(1, 4, 1, 2), 
                nrow = 2, 
                ncol = 2, 
                byrow = TRUE)
C_mat <- A_mat %*% B_mat
C_mat
```

## Transposition 

### Theory 

### Example 

## Inversion 

### Theory 

### Example 


## The Identity Matrix 

### Theory 

The identity matrix $\mathbf{I}_{n}$ is a square matrix composed entirely of zeroes *except* along the diagonal, which is composed of ones. This matrix carries some unique properties (which are listed below) that will be helpful to us later on.  

$$
\begin{aligned}
\mathbf{I}_{n} &= 
  \begin{bmatrix} 
    1 & 0 & \cdots & 0 \\ 
    0 & 1 & \cdots & 0 \\ 
    \vdots & \vdots & \ddots & 0 \\ 
    0 & 0 & 0 & 1 \\
  \end{bmatrix} \\
\mathbf{I}_{n}^\prime &= \mathbf{I}_{n} \\
\mathbf{I}_{n}^{-1} &= \mathbf{I}_{n} \\
\end{aligned}
$$

### Example 

We can set up a $3 \times 3$ identity matrix $\mathbf{I}_{3}$ in R using the `diag()` function:

```{r}
ident_mat <- diag(nrow = 3)
ident_mat
```

The transpose is also equal to $\mathbf{I}_{3}$:

```{r}
t(ident_mat)
```

As is the inverse: 

```{r}
solve(ident_mat)
```

# Linear Model 

## Setup 

For now, we'll take it for granted that the solution to a linear regression problem is defined as follows:

$$
\widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^\prime \mathbf{X} \right)^{-1} \mathbf{X}^\prime \mathbf{y}
$$

## The Intercept-only Model 

The intercept-only model (also sometimes called the null model) is defined as linear regression when $\mathbf{X}$ is simply a column vector of ones:

$$
\mathbf{X} = 
  \begin{bmatrix} 
    1 \\
    \vdots \\
    1 \\
  \end{bmatrix}
$$

We know the intercept-only model produces the mean as the one predicted value, as the mean minimizes the sum of squared errors in the absence of any other covariates. We can check this using R - we'll first generate a vector $\mathbf{y}$ consisting of 5 realizations of a random variable, such that $\mathbf{Y} \sim \mathcal{N}(0, 3)$. 

```{r}
y <- rnorm(5, mean = 0, sd = 3)
y <- matrix(y, ncol = 1)
y
```

The mean of $\mathbf{y}$ is:

```{r}
mean(y)
```

We can use R to fit an intercept-only model. We can see that the intercept coefficient $\beta_0$ is equal to the mean of $\mathbf{y}$. 

```{r}
null_mod <- lm(y ~ 1)
coef(null_mod)
```

Let's use linear algebra to figure out why this is true. Once again, we know that the linear regression closed-form solution is given by the following: 

$$
\widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^\prime \mathbf{X} \right)^{-1} \mathbf{X}^\prime \mathbf{y}
$$

Let's first define $\mathbf{X}$:

```{r}
X <- c(1, 1, 1, 1, 1)
X <- matrix(X, ncol = 1)
X
```

The value of $\mathbf{X}^\prime \mathbf{X}$ is given by the following - note that this is equal to our sample size $n = 5$. 

```{r}
t(X) %*% X
```

The inverse of which, $\left(\mathbf{X}^\prime \mathbf{X} \right)^{-1}$, is of course $n^{-1}$:

```{r}
solve(t(X) %*% X)
```

We multiply the above by $\mathbf{X}^\prime$ again to obtain $\left(\mathbf{X}^\prime \mathbf{X} \right)^{-1} \mathbf{X}^\prime$, which gives us a constant vector of length $n$ with all values being equal to $n^{-1}$:

```{r}
solve(t(X) %*% X) %*% t(X)
```

Lastly, we multiply the above by $\mathbf{y}$. Remember how multiplying vectors works - in this case we are multiplying each element of the above vector $\left(\mathbf{X}^\prime \mathbf{X} \right)^{-1} \mathbf{X}^\prime$ with each element of $\mathbf{y}$ and adding them together. We'll define $\mathbf{Z} = \left(\mathbf{X}^\prime \mathbf{X} \right)^{-1} \mathbf{X}^\prime$ for convenience of notation: 

$$
\mathbf{Z} \mathbf{y} = \sum_{i=1}^n \mathbf{Z}_i \mathbf{y}_i
$$

Since each element of $\mathbf{Z}$ is the same, $n^{-1}$, the above quantity is equivalent to:

$$
\begin{aligned}
  \mathbf{Z} \mathbf{y} &= \left(\mathbf{X}^\prime \mathbf{X} \right)^{-1} \mathbf{X}^\prime \mathbf{y} \\
  \mathbf{Z} \mathbf{y} &= \sum_{i=1}^n \mathbf{Z}_i \mathbf{y}_i \\
  \mathbf{Z} \mathbf{y} &= n^{-1} \sum_{i=1}^n \mathbf{y}_i \\
\end{aligned}
$$

This is simply the sum of all the element of $\mathbf{y}$ divided by $n$ - the mean! We can verify this with R by using linear algebra to compute the OLS solution:

```{r}
solve(t(X) %*% X) %*% t(X) %*% y
```

This is equal to simply taking the mean of $\mathbf{y}$: 

```{r}
mean(y)
```

# Generalized Linear Model

We'll next move to the more complicated case of the generalized linear model. 

# Linear Mixed Model

# Generalized Linear Mixed Model

# References 

1. Faraway, J. [Extending the Linear Model with R, 2nd Edition](https://doi.org/10.1201/9781315382722). *Chapman and Hall*. 2016. 

2. Hastie, T. *et al*. [The Elements of Statistical Learning, 2nd Edition](https://doi.org/10.1007/b94608). *Springer*. 2009. 

# Session Info 

```{r}
sessioninfo::session_info()
```
