[
  {
    "objectID": "derivations/Intercept_Interpretation.html#multiplication",
    "href": "derivations/Intercept_Interpretation.html#multiplication",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Multiplication",
    "text": "Multiplication\n\nTheory\nFirst define two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), each with 2 rows and 2 columns:\n\\[\n\\begin{aligned}\n\\mathbf{A} &=\n    \\begin{bmatrix}\n      a_{11} & a_{21} \\\\\n      a_{12} & a_{22} \\\\\n    \\end{bmatrix} \\\\\n\\mathbf{B} &=\n  \\begin{bmatrix}\n    b_{11} & b_{21} \\\\\n    b_{12} & b_{22} \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nTheir product, another matrix \\(C\\), also has 2 rows and 2 columns, and its elements are defined like so, with \\(i\\) specifying the row and \\(j\\) the column of each element. What we’re doing is finding the dot product of the \\(i^{\\text{th}}\\) row of \\(\\mathbf{A}\\) and the \\(j^{\\text{th}}\\) column of \\(\\mathbf{B}\\), the expanded definition of which is below.\n\\[\n\\begin{aligned}\nc_{ij} &= \\mathbf{A}_{i*} \\cdot \\mathbf{B}_{*j} \\\\\nc_{ij} &= \\sum_{k=1}^n a_{ik}b_{kj} \\\\\nc_{ij} &= a_{i1}b_{1j} + \\dots + a_{n1}b_{nj} \\\\\n\\end{aligned}\n\\]\nAs such, we can define the product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) like so:\n\\[\n\\begin{aligned}\n\\mathbf{C} &=  \\mathbf{A} \\mathbf{B} \\\\\n\\mathbf{C} &=\n  \\begin{bmatrix}\n    \\mathbf{A}_{1*} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*1} \\\\\n    \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*2} \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{C} &=\n  \\begin{bmatrix}\n    a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n    a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nImportant Note: To multiply two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) together, the number of rows of \\(\\mathbf{B}\\) must be equal to the number of columns in \\(\\mathbf{A}\\). To generalize:\n\\[\n\\mathbf{A}_{mn} \\cdot \\mathbf{B}_{np} = \\mathbf{C}_{mp}\n\\]\n\n\nExample\nLet’s define two matrices:\n\\[\n\\begin{aligned}\n\\mathbf{A} &=\n  \\begin{bmatrix}\n    3 & 2 \\\\\n    0 & 7 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{B} &=\n  \\begin{bmatrix}\n    1 & 4 \\\\\n    1 & 2 \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nTheir product \\(\\mathbf{C}\\) is defined as:\n\\[\n\\begin{aligned}\n\\mathbf{C} &=\n  \\begin{bmatrix}\n    3 \\times 1 + 2 \\times 1 & 3 \\times 4 + 2 \\times 2 \\\\\n    0 \\times 1 + 7 \\times 1 & 0 \\times 4 + 7 \\times 2 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{C} &=\n  \\begin{bmatrix}\n    5 & 16 \\\\\n    7 & 14 \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nWe can check this using R:\n\n\nCode\nA_mat <- matrix(c(3, 2, 0, 7), \n                nrow = 2, \n                ncol = 2, \n                byrow = TRUE)\nB_mat <- matrix(c(1, 4, 1, 2), \n                nrow = 2, \n                ncol = 2, \n                byrow = TRUE)\nC_mat <- A_mat %*% B_mat\nC_mat\n\n\n     [,1] [,2]\n[1,]    5   16\n[2,]    7   14"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#transposition",
    "href": "derivations/Intercept_Interpretation.html#transposition",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Transposition",
    "text": "Transposition\n\nTheory\n\n\nExample"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#inversion",
    "href": "derivations/Intercept_Interpretation.html#inversion",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Inversion",
    "text": "Inversion\n\nTheory\n\n\nExample"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#the-identity-matrix",
    "href": "derivations/Intercept_Interpretation.html#the-identity-matrix",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "The Identity Matrix",
    "text": "The Identity Matrix\n\nTheory\nThe identity matrix \\(\\mathbf{I}_{n}\\) is a square matrix composed entirely of zeroes except along the diagonal, which is composed of ones. This matrix carries some unique properties (which are listed below) that will be helpful to us later on.\n\\[\n\\begin{aligned}\n\\mathbf{I}_{n} &=\n  \\begin{bmatrix}\n    1 & 0 & \\cdots & 0 \\\\\n    0 & 1 & \\cdots & 0 \\\\\n    \\vdots & \\vdots & \\ddots & 0 \\\\\n    0 & 0 & 0 & 1 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{I}_{n}^\\prime &= \\mathbf{I}_{n} \\\\\n\\mathbf{I}_{n}^{-1} &= \\mathbf{I}_{n} \\\\\n\\end{aligned}\n\\]\n\n\nExample\nWe can set up a \\(3 \\times 3\\) identity matrix \\(\\mathbf{I}_{3}\\) in R using the diag() function:\n\n\nCode\nident_mat <- diag(nrow = 3)\nident_mat\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nThe transpose is also equal to \\(\\mathbf{I}_{3}\\):\n\n\nCode\nt(ident_mat)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nAs is the inverse:\n\n\nCode\nsolve(ident_mat)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#setup",
    "href": "derivations/Intercept_Interpretation.html#setup",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Setup",
    "text": "Setup\nFor now, we’ll take it for granted that the solution to a linear regression problem is defined as follows:\n\\[\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y}\n\\]"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#the-intercept-only-model",
    "href": "derivations/Intercept_Interpretation.html#the-intercept-only-model",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "The Intercept-only Model",
    "text": "The Intercept-only Model\nThe intercept-only model (also sometimes called the null model) is defined as linear regression when \\(\\mathbf{X}\\) is simply a column vector of ones:\n\\[\n\\mathbf{X} =\n  \\begin{bmatrix}\n    1 \\\\\n    \\vdots \\\\\n    1 \\\\\n  \\end{bmatrix}\n\\]\nWe know the intercept-only model produces the mean as the one predicted value, as the mean minimizes the sum of squared errors in the absence of any other covariates. We can check this using R - we’ll first generate a vector \\(\\mathbf{y}\\) consisting of 5 realizations of a random variable, such that \\(Y \\sim \\mathcal{N}(0, 3)\\).\n\n\nCode\ny <- rnorm(5, mean = 0, sd = 3)\ny <- matrix(y, ncol = 1)\ny\n\n\n           [,1]\n[1,]  2.4364366\n[2,]  4.5241548\n[3,]  2.7447766\n[4,] -2.2354806\n[5,] -0.2949723\n\n\nThe mean of \\(\\mathbf{y}\\) is:\n\n\nCode\nmean(y)\n\n\n[1] 1.434983\n\n\nWe can use R to fit an intercept-only model. We can see that the intercept coefficient \\(\\beta_0\\) is equal to the mean of \\(\\mathbf{y}\\).\n\n\nCode\nnull_mod <- lm(y ~ 1)\ncoef(null_mod)\n\n\n(Intercept) \n   1.434983 \n\n\nLet’s use linear algebra to figure out why this is true. Once again, we know that the linear regression closed-form solution is given by the following:\n\\[\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y}\n\\]\nLet’s first define \\(\\mathbf{X}\\) - just a column vector of 1s with \\(n = 5\\) rows:\n\n\nCode\nX <- c(1, 1, 1, 1, 1)\nX <- matrix(X, ncol = 1)\nX\n\n\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    1\n[5,]    1\n\n\nThe value of \\(\\mathbf{X}^\\prime \\mathbf{X}\\) is given by the following - note that this is equal to our sample size \\(n = 5\\). We knew that this quantity would be a scalar (a \\(1 \\times 1\\) matrix) since \\(\\mathbf{X}^\\prime\\) has 1 row and 5 columns, and \\(\\mathbf{X}\\) has 5 rows and 1 column, thus by the rule we defined above their product has 1 row and 1 column.\n\n\nCode\nt(X) %*% X\n\n\n     [,1]\n[1,]    5\n\n\nThe inverse of which, \\(\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1}\\), is of course \\(n^{-1}\\):\n\n\nCode\nsolve(t(X) %*% X)\n\n\n     [,1]\n[1,]  0.2\n\n\nWe multiply the above by \\(\\mathbf{X}^\\prime\\) again to obtain \\(\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime\\), which gives us a constant vector of length \\(n\\) with all values being equal to \\(n^{-1}\\):\n\n\nCode\nsolve(t(X) %*% X) %*% t(X)\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.2  0.2  0.2  0.2  0.2\n\n\nLastly, we multiply the above by \\(\\mathbf{y}\\). Remember how multiplying vectors works - in this case we are multiplying each element of the above vector \\(\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime\\) with each element of \\(\\mathbf{y}\\) and adding them together. We’ll define \\(\\mathbf{Z} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime\\) for convenience of notation:\n\\[\n\\mathbf{Z} \\mathbf{y} = \\sum_{i=1}^n \\mathbf{Z}_i \\mathbf{y}_i\n\\]\nSince each element of \\(\\mathbf{Z}\\) is the same, \\(n^{-1}\\), by the transitive property the above quantity is equivalent to:\n\\[\n\\begin{aligned}\n  \\mathbf{Z} \\mathbf{y} &= \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y} \\\\\n  \\mathbf{Z} \\mathbf{y} &= \\sum_{i=1}^n \\mathbf{Z}_i \\mathbf{y}_i \\\\\n  \\mathbf{Z} \\mathbf{y} &= n^{-1} \\sum_{i=1}^n \\mathbf{y}_i \\\\\n\\end{aligned}\n\\]\nThis is simply the sum of all the elements of \\(\\mathbf{y}\\) divided by \\(n\\) - the mean! We can verify this with R by using linear algebra to compute the OLS solution:\n\n\nCode\nsolve(t(X) %*% X) %*% t(X) %*% y\n\n\n         [,1]\n[1,] 1.434983\n\n\nThis is equal to simply taking the mean of \\(\\mathbf{y}\\):\n\n\nCode\nmean(y)\n\n\n[1] 1.434983"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-categorical-predictors",
    "href": "derivations/Intercept_Interpretation.html#models-with-categorical-predictors",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Categorical Predictors",
    "text": "Models with Categorical Predictors\nIn practice of course we usually build models with predictors of interest outside of the intercept. Categorical variables are composed of discrete values, each with a different meaning e.g., we could have a variable containing the type of treatment a patient has received. In order to fit models with categorical variables, it’s necessary to expand a categorical variable into multiple indicator variables - variables composed of 1s and 0s depending on whether a certain observation belongs to a certain category. This is a little confusing, so let’s show an example. We’ll start by creating a categorical variable .\n\n\nCode\nX <- sample(c(\"A\", \"B\"), size = 10, replace = TRUE)\nX <- matrix(X, ncol = 1)\nX\n\n\n      [,1]\n [1,] \"A\" \n [2,] \"A\" \n [3,] \"A\" \n [4,] \"B\" \n [5,] \"A\" \n [6,] \"A\" \n [7,] \"A\" \n [8,] \"B\" \n [9,] \"A\" \n[10,] \"A\" \n\n\nTo convert \\(\\mathbf{X}\\) into a numeric variable that we can use in a model, we use the model.matrix() function. To use this function though, we need to define the model we’re interested in using R’s formula syntax. The output we see shows an intercept column, which we understand, and another column composed of 1s and 0s called XB. This column is an indicator variable that tells us whether each observation belongs to category B or not - thus when XB is equal to 0, we know that the observation belongs to category A. This process of converting non-numeric categorical data to indicator variables has many names (one-hot encoding, dummifying, etc.), and there’s many ways of doing it. You can read more about the various ways of doing so here, but for now we’ll take it for granted that this is how it works under the hood in the lm() function that we use to fit linear models.\n\n\nCode\nX_2 <- model.matrix(~X)\nX_2\n\n\n   (Intercept) XB\n1            1  0\n2            1  0\n3            1  0\n4            1  1\n5            1  0\n6            1  0\n7            1  0\n8            1  1\n9            1  0\n10           1  0\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$X\n[1] \"contr.treatment\"\n\n\nFrom what we already know about matrix multiplication, we can see that the B group is going to have a different predicted average than the A group. We’ll also need to generate a new response variable \\(\\mathbf{y}\\), since we’ve increased our sample size to \\(n = 10\\).\n\n\nCode\ny <- rnorm(10, mean = 0, sd = 3)\ny <- matrix(y, ncol = 1)\ny\n\n\n           [,1]\n [1,]  4.356184\n [2,] -2.261118\n [3,]  3.439089\n [4,] -2.163296\n [5,] -1.421789\n [6,]  2.038185\n [7,]  2.707313\n [8,] -3.360417\n [9,]  2.631489\n[10,] -5.837502\n\n\nThe mean of \\(\\mathbf{y}\\) for each treatment group can be computed as follows. We’re going to use a little {dplyr} code to perform the summarization, as I find it a little more readable & replicable than base R. This will necessitate creating a data.frame to hold our \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) variables. Note that we’ve switched back to the categorical representation of \\(\\mathbf{X}\\), as it’s more interpretable than the indicator variable version for summaries such as this.\n\n\nCode\ndata.frame(X = X, y = y)\n\n\n\n\n\n\nX\ny\n\n\n\n\nA\n4.356184\n\n\nA\n-2.261118\n\n\nA\n3.439089\n\n\nB\n-2.163296\n\n\nA\n-1.421789\n\n\nA\n2.038185\n\n\nA\n2.707313\n\n\nB\n-3.360417\n\n\nA\n2.631489\n\n\nA\n-5.837502\n\n\n\n\n\n\nHere’s the mean for each group:\n\n\nCode\ndata.frame(X = X, y = y) %>% \n  with_groups(X, \n              summarise, \n              mu = mean(y))\n\n\n\n\n\n\nX\nmu\n\n\n\n\nA\n0.7064813\n\n\nB\n-2.7618564\n\n\n\n\n\n\nLet’s use the OLS formula to solve for \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]\\). Note that we’re once again using the design matrix version of \\(\\mathbf{X}\\) with the intercept column and indicator variable for group. We see that the intercept \\(\\beta_0\\) is equal to the mean of the A group - but the coefficient for the B group \\(\\beta_1\\) isn’t! This is because \\(\\beta_1\\) doesn’t have the same interpretation as \\(\\beta_0\\). While \\(\\beta_0\\) is equal to the mean of the reference group (i.e., the first level of the categorical variable, in our case group A), \\(\\beta_1\\) represents the difference between the mean for group A and the mean for group B.\n\n\nCode\ncat_mod_beta <- solve(t(X_2) %*% X_2) %*% t(X_2) %*% y\ncat_mod_beta\n\n\n                  [,1]\n(Intercept)  0.7064813\nXB          -3.4683377\n\n\nThis becomes easier to understand when we sum the coefficients and get the average for group B, which is -2.7618564.\n\n\nCode\nsum(cat_mod_beta)\n\n\n[1] -2.761856\n\n\nThis is validated by fitting a linear model with lm() and checking the output, which matches our manual solution:\n\n\nCode\ncat_mod <- lm(y ~ X)\ncoef(cat_mod)\n\n\n(Intercept)          XB \n  0.7064813  -3.4683377 \n\n\nTo summarize: when categorical variables are used in an ordinary linear regression, the intercept represents the mean of the response variable when each of the categorical variables is at its reference level. When running regressions like this, it’s important to make sure that 1) you know what the reference levels are for each of your categorical variables and 2) that those reference levels make sense. Sometimes it doesn’t matter what order the categorical variables are in, but it often does - so check! A final note is that this interpretation holds when only categorical variables are used in your model. When continuous variables are included too, the interpretation changes. More on that in a bit.\nNote: Working with categorical variables (or factors, as R labels them) can be confusing. If you want to gain a deeper understanding of how factors work, check out the chapter on them in the R for Data Science book. For factor-related tools, try the {forcats} R package, which is part of the {tidyverse} ecosystem and makes dealing with factors a lot simpler."
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-continuous-predictors",
    "href": "derivations/Intercept_Interpretation.html#models-with-continuous-predictors",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Continuous Predictors",
    "text": "Models with Continuous Predictors\nContinuous predictors differ from categorical ones in that they do not have a discrete set of possible values. The interpretation of the intercept thus differs. For any regression, the intercept interpretation is the value of the response when all predictors are equal to zero. What “all predictors equal to zero” means depends on the types of predictors you’re using; when predictors are categorical this implies that all predictors are at their reference levels (thanks to the indicator variable encoding that’s done in the background). With continuous variables, being equal to zero might have a reasonable interpretation or it might not, depending on what the variable is. In this case, think of the intercept like you would in middle school when learning \\(y = mx + b\\). The intercept, \\(b\\), is the value of \\(y\\) where \\(x = 0\\), like the plot below.\n\n\nCode\ndata.frame(x = rnorm(500, sd = 2)) %>% \n  mutate(y = x + rnorm(500, sd = 0.5)) %>% \n  ggplot(aes(x = x, y = y)) + \n  geom_point(alpha = 0.8) + \n  geom_vline(xintercept = 0, color = \"forestgreen\", size = 1) +\n  labs(x = \"X\", y = \"Y\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\nIn some cases, this value might be meaningful - for example, if your covariate of interest was the expression of a certain gene, then zero expression has a biological meaning. In other situations it makes little sense, such as when the covariate of interest is the age of each patient in your dataset. It’s unlikely that age being equal to zero would really mean much, as newborns aren’t often part of clinical trials. There’s ways to remedy this difficulty in interpretation, but we’ll focus first on why the interpretation is the way it is.\nWe’ll start by generating some data. The predictor variable we’re interested in will be negative-binomially distributed, with \\(X \\sim \\text{NB}(10, 0.7)\\). Note that we’re using the parameterization of the negative binomial used in the rnbinom() function defaults, with the size and probability parameters. Our response variable \\(\\mathbf{y}\\) will be a function of \\(\\mathbf{X}\\) with added normally-distributed noise. Since we’ve increased our sample size to \\(n = 500\\), we’ll only look at the first few rows of each variable.\n\n\nCode\nX <- rnbinom(500, size = 10, prob = 0.7)\ny <- 2 * X + rnorm(500, mean = 0, sd = 2)\nX <- matrix(X, ncol = 1)\ny <- matrix(y, ncol = 1)\ndata.frame(X = X[, 1], y = y[, 1]) %>% \n  slice_head(n = 5)\n\n\n\n\n  \n\n\n\nWe can plot the data using {ggplot2} to get an idea of what the relationship between the two variables is.\n\n\nCode\ndata.frame(X = X[, 1], y = y[, 1]) %>% \n  ggplot(aes(x = X, y = y)) + \n  geom_point() + \n  labs(x = \"X\", y = \"y\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\nUsing {dplyr}, we can find the mean of \\(\\mathbf{y}\\) when \\(\\mathbf{X} = 0\\).\n\n\nCode\ndata.frame(X = X[, 1], y = y) %>% \n  filter(X == 0) %>% \n  summarise(mu = mean(y))\n\n\n\n\n\n\nmu\n\n\n\n\n0.7797701\n\n\n\n\n\n\nLet’s fit a linear model manually and see what the coefficients are. We’ll first need to create the design matrix again using model.matrix(). This gives us a two column matrix, with the first column being the intercept (all 1s), and the second column being the negative binomial random variable \\(\\mathbf{X}\\) we just simulated. Unlike models with categorical predictors, the intercept is not simply equal to the expected value when \\(\\mathbf{X} = 0\\). Instead, the intercept is the expected value of the response variable conditional on the line of best fit that has been obtained i.e., conditional on the rest of the data in \\(\\mathbf{X}\\). See this StackOverflow post for another example.\n\n\nCode\nX_3 <- model.matrix(~X)\ncont_mod_beta <- solve(t(X_3) %*% X_3) %*% t(X_3) %*% y\ncont_mod_beta\n\n\n                   [,1]\n(Intercept) -0.02556166\nX            2.02990215\n\n\nWe can validate the above by fitting a linear model with lm() and checking the coefficients \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]\\), which are equal to our manually-computed coefficients.\n\n\nCode\ncont_mod <- lm(y ~ X)\ncoef(cont_mod)\n\n\n(Intercept)           X \n-0.02556166  2.02990215 \n\n\n\nCentering Continuous Predictors\nOne way to make models like this more interpretable is to center continuous variables around their means. Doing so ensures that the centered version of continuous variable is equal to zero when the original version of the variable is at its mean. This can give a better interpretation to some models e.g., if the continuous variable of interest was patient age, then the intercept would be the expected value of the response for a patient of mean age. Since centering doesn’t change the units of the continuous variable, only the intercept \\(\\beta_0\\) will change i.e., the coefficient for our predictor of interest will stay the same. We can validate this by creating a centered version of \\(\\mathbf{X}\\) and re-running the regression. The scale() function centers (subtracts the mean) and standardizes (divides by the standard deviation) by default, so we need to set scale = FALSE in order to only center the data.\n\n\nCode\nX_cent <- scale(X, scale = FALSE)\ncont_mod_centered <- lm(y ~ X_cent)\ncoef(cont_mod_centered)\n\n\n(Intercept)      X_cent \n   8.776094    2.029902 \n\n\n\n\nStandardizing Continuous Predictors\nStandardizing (or scaling, as R calls it) can also occasionally be useful. Dividing by the standard deviation in addition to centering results in our continuous random variable having mean 0 and standard deviation 1. This does change the units of the variable though, which is important to remember. It does not, however, change the interpretation of the intercept - which remains unchanged from the model we fit above with only centering. The coefficient \\(\\beta_1\\) differs though, and now represents the change in \\(\\mathbf{y}\\) given a one standard deviation increase in \\(\\mathbf{X}\\). For more on standardization see this StatLect post and this blog post from Columbia’s Dr. Andrew Gelman.\n\n\nCode\nX_scaled <- scale(X)\ncont_mod_scaled <- lm(y ~ X_scaled)\ncoef(cont_mod_scaled)\n\n\n(Intercept)    X_scaled \n   8.776094    4.960213 \n\n\nLastly, with respect to standardization at least, it’s important to note that if we standardize the response variable \\(\\mathbf{y}\\) in addition to standardizing the predictor matrix \\(\\mathbf{X}\\), the intercept will disappear i.e., it will become zero. This is because after standardization, the means of both the predictor and response variables are equal to zero. Since the intercept is the mean of the response when the predictor is zero, the intercept is also zero. Note that because of how integers work in computer memory the value of the intercept shown below isn’t quite zero, but it is very close. For another example of this, see this Stackoverflow post.\n\n\nCode\ny_scaled <- scale(y)\ncont_mod_resp_scaled <- lm(y_scaled ~ X_scaled)\ncoef(cont_mod_resp_scaled)\n\n\n (Intercept)     X_scaled \n1.910689e-16 9.252827e-01"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-categorical-continuous-predictors",
    "href": "derivations/Intercept_Interpretation.html#models-with-categorical-continuous-predictors",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Categorical & Continuous Predictors",
    "text": "Models with Categorical & Continuous Predictors\nFinally, let’s put it all together. In most real-life modeling scenarios you’ll have a mix of categorical and continuous predictors, and thus care must be taken when preparing the data. You generally will want your intercept to be meaningful - whatever that means for the data you’re analyzing. In this case, we’ll simulate data under the following scenario: our response variable \\(\\mathbf{y}\\) is the expression of some gene of interest in each of \\(n = 500\\) patients, and our predictor matrix \\(\\mathbf{X}\\) is composed of one categorical variable representing a treatment, a continuous variable representing patient age in years, and another categorical variable representing the facility at which each patient was treated.\n\n\nCode\nX_df <- data.frame(age = rpois(500, lambda = 30), \n                   treatment = sample(c(\"A\", \"B\"), 500, replace = TRUE), \n                   facility = sample(c(\"Hosp1\", \"Hosp2\"), 500, replace = TRUE)) %>% \n        mutate(y = 2 * age + rpois(500, lambda = 10), \n               y = case_when(treatment == \"A\" ~ 0.7 * y - rpois(1, lambda = 20), \n                             TRUE ~ y), \n               y = case_when(facility == \"Hosp2\" ~ y + rpois(1, lambda = 10), \n                             TRUE ~ y))\n\n\nThe above code to might be a bit confusing - we simulate age as a Poisson random variable with a mean of 30 years, and randomly assign one of two treatments and one of two facilities to each patient. Our response variable \\(\\mathbf{y}\\) is a function of all three predictors. We start by multiplying age by two and then adding Poisson-distributed random noise. We change the slope and subtract Poisson noise for treatment group A, and add Poisson-distributed noise for facility group Hosp2. Visualizing the data should help make sense of this process:\n\n\nCode\nggplot(X_df, aes(x = age, y = y, color = treatment)) + \n  facet_wrap(~facility) + \n  geom_point(alpha = 0.8) + \n  geom_smooth(mapping = aes(group = treatment), \n              color = \"black\",\n              method = \"lm\", \n              show.legend = FALSE) + \n  labs(x = \"Age (Years)\", \n       y = \"Gene Expression\", \n       color = \"Treatment\") + \n  theme_classic()\n\n\n\n\n\nWe can see that the lowest value of age in our dataset is 15, and thus it doesn’t really make sense to have our intercept correspond to age being equal to 0. Instead, we should center age, which will ensure that our intercept represents the expected value of \\(\\mathbf{y}\\) for a patient of mean age that was given treatment A at facility Hosp1. The reference groups for each categorical variable are known since R sorts categorical variables alphabetically by default (though this can be overridden through functions like relevel()).\n\n\nCode\nX_df <- mutate(X_df, \n               age_centered = scale(age, scale = FALSE))\n\n\nNow that we have our centered variable, we can set up our design matrix. We use the formula syntax to specify which predictors we’re interested in.\n\n\nCode\nX_design_mat <- model.matrix(~age_centered + treatment + facility, data = X_df)\nhead(X_design_mat)\n\n\n  (Intercept) age_centered treatmentB facilityHosp2\n1           1        3.622          1             0\n2           1       -1.378          1             1\n3           1        3.622          1             1\n4           1        0.622          0             0\n5           1        3.622          1             1\n6           1        6.622          0             0\n\n\nWe can now solve the ordinary linear regression problem by hand to obtain the coefficient vector \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]\\).\n\n\nCode\nfull_mod_beta <- solve(t(X_design_mat) %*% X_design_mat) %*% t(X_design_mat) %*% X_df$y\nfull_mod_beta\n\n\n                   [,1]\n(Intercept)   34.428233\nage_centered   1.672477\ntreatmentB    36.374068\nfacilityHosp2  8.128789\n\n\nWe can verify the result once again using lm(). The interpretation for \\(\\beta_0\\) is the expected response for a patient of mean age who is taking treatment A at facility Hosp1.\n\n\nCode\nfull_mod <- lm(y ~ age_centered + treatment + facility, X_df)\ncoef(full_mod)\n\n\n  (Intercept)  age_centered    treatmentB facilityHosp2 \n    34.428233      1.672477     36.374068      8.128789"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#setup-1",
    "href": "derivations/Intercept_Interpretation.html#setup-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Setup",
    "text": "Setup\nWe’ll next move to the more complicated case of the generalized linear model (GLM). We’ll start by defining the basic form of a GLM; the main difference from an ordinary linear model is that the model’s response variable is a transformation of the actual response variable. This transformation is taken via what we call a link function. There are some detail here I’m skipping over, but in practice the link function is usually the natural log, and can be others such as the logit (for logistic regression). The link function is usually denoted \\(g(\\cdot)\\), which gives us the following general form of a GLM with \\(p\\) covariates:\n\\[\ng(\\mathbb{E}[\\mathbf{y}]) = \\beta_0 + \\beta_1 \\mathbf{X}_1 + \\dots + \\beta_p\\mathbf{X}_p\n\\]\nLike ordinary linear models, GLMs are linear in their covariates (as shown above), which is what gives them their relatively easy interpretations. However, unlike with ordinary linear regression, there is no simple closed-form solution to the above equation. Instead, a solution is estimated using something like iteratively reweighted least squares or Newton’s method. As such, we won’t be able to provide exact calculations of the GLM solutions like we did above with the ordinary linear models, so we’ll stick to first providing the interpretation and then checking to make sure the fitted model matches that interpretation."
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#the-intercept-only-model-1",
    "href": "derivations/Intercept_Interpretation.html#the-intercept-only-model-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "The Intercept-only Model",
    "text": "The Intercept-only Model\nIn the intercept-only model, the GLM formula becomes:\n\\[\n\\begin{aligned}\ng(\\mathbb{E}[\\mathbf{y}]) &= \\beta_0 \\\\\n\\mathbb{E}[\\mathbf{y}] &= g^{-1}(\\beta_0) \\\\\n\\end{aligned}\n\\]\nIf we’re using \\(g(\\cdot) = \\text{log}(\\cdot)\\) (the log-link), then the inverse of \\(g(\\cdot)\\) is \\(g^{-1}(\\cdot) = \\text{exp}(\\cdot)\\). In this case, it’s easy to see that the intercept \\(\\beta_0\\) is actually the natural log of the mean of the response variable \\(\\mathbf{y}\\).\nWe can verify this in R. In this example we’ll use a Poisson GLM with a log link function, as it’s probably the simplest to understand. We’ll start by simulating a Poisson-distributed response \\(\\mathbf{y} \\sim \\text{Poisson}(5)\\) with \\(n = 10\\).\n\n\nCode\ny <- rpois(10, lambda = 5)\ny <- matrix(y, ncol = 1)\ny\n\n\n      [,1]\n [1,]    3\n [2,]    4\n [3,]    7\n [4,]    8\n [5,]    8\n [6,]    4\n [7,]    3\n [8,]    8\n [9,]   11\n[10,]    6\n\n\nThe mean of \\(\\mathbf{y}\\) is 6.2, and the log of that quantity is:\n\n\nCode\nlog(mean(y))\n\n\n[1] 1.824549\n\n\nWe can fit a Poisson GLM like so. We can clearly see that \\(\\beta_0 = \\text{log}(\\bar{\\mathbf{y}})\\).\n\n\nCode\nnull_mod_pois <- glm(y ~ 1, family = poisson(link = \"log\"))\ncoef(null_mod_pois)\n\n\n(Intercept) \n   1.824549 \n\n\nThus, when we use the inverse of the link function (exponentiation) on the estimated value of \\(\\beta_0\\), we get the value of \\(\\mathbb{E}[\\mathbf{y}] = 6.2\\).\n\n\nCode\nexp(coef(null_mod_pois))\n\n\n(Intercept) \n        6.2"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-categorical-predictors-1",
    "href": "derivations/Intercept_Interpretation.html#models-with-categorical-predictors-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Categorical Predictors",
    "text": "Models with Categorical Predictors\nThe extension to categorical predictors is much the same as we saw before with ordinary linear models, with the primary change being that we’re now working on the log scale. Let’s add a categorical predictor to our model. First we simulate a categorical \\(\\mathbf{X}\\):\n\n\nCode\nX <- sample(c(\"A\", \"B\"), size = 10, replace = TRUE)\nX <- matrix(X, ncol = 1)\nX\n\n\n      [,1]\n [1,] \"B\" \n [2,] \"B\" \n [3,] \"B\" \n [4,] \"A\" \n [5,] \"A\" \n [6,] \"A\" \n [7,] \"A\" \n [8,] \"B\" \n [9,] \"A\" \n[10,] \"A\" \n\n\nAgain using {dplyr}, we can find \\(\\bar{\\mathbf{y}}\\) and \\(\\text{log}(\\bar{\\mathbf{y}})\\) for each group in our categorical variable:\n\n\nCode\ndata.frame(y = y, \n           X = X[, 1]) %>% \n  with_groups(X, \n              summarise, \n              mu = mean(y), \n              log_mu = log(mean(y)))\n\n\n\n\n\n\nX\nmu\nlog_mu\n\n\n\n\nA\n6.666667\n1.897120\n\n\nB\n5.500000\n1.704748\n\n\n\n\n\n\nWe fit a Poisson GLM using the glm() function, again using the natural log as our link function. We see that the intercept is equal to the log of the mean of our response variable for group A.\n\n\nCode\ncat_mod_pois <- glm(y ~ X, family = poisson(link = \"log\"))\ncoef(cat_mod_pois)\n\n\n(Intercept)          XB \n  1.8971200  -0.1923719 \n\n\nNext, if we sum the coefficients, we get the log of the response variable for group B:\n\n\nCode\nsum(coef(cat_mod_pois))\n\n\n[1] 1.704748\n\n\nIf we exponentiate the coefficients the intercept becomes simply the mean of \\(\\mathbf{y}\\) for group A.\n\n\nCode\nexp(coef(cat_mod_pois))\n\n\n(Intercept)          XB \n   6.666667    0.825000 \n\n\nLastly, if we exponentiate the sum of the coefficients (the order of these operations is important), we get the mean of \\(\\mathbf{y}\\) for group B:\n\n\nCode\nexp(sum(coef(cat_mod_pois)))\n\n\n[1] 5.5"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-continuous-predictors-1",
    "href": "derivations/Intercept_Interpretation.html#models-with-continuous-predictors-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Continuous Predictors",
    "text": "Models with Continuous Predictors\nAs with ordinary linear models, in the presence of a continuous predictor the intercept becomes the expected value of the response variable conditional on the line of best fit that we generate. First we generate another \\(\\mathbf{y}\\) with a larger sample size and a slightly lower value of \\(\\lambda\\). Then we generate Gamma random noise, and define the predictor \\(\\mathbf{X} = 3.1y + \\epsilon\\).\n\n\nCode\ny <- rpois(300, lambda = 3)\ny <- matrix(y, ncol = 1)\nX <- 3.1 * y + rgamma(300, shape = 20, rate = 4)\nX <- matrix(X, ncol = 1)\n\n\nWe can plot \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to get an idea of what their relationship is. Since \\(\\mathbf{X}\\) is a Gamma random variable, it doesn’t actually have any zero values; the minimum value we’ve generated is actually 2.774473. As such, the intercept will be an extrapolation of the data that we do have.\n\n\nCode\ndata.frame(y = y, \n           X = X[, 1]) %>% \n  ggplot(aes(x = X, y = y)) + \n  geom_point(alpha = 0.8) + \n  labs(x = \"X\", y = \"Y\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\nWe fit another Poisson GLM and check the coefficients. The intercept in this case is interpreted as the log of the expected value of the response when our predictor variable is equal to zero. However - since \\(\\mathbf{X}\\) is essentially a Gamma random variable, it can actually never take a value of zero, as its support is \\((0, \\infty)\\). As such, the interpretation of \\(\\beta_0\\) is somewhat useless here, and thus it makes sense to center \\(\\mathbf{X}\\) as we did earlier with the ordinary linear model.\n\n\nCode\ncont_mod_pois <- glm(y ~ X, family = poisson(link = \"log\"))\ncoef(cont_mod_pois)\n\n\n(Intercept)           X \n-0.35916189  0.09221817 \n\n\nAfter centering and refitting the Poisson GLM, the intercept takes on a new interpretation - the log of \\(\\mathbf{y}\\) when \\(\\mathbf{X}\\) is at its mean. Note that the coefficient for \\(\\mathbf{X}\\) does not change, as centering does not change the units of \\(\\mathbf{X}\\).\n\n\nCode\nX_cent <- scale(X, scale = FALSE)\ncont_mod_pois_centered <- glm(y ~ X_cent, family = poisson(link = \"log\"))\ncoef(cont_mod_pois_centered)\n\n\n(Intercept)      X_cent \n 0.95306190  0.09221817 \n\n\nLet’s plot both the raw data and the fitted values obtained from our model. The blue horizontal line shows \\(\\text{exp}(\\beta_0)\\) i.e., the expected value of \\(\\mathbf{y}\\) when \\(\\mathbf{X}\\) is at its mean value (again, conditional on the line of best fit that we obtained). The vertical yellow line shows where the mean of \\(\\mathbf{X}\\) is; since we centered \\(\\mathbf{X}\\) this is equal to zero. Lastly, the green line shows the predicted values from our model, and this line intersects nicely with the value of the intercept as we would expect.\n\n\nCode\nintercept_exp <- exp(coef(cont_mod_pois_centered)[1])\ndata.frame(y = y, \n           y_pred = fitted(cont_mod_pois_centered), \n           X = X_cent[, 1]) %>% \n  ggplot(aes(x = X, y = y)) + \n  geom_point(alpha = 0.8) + \n  geom_hline(yintercept = intercept_exp, color = \"dodgerblue\", size = 1) + \n  geom_vline(xintercept = 0, color = \"goldenrod\", size = 1) + \n  geom_line(aes(y = y_pred), color = \"forestgreen\", size = 1) + \n  labs(x = \"X (centered)\", y = \"Y\") + \n  theme_classic()"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-categorical-continuous-predictors-1",
    "href": "derivations/Intercept_Interpretation.html#models-with-categorical-continuous-predictors-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Categorical & Continuous Predictors",
    "text": "Models with Categorical & Continuous Predictors\nLastly, we’ll again examine the most common real-life situation - models with both categorical and continuous predictors. Consider a dataset where the response is a Poisson-distributed random variable, say the number of copies of viral RNA in a sample taken from a patient. First, let’s say that our sample size is set to a generous \\(n = 1000\\). Next, imagine that we have two continuous predictors; the first being patient age in years, and the second being the number of months that the patient has been in treatment. We’ll simulate age as a Poisson random variable with a mean of 25 years, and months in treatment as a negative binomial random variable with a mean of 3 months and overdispersion parameter (denoted size in the rnbinom() function) of 4. Lastly, let’s define a categorical predictor with 3 possible treatment categories. We store all 3 predictors in a data.frame.\n\n\nCode\nX_age <- rpois(1000, 25)\nX_months <- rnbinom(1000, size = 4, mu = 3)\nX_treat <- sample(c(\"Drug_A\", \"Drug_B\", \"Drug_C\"), \n                  size = 1000, \n                  replace = TRUE)\nmod_df <- data.frame(age = X_age, \n                     months = X_months, \n                     treat = X_treat)\nslice_head(mod_df, n = 5)\n\n\n\n\n\n\nage\nmonths\ntreat\n\n\n\n\n29\n3\nDrug_A\n\n\n20\n4\nDrug_C\n\n\n28\n4\nDrug_A\n\n\n29\n1\nDrug_A\n\n\n27\n1\nDrug_C\n\n\n\n\n\n\nFinally, we’ll define \\(\\mathbf{y}\\) to be a function of all 3 predictor variables along with a large amount of Poisson-distributed random noise following the distribution \\(\\epsilon \\sim \\text{Poisson}(50)\\). We use the handy dplyr::case_when() function to create \\(\\mathbf{y}\\) as a piecewise function whose relationship to the predictor variable changes based on the treatment each patient was given. After generating \\(\\mathbf{y}\\), we round it to ensure that it is integer-valued, since we’re focused here on using Poisson GLMs.\n\n\nCode\nepsilon <- rpois(1000, 50)\nmod_df <- mod_df %>% \n          mutate(y = case_when(treat == \"Drug_A\" ~ 2.25 * age - 1.2 * months, \n                               treat == \"Drug_B\" ~ 2 * age - 3 * months, \n                               treat == \"Drug_C\" ~ 1.75 * age - 5 * months), \n                 y = round(y + epsilon))\nslice_head(mod_df, n = 5)\n\n\n\n\n\n\nage\nmonths\ntreat\ny\n\n\n\n\n29\n3\nDrug_A\n111\n\n\n20\n4\nDrug_C\n61\n\n\n28\n4\nDrug_A\n117\n\n\n29\n1\nDrug_A\n115\n\n\n27\n1\nDrug_C\n111\n\n\n\n\n\n\nWe know that for our two continuous predictors, the intercept will indicate the predicted value of \\(\\mathbf{y}\\) for patients with values of zero for those two variables. Since we also have a categorical variable, the intercept will refer to the reference group of that variable - in this case patients who were assigned Drug_A. Lastly, since we’re using a Poisson GLM, we know that the intercept will be the natural log of that quantity. Our model will be of the following form:\n\\[\n\\text{log}(\\mathbb{E}[\\text{Viral RNA}]) = \\beta_0 + \\beta_{\\text{age}} + \\beta_{\\text{months}} + \\beta_{\\text{treat}} + \\epsilon\n\\]\nLet’s visualize the data so that we can get some idea of what the relationships between our variables are, and what the intercept of the model we’re going to fit will tell us. First let’s look at the relationship between age and the response, splitting by treatment group. We add a purple vertical line showing where the overall mean of age is.\n\n\nCode\nggplot(mod_df, aes(x = age, y = y, color = treat)) + \n  geom_vline(aes(xintercept = mean(age)), color = \"purple\", size = 1) + \n  geom_point(alpha = 0.8) + \n  labs(x = \"Age\", y = \"Viral RNA\", color = \"Treatment\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\nWe repeat the visualization for months.\n\n\nCode\nggplot(mod_df, aes(x = months, y = y, color = treat)) + \n  geom_vline(aes(xintercept = mean(months)), color = \"purple\", size = 1) + \n  geom_point(alpha = 0.8) + \n  labs(x = \"Months Treated\", y = \"Viral RNA\", color = \"Treatment\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\nLastly, let’s simply compare the distribution of the response between the three treatment groups using a beeswarm plot, which is a variation on the classic violin plot that I’ve preferred recently. Check out the ggbeeswarm package docs for more. We also add a horizontal line showing \\(\\bar{\\mathbf{y}}\\) for each group.\n\n\nCode\nggplot(mod_df, aes(x = treat, y = y, color = treat)) + \n  ggbeeswarm::geom_quasirandom() + \n  stat_summary(fun = \"mean\",\n               geom = \"crossbar\", \n               width = 0.5,\n               size = 0.75, \n               color = \"black\") + \n  labs(x = \"Treatment\", y = \"Viral RNA\", color = \"Treatment\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\nLet’s fit a model. As previously noted, we use the log link function. Since our treatment variable has three categories, we now have two coefficients for each of the non-reference group levels. The intercept \\(\\beta_0\\) gives us the log of the expected value of the response for a patient that is 0 years old, has spent 0 months in treatment, and is assigned to be treated with Drug_A. We can tell right away that this isn’t the most useful interpretation, as there are no patients in our study who are zero years old (and even if there were, it probably wouldn’t be that useful to know that quantity).\n\n\nCode\nfull_mod_pois <- glm(y ~ age + months + treat, \n                     data = mod_df, \n                     family = poisson(link = \"log\"))\ncoef(full_mod_pois)\n\n\n(Intercept)         age      months treatDrug_B treatDrug_C \n 4.18283220  0.02190533 -0.03623370 -0.12200269 -0.25328199 \n\n\nSince we’re using the log-link, exponentiating the intercept gives us the expected value of \\(\\mathbf{y}\\) under the conditions we described just above.\n\n\nCode\nexp(coef(full_mod_pois))[1]\n\n\n(Intercept) \n   65.55124 \n\n\nClearly, the age variable is a prime target for centering. While it could be useful to center months (depending on the goals of the study), there are a total of 111 patients in the study who have spent zero months in treatment, so it’s at least somewhat anchored in reality. We center age, then refit the model. We see that the estimate of \\(\\beta_0\\) changes, while none of the coefficients for our predictors do (as expected).\n\n\nCode\nmod_df <- mutate(mod_df, \n                 age_cent = scale(age, scale = FALSE))\nfull_mod_pois_centered <- glm(y ~ age_cent + months + treat, \n                              data = mod_df, \n                              family = poisson(link = \"log\"))\ncoef(full_mod_pois_centered)\n\n\n(Intercept)    age_cent      months treatDrug_B treatDrug_C \n 4.73046554  0.02190533 -0.03623370 -0.12200269 -0.25328199 \n\n\nExponentiating \\(\\beta_0\\) now gives us the expected value of \\(\\mathbf{y}\\) for a patient of average age (which is 25 years) who was assigned treatment Drug_A and has spent zero months being treated. This quantity is useful because it tells us the baseline pre-treatment value for the average patient who takes Drug_A.\n\n\nCode\nexp(coef(full_mod_pois_centered))[1]\n\n\n(Intercept) \n   113.3483"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a first-year PhD student at the University of Florida under Dr. Rhonda Bacher. My primary research interest are genomics and genetics, and particularly single cell RNA-seq computational method development. I mostly write tools related to clustering, trajectory inference, and differential expression analysis. In addition, I also focus on scientific reproducibility, computational scalability, and interpretable inference."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Florida | PhD Biostatistics\nAug 2022 - Current\nUniversity of Florida | MS Biostatistics\nAug 2020 - May 2022\nUniversity of North Carolina | BS Statistics\nAug 2016 - May 2020"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience\nUniversity of Florida | Graduate Research & Teaching Assistant\nMay 2021 - present\nBlue Cross Blue Shield of Florida | Data Analyst\nJuly 2020 - July 2022\nUniversity of North Carolina | Undergraduate Research Assistant\nJune 2019 - July 2020"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Background",
    "section": "",
    "text": "My undergrad degree is in statistics, which I did at the University of North Carolina at Chapel Hill (Go Heels!). After graduating in 2020, I spent some of the next two years earning a master’s degree in biostatistics from the University of Florida. During my second year in that program, I applied to PhD programs and ended up deciding to continue at UF and do a PhD in the same department. I’m currently entering the second semester of my first year. My most relevant studies include courses on generalized linear models, statistical learning theory, convex optimization, probability theory, mathematical statistics, and statistical computing at the undergraduate and graduate levels."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Background",
    "section": "Work Experience",
    "text": "Work Experience\nIn the summer after my junior year at UNC, I joined Dr. Jen Jen Yeh’s lab at Lineberger Comprehensive Cancer Center. There I learned how to process and analyze genomic and transcriptomic data, and in Fall 2019 started focusing on developing methods for scRNA-seq analysis. My primary contribution there was the development of reproducible bioinformatics pipelines for bulk & single cell RNA-seq, ChIP-seq, and whole genome & exome sample processing. In addition, I developed a robust, evidence-based downstream analysis workflow for the lab’s scRNA-seq samples.\nAfter leaving the Yeh Lab in July 2020, I moved to Jacksonville and started working full-time as a junior data analyst at Blue Cross Blue Shield of Florida. While there, I completed a year-long rotational program during which I applied regression and clustering methods to several complex business problems. A highlight was my usage of graph-based clustering and dimension reduction algorithms (concepts lifted directly from my experience with scRNA-seq data) to identify well-performing subsets of specialist doctors. After my rotations ended, I became a full analyst and spent the next year working in care analytics, where I tested and deployed machine learning algorithms used to identify people to be targeted for various preventive healthcare programs. During this time, I also completed a manuscript started while I was at the Yeh Lab detailing a method for improved clustering of single cell data containing rare cell types.\nWhile all this was going on, I also started doing research work (once again focused on scRNA-seq method development) in Dr. Rhonda Bacher’s group at UF. Our original focus was in profiling, testing, and improving existing methods for RNA velocity and trajectory inference through simulations and analysis of publicly available single cell datasets.\nAfter two years at Blue Cross, and graduating with my master’s in May of 2022, I decided to join Dr. Bacher as a PhD student in Fall 2022. My first semester was spent pursuing scRNA-seq research, with a focus on improving the interpretability of trajectory differential expression methods. I also provided downstream analysis of single cell data generated by Dr. Phillip Efron’s lab containing different types of peripheral blood mononuclear cells. In addition, I was the instructor for the online section of PHC 6790 (Biostatistical Computing with SAS), a master’s-level course developed by fellow Tar Heel Dr. John Kairalla."
  },
  {
    "objectID": "about.html#personal-interests",
    "href": "about.html#personal-interests",
    "title": "Background",
    "section": "Personal Interests",
    "text": "Personal Interests\nOutside of research, I enjoy reading (my favorite genres are nautical adventure, weird history, and narrative science), riding my bike, cooking for my friends, and rock climbing."
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "Background",
    "section": "Publications",
    "text": "Publications\n\nJack R. Leary, Yi Xu, Ashley Morrison, Chong Jin, Emily C. Shen, Ye Su, Naim U. Rashid, Jen Jen Yeh, Xianlu Laura Peng. Sub-cluster identification through semi-supervised optimization of rare-cell silhouettes (SCISSORS) in single-cell sequencing. BioRxiv. 466448, (2021).\nMatthew E. Berginski, Madison R. Jenner, Chinmaya U. Joisa, Silvia G.Herrera Loeza, Brian T. Golitz, Matthew B. Lipner, Jack R. Leary, Naim U. Rashid, Gary L. Johnson, Jen Jen Yeh, Shawn M. Gomez. Kinome state is predictive of cell viability in pancreatic cancer tumor and stroma cell lines. BioRxiv. 451515, (2021)."
  },
  {
    "objectID": "tutorials/SCISSORS_Reclustering.html",
    "href": "tutorials/SCISSORS_Reclustering.html",
    "title": "scRNA-seq Reclustering with {SCISSORS}",
    "section": "",
    "text": "In this tutorial we’ll walk through a basic single cell analysis, with a focus on fine-tuning clustering results using the {SCISSORS} package, which I wrote during my time at UNC Chapel Hill."
  },
  {
    "objectID": "tutorials/SCISSORS_Reclustering.html#preprocessing",
    "href": "tutorials/SCISSORS_Reclustering.html#preprocessing",
    "title": "scRNA-seq Reclustering with {SCISSORS}",
    "section": "Preprocessing",
    "text": "Preprocessing\nWe’ll do some minor quality-control checking first by filtering out cells with a high percentage of mitochondrial reads or very low or high numbers of detected genes.\n\n\nCode\npbmc <- PercentageFeatureSet(pbmc, \n                             pattern = \"^MT-\", \n                             col.name = \"percent_MT\")\npbmc <- pbmc[, pbmc$nFeature_RNA >= 200 & pbmc$nFeature_RNA <= 2500 & pbmc$percent_MT <= 10]\n\n\nWe’ll process the raw counts in the usual fashion: QC, normalization, identification of highly variable genes (HVGs), linear & non-linear dimension reduction, and a broad clustering that will (hopefully) capture our major celltypes. When computing the shared nearest-neighbor (SNN) graph, we use the heuristic \\(k = \\sqrt{n}\\) for the number of nearest-neighbors to consider for each cell. This ensures that the clustering will be broad i.e., a smaller number of large clusters will be returned instead of a larger number of small clusters.\n\n\nCode\npbmc <- NormalizeData(pbmc, \n                      normalization.method = \"LogNormalize\", \n                      verbose = FALSE) %>% \n        FindVariableFeatures(selection.method = \"vst\", \n                             nfeatures = 3000, \n                             verbose = FALSE) %>% \n        CellCycleScoring(s.features = cc.genes.updated.2019$s.genes, \n                         g2m.features = cc.genes.updated.2019$g2m.genes, \n                         set.ident = FALSE) %>% \n        AddMetaData(metadata = c(.$S.Score - .$G2M.Score), col.name = \"CC_difference\") %>% \n        ScaleData(vars.to.regress = \"CC_difference\", verbose = FALSE) %>% \n        RunPCA(features = VariableFeatures(.), \n               npcs = 50, \n               verbose = FALSE, \n               seed.use = 312) %>% \n        RunUMAP(reduction = \"pca\",\n                dims = 1:20, \n                n.components = 2, \n                metric = \"cosine\", \n                seed.use = 312, \n                verbose = FALSE) %>% \n        FindNeighbors(reduction = \"pca\", \n                      dims = 1:20, \n                      k.param = sqrt(ncol(.)), \n                      nn.method = \"annoy\", \n                      annoy.metric = \"cosine\", \n                      verbose = FALSE) %>% \n        FindClusters(resolution = 0.3, \n                     random.seed = 312, \n                     verbose = FALSE)\n\n\nLet’s visualize the principal components. Notable genes in PC 1 include MALAT1, high abundance of which is a common artifact of 10X-sequenced data. PC 2 seems to separate NK cells (NKG7, GZMB) and myeloid cells (HLA-DRA, CD79A). PC 3 is composed of variation that could originate from platelets (PPBP). PCs 4-6 look like they separate several types of monocytic, T, NK, and dendritic cells.\n\n\nCode\nDimHeatmap(pbmc, \n           reduction = \"pca\", \n           dims = 1:6, \n           nfeatures = 15, \n           combine = TRUE)\n\n\n\n\n\nWe visualize the Louvain clustering via a UMAP plot. We see 5 major clusters, which we’ll annotate next.\n\n\nCode\nDimPlot(pbmc, pt.size = 1) + \n  scale_color_paletteer_d(\"ggsci::nrc_npg\") + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank())"
  },
  {
    "objectID": "tutorials/SCISSORS_Reclustering.html#broad-annotations",
    "href": "tutorials/SCISSORS_Reclustering.html#broad-annotations",
    "title": "scRNA-seq Reclustering with {SCISSORS}",
    "section": "Broad Annotations",
    "text": "Broad Annotations\nFirst we identify CD8+ T-cells via CD8A, and CD4+ T-cells with IL7R. Lastly, FCGR3A (aka CD16) is specific to CD16+ monocytes. We can combine the plots using the {patchwork} package.\n\n\nCode\np1 <- FeaturePlot(pbmc, features = \"CD8A\", pt.size = 1) + \n      scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme(axis.ticks = element_blank(), \n            axis.text = element_blank())\np2 <- FeaturePlot(pbmc, features = \"IL7R\", pt.size = 1) + \n      scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme(axis.ticks = element_blank(), \n            axis.text = element_blank())\np1 / p2\n\n\n\n\n\nNext, we use HLA-DRA to broadly identify monocytic cells, and FCGR3A (aka CD16) to single out the CD16+ monocytes.\n\n\nCode\np1 <- FeaturePlot(pbmc, features = \"HLA-DRA\", pt.size = 1) + \n      scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme(axis.ticks = element_blank(), \n            axis.text = element_blank())\np2 <- FeaturePlot(pbmc, features = \"FCGR3A\", pt.size = 1) + \n      scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme(axis.ticks = element_blank(), \n            axis.text = element_blank())\np1 / p2\n\n\n\n\n\nLastly, abundance of MS4A1 points out a cluster of B cells.\n\n\nCode\nFeaturePlot(pbmc, features = \"MS4A1\", pt.size = 1) + \n  scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank())\n\n\n\n\n\nWe’ll add broad celltype labels to our object’s metadata.\n\n\nCode\npbmc@meta.data <- mutate(pbmc@meta.data, \n                         broad_celltype = case_when(seurat_clusters == 0 ~ \"CD4+ T\", \n                                                    seurat_clusters == 1 ~ \"Monocyte\", \n                                                    seurat_clusters == 2 ~ \"CD8+ T\", \n                                                    seurat_clusters == 3 ~ \"B\", \n                                                    seurat_clusters == 4 ~ \"CD16+ Monocyte\", \n                                                    TRUE ~ NA_character_), \n                         broad_celltype = factor(broad_celltype, levels = c(\"CD4+ T\", \n                                                                            \"Monocyte\", \n                                                                            \"CD8+ T\", \n                                                                            \"B\", \n                                                                            \"CD16+ Monocyte\")))\n\n\nAnd visualize the results.\n\n\nCode\nDimPlot(pbmc, pt.size = 1, group.by = \"broad_celltype\") + \n  scale_color_paletteer_d(\"ggsci::nrc_npg\") + \n  labs(x = \"UMAP 1\", \n       y = \"UMAP 2\", \n       color = \"Broad Celltype\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.title = element_blank())"
  },
  {
    "objectID": "tutorials/SCISSORS_Reclustering.html#reclustering",
    "href": "tutorials/SCISSORS_Reclustering.html#reclustering",
    "title": "scRNA-seq Reclustering with {SCISSORS}",
    "section": "Reclustering",
    "text": "Reclustering\nFrom the plot above, there appears to be some visible subgroups in the monocyte cluster. With that being said - I would generally be very cautious about using UMAPs alone to define heterogeneous groups. In general, I would suggest using something like silhouette score distributions, other clustering statistics, or biological knowledge to determine subclustering targets. We can do this below using SCISSORS::ComputSilhouetteScores(), which returns a silhouette score for each individual cell. Visualizing the results can help us identify which clusters are “poor” fits. For more information, check out the Wikipedia article on clustering scores.\n\n\nCode\nsil_scores <- ComputeSilhouetteScores(pbmc, avg = FALSE)\n\n\nWe can see that the B cell and CD16+ monocyte clusters seem to be well-fit, but the other clusters are less so. We’ll focus on the other monocyte cluster, as it seems to have the highest variance.\n\n\nCode\nsil_scores %>% \n  left_join(distinct(pbmc@meta.data, seurat_clusters, broad_celltype), \n            by = c(\"Cluster\" = \"seurat_clusters\")) %>% \n  ggplot(aes(x = broad_celltype, y = Score, fill = broad_celltype)) + \n  geom_violin(scale = \"width\", \n              color = \"black\", \n              draw_quantiles = 0.5, \n              size = 0.75) + \n  scale_fill_paletteer_d(\"ggsci::nrc_npg\") + \n  labs(y = \"Silhouette Score\", fill = \"Broad Celltype\") + \n  theme_classic(base_size = 14) + \n  theme(panel.grid.major.y = element_line(), \n        axis.title.x = element_blank())\n\n\n\n\n\n\nMonocytes\n\n\nCode\nmono_reclust <- ReclusterCells(pbmc, \n                               which.clust = 1, \n                               use.parallel = FALSE, \n                               n.HVG = 3000,\n                               n.PC = 15, \n                               k.vals = c(20, 30, 40), \n                               resolution.vals = c(.2, .3, .4), \n                               random.seed = 312)\n\n\n[1] \"Reclustering cells in cluster 1 using k = 20 & resolution = 0.2; S = 0.419\"\n\n\nLet’s check out the UMAP embedding:\n\n\nCode\nDimPlot(mono_reclust) + \n  scale_color_paletteer_d(\"MetBrewer::Egypt\") + \n  labs(x = \"UMAP 1\", \n       y = \"UMAP 2\", \n       color = \"Subcluster\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.title = element_blank())\n\n\n\n\n\nHighly-specific abundance of FCER1A allows us to identify the dendritic cells in cluster 2.\n\n\nCode\ndata.frame(exp = mono_reclust@assays$RNA@data[\"FCER1A\", ], \n           label = mono_reclust$seurat_clusters) %>% \n  ggplot(aes(x = label, y = exp, fill = label)) + \n  geom_violin(scale = \"width\",\n              color = \"black\", \n              draw_quantiles = 0.5, \n              size = 0.75) + \n  scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n  labs(y = \"FCER1A\", fill = \"Subcluster\") + \n  theme_classic(base_size = 14) + \n  theme(panel.grid.major.y = element_line(), \n        axis.title.x = element_blank())\n\n\n\n\n\nBoth cluster 0 & cluster 1 seem to be CD14+, and cluster 1 appears to have slightly higher (but still low) abundance of FCGR3A.\n\n\nCode\np1 <- data.frame(exp = mono_reclust@assays$RNA@data[\"CD14\", ], \n                 label = mono_reclust$seurat_clusters) %>% \n      filter(label %in% c(0, 1)) %>% \n      ggplot(aes(x = label, y = exp, fill = label)) + \n      geom_violin(scale = \"width\",\n                  color = \"black\", \n                  draw_quantiles = 0.5, \n                  size = 0.75) + \n      scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n      labs(y = \"CD14\", fill = \"Subcluster\") + \n      theme_classic(base_size = 14) + \n      theme(panel.grid.major.y = element_line(), \n            axis.title.x = element_blank())\np2 <- data.frame(exp = mono_reclust@assays$RNA@data[\"FCGR3A\", ], \n                 label = mono_reclust$seurat_clusters) %>% \n      filter(label %in% c(0, 1)) %>% \n      ggplot(aes(x = label, y = exp, fill = label)) + \n      geom_violin(scale = \"width\",\n                  color = \"black\", \n                  draw_quantiles = 0.5, \n                  size = 0.75) + \n      scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n      labs(y = \"FCGR3A\", fill = \"Subcluster\") + \n      theme_classic(base_size = 14) + \n      theme(panel.grid.major.y = element_line(), \n            axis.title.x = element_blank())\np1 / p2\n\n\n\n\n\nFrom Kapellos et al (2019), we know that intermediate monocytes have high abundance of CD14, low but non-zero abundance of CD16 (which again is denoted FCGR3A in this dataset), and can be identified through higher abundance of other markers like HLA-DPB1 and CD74 in comparison to CD14+ monocytes. With all this information, we’ll conclude that cluster 0 is likely composed of CD14+ monocytes and cluster 1 of intermediate monocytes.\n\n\nCode\np1 <- data.frame(exp = mono_reclust@assays$RNA@data[\"HLA-DQB1\", ], \n                 label = mono_reclust$seurat_clusters) %>% \n      filter(label %in% c(0, 1)) %>% \n      ggplot(aes(x = label, y = exp, fill = label)) + \n      geom_violin(scale = \"width\",\n                  color = \"black\", \n                  draw_quantiles = 0.5, \n                  size = 0.75) + \n      scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n      labs(y = \"HLA-DQB1\", fill = \"Subcluster\") + \n      theme_classic(base_size = 14) + \n      theme(panel.grid.major.y = element_line(), \n            axis.title.x = element_blank())\np2 <- data.frame(exp = mono_reclust@assays$RNA@data[\"CD74\", ], \n                 label = mono_reclust$seurat_clusters) %>% \n      filter(label %in% c(0, 1)) %>% \n      ggplot(aes(x = label, y = exp, fill = label)) + \n      geom_violin(scale = \"width\",\n                  color = \"black\", \n                  draw_quantiles = 0.5, \n                  size = 0.75) + \n      scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n      labs(y = \"CD74\", fill = \"Subcluster\") + \n      theme_classic(base_size = 14) + \n      theme(panel.grid.major.y = element_line(), \n            axis.title.x = element_blank())\np1 / p2\n\n\n\n\n\nLastly, we can tell that cluster 3 is composed of platelets thanks to high abundance of PPBP.\n\n\nCode\ndata.frame(exp = mono_reclust@assays$RNA@data[\"PPBP\", ], \n           label = mono_reclust$seurat_clusters) %>% \n  ggplot(aes(x = label, y = exp, fill = label)) + \n  geom_violin(scale = \"width\",\n              color = \"black\", \n              draw_quantiles = 0.5, \n              size = 0.75) + \n  scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n  labs(y = \"PPBP\", fill = \"Subcluster\") + \n  theme_classic(base_size = 14) + \n  theme(panel.grid.major.y = element_line(), \n        axis.title.x = element_blank())\n\n\n\n\n\nWe can add the new subcluster labels back in to our original object using SCISSORS::IntegrateSubclusters(). We also add labels to the original object reflecting the subcluster annotations.\n\n\nCode\npbmc <- IntegrateSubclusters(pbmc, reclust.results = mono_reclust)\npbmc@meta.data <- mutate(pbmc@meta.data, \n                         celltype = case_when(seurat_clusters == 0 ~ \"CD4+ T\", \n                                              seurat_clusters == 1 ~ \"Platelet\", \n                                              seurat_clusters == 2 ~ \"CD8+ T\", \n                                              seurat_clusters == 3 ~ \"B\", \n                                              seurat_clusters == 4 ~ \"CD16+ Monocyte\", \n                                              seurat_clusters == 5 ~ \"CD14+ Monocyte\", \n                                              seurat_clusters == 6 ~ \"Intermediate Monocyte\", \n                                              seurat_clusters == 7 ~ \"Dendritic Cell\", \n                                              TRUE ~ NA_character_))\n\n\nHere’s the final celltype annotations on our original UMAP embedding.\n\n\nCode\nDimPlot(pbmc, group.by = \"celltype\", pt.size = 1) + \n  scale_color_paletteer_d(\"ggsci::default_nejm\") + \n  labs(x = \"UMAP 1\", \n       y = \"UMAP 2\", \n       color = \"Celltype\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.title = element_blank())"
  },
  {
    "objectID": "tutorials/scaffold_simulation.html",
    "href": "tutorials/scaffold_simulation.html",
    "title": "Simulating scRNA-seq Data with {scaffold}",
    "section": "",
    "text": "Introduction\n\n\nLibraries\n\n\nData"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html",
    "href": "tutorials/scLANE_Trajectory_DE.html",
    "title": "Interpretable scRNA-seq Trajectory DE with {scLANE}",
    "section": "",
    "text": "In this tutorial we’ll walk through a basic trajectory differential expression analysis. We’ll use the {scLANE} package, which we developed with the goal of providing accurate and biologically interpretable models of expression over the course of a biological process. At the end are a list of references we used in developing the method & writing the accompanying manuscript, as well as the poster I presented at ENAR 2023 in Nashville."
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#pseudotime-estimation",
    "href": "tutorials/scLANE_Trajectory_DE.html#pseudotime-estimation",
    "title": "Interpretable scRNA-seq Trajectory DE with {scLANE}",
    "section": "Pseudotime Estimation",
    "text": "Pseudotime Estimation\nWe’ll start by fitting a trajectory using the {slingshot} R package. We define cluster 4 as the starting cluster. After generating the estimates for each cell, we rescale the ordering to be defined on \\([0, 1]\\). This has no effect on the trajectory DE, and is mostly an aesthetic choice.\n\n\nCode\nsling_res <- slingshot(sce, \n                       clusterLabels = \"label\", \n                       reducedDim = \"PCAsub\", \n                       start.clus = \"4\", \n                       approx_points = 1000)\nsling_pt <- slingPseudotime(sling_res) %>% \n            as.data.frame() %>% \n            magrittr::set_colnames(c(\"PT\")) %>% \n            mutate(PT = PT / max(PT))\n\n\nLet’s visualize the results on our UMAP embedding. They match what we would expect (knowing the biological background of the data), with ductal cells at the start of the process and endocrine celltypes such as alpha, beta, & delta cells at the end of it.\n\n\nCode\np3 <- reducedDim(sce, \"UMAP\") %>% \n      as.data.frame() %>% \n      magrittr::set_colnames(c(\"UMAP_1\", \"UMAP_2\")) %>% \n      mutate(PT = sling_pt$PT) %>% \n      ggplot(aes(x = UMAP_1, y = UMAP_2, color = PT)) + \n      geom_point(size = 1, alpha = 0.75) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\", color = \"Estimated\\nPseudotime\") + \n      scale_color_gradientn(colors = palette_heatmap, \n                            labels = scales::label_number(accuracy = 0.1)) + \n      theme_umap()\np4 <- (p3 / p1) + plot_annotation(title = \"Estimated Cell Ordering from Slingshot\", \n                                  theme = theme(plot.title = element_text(face = \"bold\")))\np4"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#trajectory-differential-expression",
    "href": "tutorials/scLANE_Trajectory_DE.html#trajectory-differential-expression",
    "title": "Interpretable scRNA-seq Trajectory DE with {scLANE}",
    "section": "Trajectory Differential Expression",
    "text": "Trajectory Differential Expression\nNext, we prepare the primary inputs to {scLANE}: a dense counts matrix (with cells as rows and genes as columns - this is important), a dataframe containing our estimated pseudotime ordering, and a character vector of the genes that we’re interested in modeling. We parallelize over genes in order to speed up the computation at the expense of using a little more memory. The models are fit using NB GLMs with optimal spline knots identified empirically, and differential expression is quantified using a likelihood ratio test of the fitted model vs. the null (intercept-only) model. We take a stratified sample of 100 genes each from the set of HVGs we identified earlier as well as the set of non-HVGs.\n\n\nCode\nset.seed(312)\ngene_vec <- c(sample(rownames(sce)[-which(rownames(sce) %in% top2k_hvgs)], 100, replace = FALSE), \n              sample(top2k_hvgs, 100, replace = FALSE))\nraw_counts <- as.matrix(t(counts(sce)))[, gene_vec]\nscLANE_res <- testDynamic(expr.mat = raw_counts, \n                          pt = sling_pt, \n                          genes = gene_vec, \n                          n.potential.basis.fns = 4, \n                          parallel.exec = TRUE, \n                          n.cores = 3, \n                          track.time = TRUE)\nscLANE_res_tidy <- getResultsDE(test.dyn.results = scLANE_res)\n\n\n[1] \"testDynamic evaluated 200 genes with 1 lineages apiece in 1.555 mins\"\n\n\nWe pull the top 10 most significant DE genes (the results table is already sorted by adjusted \\(p\\)-value) from the results & display their test statistics.\n\n\nCode\nselect(scLANE_res_tidy, \n       Gene, \n       Test_Stat, \n       P_Val, \n       P_Val_Adj,\n       Gene_Dynamic_Overall) %>% \n  mutate(Gene_Dynamic_Overall = if_else(Gene_Dynamic_Overall == 1, \"Dynamic\", \"Not Dynamic\")) %>% \n  slice_head(n = 10) %>% \n  kableExtra::kbl(digits = 3, \n                  booktabs = TRUE, \n                  caption = \"Top 10 DE Genes\", \n                  col.names = c(\"Gene\", \"LRT Statistic\", \"P-value\", \"Adj. P-value\", \"Predicted Gene Status\")) %>% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\nTop 10 DE Genes\n \n  \n    Gene \n    LRT Statistic \n    P-value \n    Adj. P-value \n    Predicted Gene Status \n  \n \n\n  \n    Cnih2 \n    1550.572 \n    0 \n    0 \n    Dynamic \n  \n  \n    Tspan7 \n    2552.258 \n    0 \n    0 \n    Dynamic \n  \n  \n    Rpl9 \n    2998.102 \n    0 \n    0 \n    Dynamic \n  \n  \n    Ptn \n    2289.438 \n    0 \n    0 \n    Dynamic \n  \n  \n    Anp32b \n    1685.980 \n    0 \n    0 \n    Dynamic \n  \n  \n    Eno1 \n    2571.359 \n    0 \n    0 \n    Dynamic \n  \n  \n    Gsta3 \n    3511.756 \n    0 \n    0 \n    Dynamic \n  \n  \n    Tpm1 \n    1593.841 \n    0 \n    0 \n    Dynamic \n  \n  \n    Neurog3 \n    2398.567 \n    0 \n    0 \n    Dynamic \n  \n  \n    Ypel3 \n    1848.632 \n    0 \n    0 \n    Dynamic \n  \n\n\n\n\n\nWith the output from testDynamic() in hand we can use the plotModels() function to visualize the fitted models from {scLANE} and compare them to other modeling methods. We see that the {scLANE} model output shows essentially the same trend as the GAM output for a gene of interest, with the advantage of being a much more straightforwardly interpretable model.\n\n\nCode\np5 <- plotModels(test.dyn.res = scLANE_res, \n                 gene = \"Gsta3\", \n                 pt = sling_pt, \n                 gene.counts = raw_counts) + \n      scale_color_manual(values = c(\"forestgreen\"))\np5\n\n\n\n\n\nWe can check out the actual regression output for our gene of interest as well.\n\n\nCode\nscLANE_res$Gsta3$Lineage_A$MARGE_Summary %>% \n  kableExtra::kbl(digits = 3, \n                  booktabs = TRUE, \n                  caption = \"Model Output for Gsta3\", \n                  col.names = c(\"Hinge Function\", \"Coefficient\", \"Std. Error\", \"T-statistic\", \"P-value\")) %>% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\nModel Output for Gsta3\n \n  \n    Hinge Function \n    Coefficient \n    Std. Error \n    T-statistic \n    P-value \n  \n \n\n  \n    B_finalIntercept \n    2.710 \n    0.263 \n    10.311 \n    0 \n  \n  \n    B_final(0.5-Lineage_A) \n    -2.726 \n    0.582 \n    -4.683 \n    0 \n  \n  \n    B_final(Lineage_A-0.07) \n    -9.946 \n    0.605 \n    -16.436 \n    0 \n  \n\n\n\n\n\nLastly, let’s cluster the set of genes we fit models for using the clusterGenes() function. This function supports several different clustering algorithms, and assigns genes to clusters based on their fitted values as generated internally in testDynamic(). Next, the plotClusteredGenes() function generates the data we need to plot the clustered fitted values, which we then use to create our visualization.\n\n\nCode\ngene_clusters <- clusterGenes(test.dyn.results = scLANE_res, \n                              clust.algo = \"leiden\", \n                              use.pca = TRUE)\ngene_fitted_vals <- plotClusteredGenes(test.dyn.results = scLANE_res, \n                                       gene.clusters = gene_clusters, \n                                       pt = sling_pt)\n\n\nVisualizing the clustered gene models allows us to identify groups of similarly-acting genes, and can also help identify genes of interest based on our existing biological hypotheses.\n\n\nCode\np6 <- ggplot(gene_fitted_vals, aes(x = PT, y = FITTED, color = CLUSTER, group = GENE)) + \n      facet_wrap(~paste0(\"Lineage \", LINEAGE) + CLUSTER) + \n      geom_line() + \n      labs(x = \"Pseudotime\", \n           y = \"Fitted Values\", \n           color = \"Leiden\\nCluster\", \n           title = \"Unsupervised Clustering of Gene Patterns\") + \n      theme_classic(base_size = 14) + \n      guide_umap()\np6"
  },
  {
    "objectID": "knowledge_base/R_Development.html",
    "href": "knowledge_base/R_Development.html",
    "title": "R Development Resources",
    "section": "",
    "text": "Introduction\nThis document houses a list of resources I’ve put together that have helped me in my journey from novice R user to experienced (kinda) R package developer. Most of the resources will be centered around developing packages specifically, but some pertain to other types of R projects.\n\n\nPackage Development\n\nR Packages\n\nThe official R package development book, written by Hadley Wickham & Jenny Bryan. Contains a startup guide and sections on metadata, dependencies, unit testing, and documentation. Good comprehensive resource, but not the quickest way to get up & running. This package does have non-R dependencies, but they’re free & easy to install.\n\nThe {usethis} package\n\nThis R package removes a lot of headaches from setting up a new R package or other project. It contains functions to create a new R package, set up continuous integration (CI) tools, connect your package to a GitHub repository, and much more. Essentially, it removes a lot of the point-and-click steps that used to be necessary to set up an R package & get it version-controlled, and generally just makes development a lot easier.\n\nWriting R Extensions\n\nThis book, written by the R Core Team, is a lower-level guide to creating R packages, writing R documentation, debugging, linking R with C / C++, and other advanced topics. It’s useful if you’re getting deeper into package development & already have a very solid handle on the basics.\n\n\n\n\nComputational Efficiency\n\nCode profiling with {proffer}\n\nCode profiling allows you to see a breakdown in graph or table form of which functions in a piece of code are taking the longest to execute. This is done at varying hierarchical levels of resolution, which means you can see that e.g., a function you wrote called run_analysis() is taking up 50% of your computation time, and that within that function the main culprit is the subfunction load_large_dataset(). As such, it becomes very simple & quick to identify the best targets for further optimization of your code with respect to runtime.\n\nBenchmarking runtime using {microbenchmark}\n\nThis package is a very light benchmarking utility. It allows you to compare a list of different functions’ runtime by executing each function a given number of times e.g., 100 times, and returning the distribution of runtime for each function. This makes it very easy to quickly see which implementation of a given task is better with respect to runtime. The source repository can be found here.\n\n\n\n\nReproducible Research\n\nReproducible pipelines in R using {targets}\n\nThe {targets} package is one of my favorite R tools, & the well-written docs above show how to create version-controlled pipelines entirely using R. This framework is an absolute godsend for large projects, simulation studies, etc., and I’ve used it on every longterm computational project I’ve worked on in the past 2 years. It makes tracking, reproducing, & parallelizing the execution of large codebases very easy, and makes reproducible research accessible to anyone with a good handle on R.\n\nScientific writing via quarto\n\nquarto is a more fully-featured successor to RMarkdown, and enables the user to combine code, Markdown-formatted text, images, equations, etc. in a single document. Citations via supported as well, which makes technical writing relatively easy. There is support for LaTeX, which is great for homework assignments, proofs, and Methods sections. In addition, you can use quarto to create websites, books, presentations, and more. This site itself is actually run using quarto. I’d highly recommend it over RMarkdown at this point, both for its breadth of features & its expanded support of languages (Python, Julia, R, etc.) & IDEs (RStudio, VS Code, Jupyter notebooks). Full documentation can be found here.\n\nHow to control stochasticity when processing in parallel\n\nThis presentation describes how to produce reproducible random number streams when using one of R’s several parallel processing frameworks. It’s pretty applied (not theoretical), and several useful code examples are shown.\n\n\n\n\nDevelopment Best Practices\n\nWhat They Forgot to Teach You About R\n\nThis online book is less about package development & more about R’s idiosyncrasies, but it contains a bunch of useful tips & tricks on how to make your code more reproducible & less brittle. The debugging section is clear & concise, and contains links to other, more detailed resources as well.\n\n\n\n\nMiscellaneous\n\nThe R Inferno\n\nThis amusingly-titled & engagingly-written book covers a variety of oddities & frustrating idiosyncrasies that R has, many of which are holdovers from when R was being first developed. If you’re having an annoyingly difficult low-level problem, this book might have the answer.\n\nradian: an improved R console\n\nIf you often use R in the terminal, it can be a frustrating experience when compared to all the features (syntax highlighting, autocomplete, bracket-matching, etc.) that RStudio has. radian is a command line tool that adds all these aesthetic features to your R console. Simply install the Python-based library, then start calling radian instead of R when using R from the terminal."
  },
  {
    "objectID": "knowledge_base/scRNAseq.html",
    "href": "knowledge_base/scRNAseq.html",
    "title": "scRNA-seq Resources",
    "section": "",
    "text": "Here I’ll catalog useful papers, preprints, method vignettes, Twitter discussions, etc. that I’ve found helpful while learning how to process and analyze single cell RNA-seq data. When possible, I’ll link to static versions of things in the hopes that links don’t break. I’ll categorize resources according to which problem they address e.g., raw data processing, clustering, visualization, etc., though some resources will of course touch on multiple topics."
  },
  {
    "objectID": "knowledge_base/scRNAseq.html#via-pseudotime",
    "href": "knowledge_base/scRNAseq.html#via-pseudotime",
    "title": "scRNA-seq Resources",
    "section": "Via Pseudotime",
    "text": "Via Pseudotime\n\nSlingshot: cell lineage and pseudotime inference for single-cell transcriptomics\n\nThe {slingshot} package is my currently preferred method for estimating a pseudotemporal cellular ordering. A decent vignette can be found here. I would absolutely recommend using principal components as input to the algorithm instead of UMAP / t-SNE components.\n\nTrajectory-based differential expression analysis for single-cell sequencing data\n\nThis paper describes the development of the {tradeSeq} R package, which uses generalized additive models (GAMs) to perform differential expression over an inferred cellular trajectory. The package has some limitations, but provides a variety of different tests of different patterns of gene expression, and overall strikes a good balance between running quickly & providing accurate results. A good vignette can be accessed here. A nice characteristic of the method is that it is agnostic with respect to the type of pseudotime estimation used, meaning the user can derive their cellular ordering using any pseudotime or RNA velocity method prior to running {tradeSeq}."
  },
  {
    "objectID": "knowledge_base/scRNAseq.html#via-rna-velocity",
    "href": "knowledge_base/scRNAseq.html#via-rna-velocity",
    "title": "scRNA-seq Resources",
    "section": "Via RNA Velocity",
    "text": "Via RNA Velocity\n\nRNA velocity unravaled\nOn the mathematics of RNA velocity I: Theoretical analysis"
  }
]