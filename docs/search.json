[
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html",
    "href": "tutorials/scLANE_Trajectory_DE.html",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "",
    "text": "In this tutorial we’ll walk through a basic trajectory differential expression analysis. We’ll use the scLANE R package, which we developed with the goal of providing accurate and biologically interpretable models of expression over pseudotime. At the end are some best-practices recommendations, along with a short list of references we used in developing the method & writing the accompanying manuscript. If you want to skip the preprocessing steps and get right into the analysis, head to Section 5."
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#conversion-from-python",
    "href": "tutorials/scLANE_Trajectory_DE.html#conversion-from-python",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "4.1 Conversion from Python",
    "text": "4.1 Conversion from Python\nThe reticulate package allows us to pass the counts matrices & metadata from Python back to R.\n\n\nCode\nspliced_counts = adata.layers['spliced'].toarray()\nunspliced_counts = adata.layers['unspliced'].toarray()\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile downloading this dataset requires a Python installation as well as the installation of the scVelo Python library (and its dependencies), running scLANE is done purely in R & requires no Python whatsoever.\n\n\nWe’ll use the spliced mRNA counts as our default assay, and also define a new assay containing the total (spliced + unspliced) mRNA in each cell. Lastly, we remove genes with non-zero spliced mRNA in 3 or fewer cells.\n\n\nCode\nspliced_counts &lt;- Matrix::Matrix(t(py$spliced_counts), sparse = TRUE)\nunspliced_counts &lt;- Matrix::Matrix(t(py$unspliced_counts), sparse = TRUE)\nrna_counts &lt;- spliced_counts + unspliced_counts\ncolnames(rna_counts) &lt;- colnames(spliced_counts) &lt;- colnames(unspliced_counts) &lt;- py$adata$obs_names$to_list()\nrownames(rna_counts) &lt;- rownames(spliced_counts) &lt;- rownames(unspliced_counts) &lt;- py$adata$var_names$to_list()\nspliced_assay &lt;- CreateAssayObject(counts = spliced_counts)\nspliced_assay@key &lt;- \"spliced_\"\nunspliced_assay &lt;- CreateAssayObject(counts = unspliced_counts)\nunspliced_assay@key &lt;- \"unspliced_\"\nrna_assay &lt;- CreateAssayObject(counts = rna_counts)\nrna_assay@key &lt;- \"rna_\"\nmeta_data &lt;- py$adata$obs %&gt;% \n             mutate(cell_name = rownames(.), \n                    .before = 1) %&gt;% \n             rename(celltype = clusters, \n                    celltype_coarse = clusters_coarse) %&gt;% \n             mutate(nCount_spliced = colSums(spliced_counts), \n                    nFeature_spliced = colSums(spliced_counts &gt; 0), \n                    nCount_unspliced = colSums(unspliced_counts), \n                    nFeature_unspliced = colSums(unspliced_counts &gt; 0), \n                    nCount_RNA = colSums(rna_counts), \n                    nFeature_RNA = colSums(rna_counts &gt; 0))\nseu &lt;- CreateSeuratObject(counts = spliced_assay, \n                          assay = \"spliced\", \n                          project = \"Mm_Panc_Endo\", \n                          meta.data = meta_data)\nseu@assays$unspliced &lt;- unspliced_assay\nseu@assays$RNA &lt;- rna_assay\nseu &lt;- seu[rowSums(seu@assays$spliced) &gt; 3, ]"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#preprocessing",
    "href": "tutorials/scLANE_Trajectory_DE.html#preprocessing",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "4.2 Preprocessing",
    "text": "4.2 Preprocessing\nWe preprocess the counts using a typical pipeline with QC, normalization & scaling, dimension reduction, and graph-based clustering via the Leiden algorithm.\n\n\nCode\nseu &lt;- PercentageFeatureSet(seu, \n                            pattern = \"^mt-\", \n                            col.name = \"percent_mito\", \n                            assay = \"spliced\") %&gt;% \n       PercentageFeatureSet(pattern = \"^Rp[sl]\", \n                            col.name = \"percent_ribo\", \n                            assay = \"spliced\") %&gt;% \n       NormalizeData(assay = \"spliced\", verbose = FALSE) %&gt;% \n       NormalizeData(assay = \"unspliced\", verbose = FALSE) %&gt;% \n       NormalizeData(assay = \"RNA\", verbose = FALSE) %&gt;% \n       FindVariableFeatures(assay = \"spliced\", \n                            nfeatures = 3000, \n                            verbose = FALSE) %&gt;% \n       ScaleData(assay = \"spliced\", \n                 vars.to.regress = c(\"percent_mito\", \"percent_ribo\"), \n                 model.use = \"poisson\", \n                 verbose = FALSE) %&gt;% \n       RunPCA(assay = \"spliced\", \n              npcs = 30, \n              approx = TRUE, \n              seed.use = 312, \n              verbose = FALSE) %&gt;% \n       RunUMAP(reduction = \"pca\", \n               dims = 1:30, \n               n.components = 2, \n               metric = \"cosine\", \n               seed.use = 312, \n               verbose = FALSE) %&gt;% \n       FindNeighbors(reduction = \"pca\", \n                     k.param = 30,\n                     nn.method = \"annoy\", \n                     annoy.metric = \"cosine\", \n                     verbose = FALSE) %&gt;% \n       FindClusters(algorithm = 4, \n                    method = \"igraph\", \n                    resolution = 0.5, \n                    random.seed = 312, \n                    verbose = FALSE)\n\n\nLet’s visualize the results on our UMAP embedding. The clustering generally agrees with the celltype labels, though there is some overclustering in the ductal cells & underclustering in the mature endocrine celltypes.\n\n\nCode\np0 &lt;- Embeddings(seu, \"umap\") %&gt;% \n      as.data.frame() %&gt;% \n      magrittr::set_colnames(c(\"UMAP_1\", \"UMAP_2\")) %&gt;% \n      mutate(leiden = seu$seurat_clusters) %&gt;% \n      ggplot(aes(x = UMAP_1, y = UMAP_2, color = leiden)) + \n      geom_point(size = 1.5, \n                 alpha = 0.75, \n                 stroke = 0) + \n      scale_color_manual(values = palette_cluster) + \n      labs(color = \"Leiden Cluster\") + \n      theme_scLANE(umap = TRUE) + \n      theme(plot.title = element_blank(), \n            axis.title = element_blank(), \n            axis.line.x = element_blank()) + \n      guide_umap()\np1 &lt;- Embeddings(seu, \"umap\") %&gt;% \n      as.data.frame() %&gt;% \n      magrittr::set_colnames(c(\"UMAP_1\", \"UMAP_2\")) %&gt;% \n      mutate(celltype = seu$celltype) %&gt;% \n      ggplot(aes(x = UMAP_1, y = UMAP_2, color = celltype)) + \n      geom_point(size = 1.5, \n                 alpha = 0.75, \n                 stroke = 0) + \n      scale_color_manual(values = palette_celltype) + \n      labs(x = \"UMAP 1\", \n           y = \"UMAP 2\", \n           color = \"Celltype\") + \n      theme_scLANE(umap = TRUE) + \n      theme(plot.title = element_blank()) + \n      guide_umap()\np2 &lt;- (p0 / p1) +\n      plot_layout(guides = \"collect\")\np2\n\n\n\n\n\n\n\n\nFigure 1: Cluster & celltype labels in UMAP space"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#pseudotime-estimation",
    "href": "tutorials/scLANE_Trajectory_DE.html#pseudotime-estimation",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "5.1 Pseudotime estimation",
    "text": "5.1 Pseudotime estimation\nWe’ll start by fitting a trajectory using the slingshot R package. We define cluster 5 as the starting cluster, since in this case we’re already aware of the dataset’s underlying biology. After generating the estimates for each cell, we rescale the ordering to be defined on \\([0, 1]\\). This has no effect on the trajectory DE results however, and is mostly an aesthetic choice.\n\n\nCode\nsling_res &lt;- slingshot(Embeddings(seu, \"umap\"),\n                       start.clus = \"5\",\n                       clusterLabels = seu$seurat_clusters, \n                       approx_points = 500)\nsling_pt &lt;- slingPseudotime(sling_res) %&gt;% \n            as.data.frame() %&gt;% \n            magrittr::set_colnames(c(\"PT\")) %&gt;% \n            mutate(PT = (PT - min(PT)) / (max(PT) - min(PT)))\nseu &lt;- AddMetaData(seu, \n                   metadata = sling_pt, \n                   col.name = \"sling_pt\")\n\n\nLet’s visualize the results on our UMAP embedding. They match what we would expect (knowing the biological background of the data), with ductal cells at the start of the process and endocrine celltypes such as alpha, beta, & delta cells at the end of it.\n\n\nCode\np3 &lt;- Embeddings(seu, \"umap\") %&gt;% \n      as.data.frame() %&gt;% \n      magrittr::set_colnames(c(\"UMAP_1\", \"UMAP_2\")) %&gt;% \n      mutate(PT = sling_pt$PT) %&gt;% \n      ggplot(aes(x = UMAP_1, y = UMAP_2, color = PT)) + \n      geom_point(size = 1.5, \n                 alpha = 0.75, \n                 stroke = 0) + \n      labs(color = \"Pseudotime\") + \n      scale_color_gradientn(colors = palette_heatmap, \n                            labels = scales::label_number(accuracy = 0.01)) + \n      theme_scLANE(umap = TRUE) +\n      theme(axis.title = element_blank(), \n            axis.line.x = element_blank())\np4 &lt;- (p3 / p1) + \n      plot_layout(guides = \"collect\")\np4\n\n\n\n\n\n\n\n\nFigure 2: Estimated pseudotime in UMAP space"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#trajectory-de-testing",
    "href": "tutorials/scLANE_Trajectory_DE.html#trajectory-de-testing",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "5.2 Trajectory DE testing",
    "text": "5.2 Trajectory DE testing\nNext, we prepare the primary inputs to scLANE: our Seurat object with the spliced counts set as the default assay, a dataframe containing our estimated pseudotime ordering, a vector of size factors to use as an offset in each model, and a set of genes whose dynamics we want to model. scLANE parallelizes over genes in order to speed up the computation at the expense of using a little more memory. The models are fit using NB GLMs with optimal spline knots identified empirically, and differential expression is quantified using a likelihood ratio test of the fitted model vs. a constant (intercept-only) model. In practice, genes designated as HVGs are usually the best candidates for modeling, so we choose the top 3,000 HVGs as our input.\n\n\n\n\n\n\nNote\n\n\n\nThe testing of the HVG set on its own is also justified by the reality that almost all trajectories are inferred using some sort of dimension-reduced space, and those embeddings are nearly universally generated using a set of HVGs. As such, genes not included in the HVG set actually have no direct relationship with the estimated trajectory, & it’s generally safe to exclude them from trajectory analyses.\n\n\n\n\nCode\ntop3k_hvg &lt;- HVFInfo(seu) %&gt;% \n             arrange(desc(variance.standardized)) %&gt;% \n             slice_head(n = 3000) %&gt;% \n             rownames(.)\ncell_offset &lt;- createCellOffset(seu)\nscLANE_models &lt;- testDynamic(seu, \n                             pt = sling_pt, \n                             genes = top3k_hvg, \n                             size.factor.offset = cell_offset, \n                             n.cores = 6L, \n                             verbose = FALSE)\n\n\nRegistered S3 method overwritten by 'bit':\n  method   from  \n  print.ri gamlss\n\n\nscLANE testing completed for 3000 genes across 1 lineage in 18.028 mins\n\n\nCode\nscLANE_res_tidy &lt;- getResultsDE(scLANE_models)\n\n\nAfter tidying the TDE results with getResultsDE(), we pull a sample of 6 genes from the results & display their test statistics. By default, any gene with an adjusted p-value less than 0.01 is predicted to be dynamic, though this threshold can be easily adjusted.\n\n\nCode\nselect(scLANE_res_tidy, \n       Gene, \n       Test_Stat, \n       P_Val, \n       P_Val_Adj,\n       Gene_Dynamic_Overall) %&gt;% \n  mutate(Gene_Dynamic_Overall = if_else(Gene_Dynamic_Overall == 1, \"Dynamic\", \"Static\")) %&gt;% \n  with_groups(Gene_Dynamic_Overall, \n              slice_sample, \n              n = 3) %&gt;% \n  kableExtra::kbl(digits = 4, \n                  booktabs = TRUE, \n                  col.names = c(\"Gene\", \"LRT stat.\", \"P-value\", \"Adj. p-value\", \"Predicted gene status\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nGene\nLRT stat.\nP-value\nAdj. p-value\nPredicted gene status\n\n\n\n\nRipply3\n997.8892\n0.0000\n0.0000\nDynamic\n\n\nDusp3\n33.0268\n0.0000\n0.0000\nDynamic\n\n\nHdc\n30.8371\n0.0000\n0.0001\nDynamic\n\n\n2700016F22Rik\n2.4098\n0.1206\n1.0000\nStatic\n\n\nTlr5\nNA\nNA\nNA\nStatic\n\n\nEgln3\n15.5168\n0.0004\n0.1401\nStatic\n\n\n\n\n\n\n\n\nTable 1: TDE test results from scLANE\n\n\n\n\nNext, we can use the plotModels() function to visualize the fitted models from scLANE and compare them to other modeling methods. The gene Neurog3 is strongly associated with epithelial cell differentiation, and indeed we see a clear, nonlinear transcriptional dynamic across pseudotime for that gene. A traditional GLM fails to capture that nonlinearity, and while a GAM fits the trend smoothly, it seems to overfit the dynamics near the boundaries of pseudotime - a known issue with additive models. Only the scLANE model accurately models the rapid upregulation and equally swift downregulation of the transcription factor neurogenin-3 (Neurog3) over pseudotime, in addition to identifying the cutpoint in pseudotime at which downregulation begins.\n\n\nCode\np5 &lt;- plotModels(scLANE_models, \n                 gene = \"Neurog3\", \n                 pt = sling_pt, \n                 expr.mat = seu, \n                 size.factor.offset = cell_offset, \n                 plot.glm = TRUE, \n                 plot.gam = TRUE) + \n      scale_color_manual(values = c(\"forestgreen\"))\np5\n\n\n\n\n\n\n\n\nFigure 3: Modeling framework comparison"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#gene-dynamics-plots",
    "href": "tutorials/scLANE_Trajectory_DE.html#gene-dynamics-plots",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "6.1 Gene dynamics plots",
    "text": "6.1 Gene dynamics plots\nUsing the getFittedValues() function allows us to generate predictions from the models we fit, which we then use to visualize the dynamics of a few genes that are known to be strongly associated with the differentiation of immature progenitors into mature endocrine phenotypes (source 1, source 2). For all four genes, the fitted models show knots chosen in the area of pseudotime around the pre-endocrine cells. This tells us that these driver genes are being upregulated in precursor celltypes & are driving differentiation into the mature celltypes such as alpha & beta cells, after which the genes are downregulated.\n\n\nCode\np6 &lt;- getFittedValues(scLANE_models, \n                      genes = c(\"Chga\", \"Chgb\", \"Fev\", \"Cck\"), \n                      pt = sling_pt, \n                      expr.mat = seu, \n                      size.factor.offset = cell_offset, \n                      cell.meta.data = select(seu@meta.data, celltype, celltype_coarse)) %&gt;% \n      ggplot(aes(x = pt, y = rna_log1p)) + \n      facet_wrap(~gene, \n                 ncol = 2, \n                 scales = \"free_y\") + \n      geom_point(aes(color = celltype), \n                 size = 2, \n                 alpha = 0.75, \n                 stroke = 0) + \n      geom_vline(data = data.frame(gene = \"Chga\", knot = unique(scLANE_models$Chga$Lineage_A$MARGE_Slope_Data$Breakpoint)), \n                 mapping = aes(xintercept = knot), \n                 linetype = \"dashed\", \n                 color = \"grey20\") + \n      geom_vline(data = data.frame(gene = \"Chgb\", knot = unique(scLANE_models$Chgb$Lineage_A$MARGE_Slope_Data$Breakpoint)), \n                 mapping = aes(xintercept = knot), \n                 linetype = \"dashed\", \n                 color = \"grey20\") + \n      geom_vline(data = data.frame(gene = \"Cck\", knot = unique(scLANE_models$Cck$Lineage_A$MARGE_Slope_Data$Breakpoint)), \n                 mapping = aes(xintercept = knot), \n                 linetype = \"dashed\", \n                 color = \"grey20\") + \n      geom_vline(data = data.frame(gene = \"Fev\", knot = unique(scLANE_models$Fev$Lineage_A$MARGE_Slope_Data$Breakpoint)), \n                 mapping = aes(xintercept = knot), \n                 linetype = \"dashed\", \n                 color = \"grey20\") + \n      geom_ribbon(aes(ymin = scLANE_ci_ll_log1p, ymax = scLANE_ci_ul_log1p), \n                  linewidth = 0, \n                  fill = \"grey70\", \n                  alpha = 0.9) + \n      geom_line(aes(y = scLANE_pred_log1p), \n                color = \"black\", \n                linewidth = 0.75) + \n      scale_x_continuous(labels = scales::label_number(accuracy = 0.01)) + \n      scale_color_manual(values = palette_celltype) + \n      labs(x = \"Pseudotime\", \n           y = \"Normalized Expression\") + \n      theme_scLANE() + \n      theme(legend.title = element_blank(), \n            strip.text.x = element_text(face = \"italic\")) + \n      guide_umap()\np6\n\n\n\n\n\n\n\n\nFigure 4: scLANE models of endocrinogenesis drivers\n\n\n\n\n\nOn the other hand, if we use additive models the “peak” of expression is placed among the mature endocrine celltypes - which doesn’t make biological sense if we know that these genes are driving that process of differentiation. This can of course be tweaked by changing the degree or degrees of freedom of the underlying basis spline, but choosing a “best” value for those hyperparameters can be difficult, whereas scLANE identifies optimal parameters internally by default. In addition, the knots chosen by scLANE for each gene can be informative with respect to the underlying biology, whereas the knots from GAMs are evenly spaced at quantiles & carry no biological interpretation.\n\n\nCode\np7 &lt;- getFittedValues(scLANE_models, \n                      genes = c(\"Chga\", \"Chgb\", \"Fev\", \"Cck\"), \n                      pt = sling_pt, \n                      expr.mat = seu, \n                      size.factor.offset = cell_offset, \n                      cell.meta.data = select(seu@meta.data, celltype, celltype_coarse)) %&gt;% \n      mutate(rna_raw = rna / size_factor, .before = 7) %&gt;% \n      with_groups(gene, \n                  mutate, \n                  GAM_fitted_link = predict(nbGAM(expr = rna_raw, \n                                                  pt = sling_pt, \n                                                  Y.offset = cell_offset, \n                                                  spline.df = 3)), \n                  GAM_se_link = predict(nbGAM(expr = rna_raw, \n                                              pt = sling_pt, \n                                              Y.offset = cell_offset, \n                                              spline.df = 3), se.fit = TRUE)[[2]]) %&gt;% \n      mutate(GAM_pred = exp(GAM_fitted_link) * cell_offset, \n             GAM_ci_ll = exp(GAM_fitted_link - qnorm(0.975) * GAM_se_link) * cell_offset, \n             GAM_ci_ul = exp(GAM_fitted_link + qnorm(0.975) * GAM_se_link) * cell_offset, \n             GAM_pred_log1p = log1p(GAM_pred), \n             GAM_ci_ll_log1p = log1p(GAM_ci_ll), \n             GAM_ci_ul_log1p = log1p(GAM_ci_ul)) %&gt;% \n      ggplot(aes(x = pt, y = rna_log1p)) + \n      facet_wrap(~gene, \n                 ncol = 2, \n                 scales = \"free_y\") + \n      geom_point(aes(color = celltype), \n                 size = 2, \n                 alpha = 0.75, \n                 stroke = 0) + \n      geom_ribbon(aes(ymin = GAM_ci_ll_log1p, ymax = GAM_ci_ul_log1p), \n                  linewidth = 0, \n                  fill = \"grey70\", \n                  alpha = 0.9) + \n      geom_line(aes(y = GAM_pred_log1p), \n                color = \"black\", \n                linewidth = 0.75) + \n      scale_x_continuous(labels = scales::label_number(accuracy = 0.01)) + \n      scale_color_manual(values = palette_celltype) + \n      labs(x = \"Pseudotime\", \n           y = \"Normalized Expression\") + \n      theme_scLANE() + \n      theme(legend.title = element_blank(), \n            strip.text.x = element_text(face = \"italic\")) + \n      guide_umap()\np7\n\n\n\n\n\n\n\n\nFigure 5: Additive models of endocrinogenesis drivers"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#distribution-of-knot-locations",
    "href": "tutorials/scLANE_Trajectory_DE.html#distribution-of-knot-locations",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "6.2 Distribution of knot locations",
    "text": "6.2 Distribution of knot locations\nLet’s take a broader view of the dataset by examining the distribution of adaptively chosen knots from our models. We limit the analysis to the set of 2444 genes determined to be dynamic.\n\n\nCode\ndyn_genes &lt;- filter(scLANE_res_tidy, Gene_Dynamic_Overall == 1) %&gt;% \n             pull(Gene)\nknot_df &lt;- getKnotDist(scLANE_models, dyn_genes)\n\n\nWe’ll plot a histogram of the knot values along with a ridgeplot of the pseudotime distribution for each celltype. We see that a large number of the selected knots are placed at the beginning of the trajectory, around where the ductal cells transition into endocrine progenitors. A smaller set of knots is placed about halfway through the trajectory, which we’ve annotated as the point at which pre-endocrine cells begin differentiating into mature endocrine phenotypes.\n\n\nCode\np8 &lt;- ggplot(knot_df, aes(x = knot)) + \n      geom_histogram(aes(y = after_stat(density)), \n                     color = \"black\", \n                     fill = \"white\", \n                     linewidth = 0.5) + \n      geom_density(fill = \"deepskyblue3\", \n                   alpha = 0.5, \n                   color = \"deepskyblue4\", \n                   linewidth = 1) + \n      scale_x_continuous(limits = c(0, 1), labels = scales::label_number(accuracy = 0.01)) + \n      theme_scLANE() + \n      theme(axis.title = element_blank(), \n            axis.text = element_blank(), \n            axis.ticks.y = element_blank())\np9 &lt;- data.frame(celltype = seu$celltype, \n                 pt = seu$sling_pt) %&gt;% \n      ggplot(aes(x = pt, y = celltype, fill = celltype, color = celltype)) + \n      ggridges::geom_density_ridges(alpha = 0.75, size = 1, scale = 0.95) + \n      scale_x_continuous(labels = scales::label_number(accuracy = 0.01), limits = c(0, 1)) + \n      scale_fill_manual(values = palette_celltype) + \n      scale_color_manual(values = palette_celltype) + \n      labs(x = \"Pseudotime\") + \n      theme_scLANE() + \n      theme(axis.title.y = element_blank(), \n            legend.title = element_blank()) + \n      guide_umap()\np10 &lt;- (p8 / p9) + \n       plot_layout(heights = c(1/4, 3/4))\np10\n\n\n\n\n\n\n\n\nFigure 6: Distribution of adaptively-chosen knots from scLANE"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#dynamic-gene-clustering",
    "href": "tutorials/scLANE_Trajectory_DE.html#dynamic-gene-clustering",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "6.3 Dynamic gene clustering",
    "text": "6.3 Dynamic gene clustering\nWe can extract a matrix of fitted dynamics using smoothedCountsMatrix(). Next, the embedGenes() function reduces dimensionality with PCA, clusters the genes using the Leiden algorithm, & embeds the genes in two dimensions with UMAP.\n\n\nCode\nsmoothed_counts &lt;- smoothedCountsMatrix(scLANE_models, \n                                        pt = sling_pt, \n                                        genes = dyn_genes, \n                                        size.factor.offset = cell_offset)\ngene_embed &lt;- embedGenes(log1p(smoothed_counts$Lineage_A), resolution.param = 0.2)\n\n\nFirst we’ll visualize the gene clusters on the first two PCs.\n\n\nCode\np11 &lt;- ggplot(gene_embed, aes(x = pc1, y = pc2, color = leiden)) + \n       geom_point(size = 2, \n                  alpha = 0.75, \n                  stroke = 0) + \n       labs(x = \"PC 1\", \n            y = \"PC 2\", \n            color = \"Leiden Cluster\") +\n       paletteer::scale_color_paletteer_d(\"ggsci::default_igv\") + \n       theme_scLANE(umap = TRUE) + \n       guide_umap()\np11\n\n\n\n\n\n\n\n\nFigure 7: Unsupervised clustering of genes in PCA space\n\n\n\n\n\nThe UMAP embedding shows that even with the relatively small number of genes, clear patterns are visible.\n\n\nCode\np12 &lt;- ggplot(gene_embed, aes(x = umap1, y = umap2, color = leiden)) + \n       geom_point(size = 2, \n                  alpha = 0.75, \n                  stroke = 0) + \n       labs(x = \"UMAP 1\", \n            y = \"UMAP 2\", \n            color = \"Gene Cluster\") +\n       paletteer::scale_color_paletteer_d(\"ggsci::default_igv\") + \n       theme_scLANE(umap = TRUE) + \n       guide_umap()\np12\n\n\n\n\n\n\n\n\nFigure 8: Unsupervised clustering of genes in UMAP space"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#expression-cascades",
    "href": "tutorials/scLANE_Trajectory_DE.html#expression-cascades",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "6.4 Expression cascades",
    "text": "6.4 Expression cascades\nWe can also plot a heatmap of the dynamic genes; this requires a bit of setup, for which we’ll use the ComplexHeatmap package. We scale each gene, and clip values to be on \\([-6, 6]\\). The columns (cells) of the heatmap are ordered by estimated pseudotime, and the rows (genes) are ordered by expression peak.\n\n\nCode\ncol_anno_df &lt;- select(seu@meta.data, \n                      cell_name, \n                      celltype, \n                      sling_pt) %&gt;% \n               mutate(celltype = as.factor(celltype)) %&gt;% \n               arrange(sling_pt)\ngene_order &lt;- sortGenesHeatmap(smoothed_counts$Lineage_A, pt.vec = sling_pt$PT)\nheatmap_mat &lt;- t(scale(smoothed_counts$Lineage_A))\nheatmap_mat[heatmap_mat &gt; 6] &lt;- 6\nheatmap_mat[heatmap_mat &lt; -6] &lt;- -6\ncolnames(heatmap_mat) &lt;- seu$cell_name\nheatmap_mat &lt;- heatmap_mat[, col_anno_df$cell_name]\nheatmap_mat &lt;- heatmap_mat[gene_order, ]\npalette_celltype_hm &lt;- as.character(palette_celltype[1:length(unique(seu$celltype))])\nnames(palette_celltype_hm) &lt;- levels(col_anno_df$celltype)\ncol_anno &lt;- HeatmapAnnotation(Celltype = col_anno_df$celltype, \n                              Pseudotime = col_anno_df$sling_pt, \n                              col = list(Celltype = palette_celltype_hm, \n                                         Pseudotime = circlize::colorRamp2(seq(0, 1, by = 0.25), palette_heatmap)),\n                              show_legend = TRUE, \n                              show_annotation_name = FALSE, \n                              gap = unit(1, \"mm\"), \n                              border = TRUE)\npalette_cluster_hm &lt;- as.character(paletteer::paletteer_d(\"ggsci::default_igv\")[1:length(unique(gene_embed$leiden))])\nnames(palette_cluster_hm) &lt;- as.character(unique(gene_embed$leiden))\nrow_anno &lt;- HeatmapAnnotation(Cluster = as.factor(gene_embed$leiden), \n                              col = list(Cluster = palette_cluster_hm), \n                              show_legend = TRUE, \n                              show_annotation_name = FALSE, \n                              annotation_legend_param = list(title = \"Gene\\nCluster\"), \n                              gap = unit(1, \"mm\"), \n                              border = TRUE, \n                              which = \"row\")\n\n\nThe heatmap shows clear dynamic patterns across pseudotime; these patterns are often referred to as expression cascades, and represent periodic up- and down-regulation of different gene programs during the course of the underlying biological process.\n\n\nCode\nHeatmap(matrix = heatmap_mat, \n        name = \"Spliced\\nmRNA\", \n        col = circlize::colorRamp2(colors = viridis::inferno(50), \n                                   breaks = seq(min(heatmap_mat), max(heatmap_mat), length.out = 50)), \n        cluster_columns = FALSE,\n        width = 12, \n        height = 6, \n        column_title = \"\",\n        cluster_rows = FALSE,\n        top_annotation = col_anno, \n        left_annotation = row_anno, \n        show_column_names = FALSE, \n        show_row_names = FALSE, \n        use_raster = TRUE,\n        raster_by_magick = TRUE, \n        raster_quality = 5)\n\n\n\n\n\n\n\n\nFigure 9: Expression cascades of dynamic genes"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#gene-programs",
    "href": "tutorials/scLANE_Trajectory_DE.html#gene-programs",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "6.5 Gene programs",
    "text": "6.5 Gene programs\nUsing our gene clusters & the gprofiler2 package, we run an enrichment analysis against the biological process (BP) set of gene ontologies. We make sure to order the genes in each cluster by their test statistics by joining to the results table from scLANE.\n\n\nCode\ngene_clust_list &lt;- purrr::map(unique(gene_embed$leiden), \\(x) { \n  filter(gene_embed, leiden == x) %&gt;% \n  inner_join(scLANE_res_tidy, by = c(\"gene\" = \"Gene\")) %&gt;% \n  arrange(desc(Test_Stat)) %&gt;% \n  pull(gene)\n}) \nnames(gene_clust_list) &lt;- paste0(\"Leiden_\", unique(gene_embed$leiden))\nenrich_res &lt;- gprofiler2::gost(gene_clust_list, \n                               organism = \"mmusculus\", \n                               ordered_query = TRUE, \n                               multi_query = FALSE, \n                               sources = \"GO:BP\", \n                               significant = TRUE)\n\n\nA look at the top 5 most-significant GO terms for each gene cluster reveals heterogeneous functionalities across groups of genes.\n\n\nCode\nmutate(enrich_res$result, \n       query = gsub(\"Leiden_\", \"\", query)) %&gt;% \n  rename(cluster = query) %&gt;% \n  with_groups(cluster, \n              slice_head,\n              n = 5) %&gt;% \n  select(cluster, \n         term_name, \n         p_value, \n         term_size, \n         query_size, \n         intersection_size, \n         term_id) %&gt;% \n  kableExtra::kbl(digits = 3, \n                  booktabs = TRUE, \n                  col.names = c(\"Gene Cluster\", \"Term Name\", \"Adj. P-value\", \"Term Size\", \n                                \"Query Size\", \"Intersection Size\", \"Term ID\")) %&gt;% \n  kableExtra::kable_classic(c(\"hover\"), full_width = FALSE)\n\n\n\n\n\n\n\n\nGene Cluster\nTerm Name\nAdj. P-value\nTerm Size\nQuery Size\nIntersection Size\nTerm ID\n\n\n\n\n0\nregulation of multicellular organismal development\n0\n1540\n333\n76\nGO:2000026\n\n\n0\nmulticellular organism development\n0\n4828\n336\n142\nGO:0007275\n\n\n0\ntube development\n0\n1178\n345\n67\nGO:0035295\n\n\n0\nanatomical structure development\n0\n6222\n333\n162\nGO:0048856\n\n\n0\nregulation of developmental process\n0\n2702\n333\n99\nGO:0050793\n\n\n1\nexport from cell\n0\n977\n452\n68\nGO:0140352\n\n\n1\nsecretion by cell\n0\n908\n452\n65\nGO:0032940\n\n\n1\nsecretion\n0\n1084\n452\n71\nGO:0046903\n\n\n1\nregulation of secretion\n0\n767\n281\n47\nGO:0051046\n\n\n1\nregulation of secretion by cell\n0\n672\n281\n44\nGO:1903530\n\n\n2\nnitrogen compound transport\n0\n2104\n162\n48\nGO:0071705\n\n\n2\nregulation of peptide hormone secretion\n0\n262\n159\n19\nGO:0090276\n\n\n2\nregulation of peptide secretion\n0\n267\n159\n19\nGO:0002791\n\n\n2\nregulation of peptide transport\n0\n269\n159\n19\nGO:0090087\n\n\n2\nregulation of secretion by cell\n0\n672\n159\n27\nGO:1903530\n\n\n3\norganonitrogen compound metabolic process\n0\n6389\n522\n235\nGO:1901564\n\n\n3\nsystem development\n0\n4076\n521\n169\nGO:0048731\n\n\n3\npositive regulation of biological process\n0\n6614\n527\n233\nGO:0048518\n\n\n3\npositive regulation of cellular process\n0\n6091\n527\n219\nGO:0048522\n\n\n3\nregulation of primary metabolic process\n0\n5856\n527\n213\nGO:0080090\n\n\n4\ncell cycle\n0\n1802\n326\n174\nGO:0007049\n\n\n4\ncell cycle process\n0\n1240\n274\n139\nGO:0022402\n\n\n4\nmitotic cell cycle\n0\n870\n336\n129\nGO:0000278\n\n\n4\nmitotic cell cycle process\n0\n730\n270\n109\nGO:1903047\n\n\n4\nchromosome segregation\n0\n397\n262\n81\nGO:0007059\n\n\n5\nmulticellular organism development\n0\n4828\n330\n125\nGO:0007275\n\n\n5\nsystem development\n0\n4076\n330\n110\nGO:0048731\n\n\n5\nanatomical structure development\n0\n6222\n336\n146\nGO:0048856\n\n\n5\ndevelopmental process\n0\n6808\n324\n149\nGO:0032502\n\n\n5\nregulation of multicellular organismal process\n0\n3173\n328\n91\nGO:0051239\n\n\n\n\n\n\n\n\nTable 2: Top 5 biological process GO terms per cluster\n\n\n\n\nWe can use the geneProgramScoring() function to add module scores for each gene cluster to our Seurat object.\n\n\nCode\nprogram_labels &lt;- c(\"organogenesis\", \n                    \"secretion\", \n                    \"peptide production\", \n                    \"regulation of organogenesis\", \n                    \"differentiation\", \n                    \"cell cycle\")\nseu &lt;- geneProgramScoring(seu, \n                          genes = gene_embed$gene, \n                          gene.clusters = gene_embed$leiden, \n                          program.labels = program_labels)\n\n\nVisualizing the scores on our UMAP embedding shows us that the peptide program is highly-enriched only in mature endocrine cells. This makes sense biologically as mature endocrine celltypes’ primary roles are to produce peptides such as glucagon (alpha cells), insulin (beta cells), somatostatin (ductal cells), and pancreatic polypeptide (gamma cells).\n\n\nCode\np13 &lt;- Embeddings(seu, \"umap\") %&gt;% \n       as.data.frame() %&gt;% \n       magrittr::set_colnames(c(\"UMAP_1\", \"UMAP_2\")) %&gt;% \n       mutate(peptide_program_score = seu$peptide_production) %&gt;% \n       ggplot(aes(x = UMAP_1, y = UMAP_2, color = peptide_program_score)) + \n       geom_point(size = 1.5, alpha = 0.75, stroke = 0) + \n       labs(color = \"Program Score\") + \n       scale_color_gradientn(colors = palette_heatmap, \n                             labels = scales::label_number(accuracy = 0.1)) + \n       theme_scLANE(umap = TRUE) + \n       theme(axis.title = element_blank(), \n             axis.line.x = element_blank())\np14 &lt;- (p13 / p1) + \n       plot_layout(guides = \"collect\")\np14\n\n\n\n\n\n\n\n\nFigure 10: Enrichment of peptide production gene program\n\n\n\n\n\nWe can also visualize the trend in the peptide program scores over time, which confirms the biological conclusions we came to by inspecting the UMAP.\n\n\nCode\np15 &lt;- data.frame(PT = sling_pt$PT, \n                  peptide_program_score = seu$peptide_production, \n                  celltype = seu$celltype) %&gt;% \n       ggplot(aes(x = PT, y = peptide_program_score, color = celltype)) + \n       geom_point(alpha = 0.75, \n                  stroke = 0, \n                  size = 2) + \n       geom_smooth(color = \"black\", \n                   method = \"loess\", \n                   linewidth = 0.75) + \n       scale_color_manual(values = palette_celltype) + \n       labs(x = \"Pseudotime\", y = \"Peptide Program Score\") + \n       theme_scLANE() + \n       theme(legend.title = element_blank()) + \n       guide_umap()\np15\n\n\n\n\n\n\n\n\nFigure 11: Peptide production gene program scores over pseudotime\n\n\n\n\n\nNext, in order to identify which genes are “drivers” of a certain gene program, we can use the geneProgramDrivers() function to correlate normalized expression with program scores.\n\n\nCode\nprogram_drivers &lt;- geneProgramDrivers(seu, \n                                      genes = gene_embed$gene, \n                                      gene.program = seu$peptide_production, \n                                      verbose = FALSE)\n\n\nWe display the top 10 most-correlated genes here. For example, carboxypeptidase E (Cpe) is a known marker of pancreatic endocrine cells, and is involved in the regulation of peptide production.\n\n\nCode\nslice_head(program_drivers, n = 10) %&gt;% \n  kableExtra::kbl(digits = 3, \n                  booktabs = TRUE, \n                  row.names = FALSE,\n                  col.names = c(\"Gene\", \"Correlation\", \"P-value\", \"Adj. P-value\")) %&gt;% \n  kableExtra::kable_classic(c(\"hover\"), full_width = FALSE)\n\n\n\n\n\n\n\n\nGene\nCorrelation\nP-value\nAdj. P-value\n\n\n\n\nRbp4\n0.723\n0\n0\n\n\nPcsk2\n0.704\n0\n0\n\n\nIapp\n0.694\n0\n0\n\n\nScg2\n0.662\n0\n0\n\n\nTmem27\n0.659\n0\n0\n\n\nScgn\n0.659\n0\n0\n\n\nAbcc8\n0.651\n0\n0\n\n\nDbpht2\n0.646\n0\n0\n\n\n1700086L19Rik\n0.642\n0\n0\n\n\nPyy\n0.639\n0\n0\n\n\n\n\n\n\n\n\nTable 3: Top 10 driver genes for the peptide production gene program\n\n\n\n\nIndeed, normalized expression of Cpe is high across all mature endocrine celltypes, with alpha cells showing the highest overall mean expression.\n\n\nCode\np16 &lt;- data.frame(celltype = seu$celltype, \n                  rna = seu@assays$spliced@data[\"Cpe\", ]) %&gt;% \n       ggplot(aes(x = celltype, y = rna, color = celltype)) + \n       ggbeeswarm::geom_quasirandom(alpha = 0.75, \n                                    size = 2, \n                                    stroke = 0, \n                                    show.legend = FALSE) +\n       scale_color_manual(values = palette_celltype) + \n       stat_summary(fun = \"mean\", \n                    geom = \"point\", \n                    color = \"black\",\n                    size = 3) + \n       labs(y = \"Normalized Expression\") + \n       theme_scLANE() + \n       theme(axis.title.x = element_blank())\np16\n\n\n\n\n\n\n\n\nFigure 12: Driver gene expression by celltype\n\n\n\n\n\nFinally, we can use additive models to estimate the statistical significance of each gene program’s enrichment across the trajectory.\n\n\nCode\nprog_signif &lt;- geneProgramSignificance(list(seu$peptide_production, \n                                            seu$secretion, \n                                            seu$cell_cycle, \n                                            seu$differentiation, \n                                            seu$regulation_of_organogenesis, \n                                            seu$organogenesis), \n                                       pt = sling_pt$PT, \n                                       program.labels = c(\"Peptide production\",\n                                                          \"Secretion\", \n                                                          \"Cell cycle\", \n                                                          \"Differentiation\", \n                                                          \"Regulation of organogenesis\", \n                                                          \"Organogenesis\"))\n\n\nAll six gene programs appear to be significantly associated with pseudotime.\n\n\nCode\nkableExtra::kbl(prog_signif, \n                digits = 4, \n                booktabs = TRUE, \n                row.names = FALSE,\n                col.names = c(\"Program\", \"LRT Statistic\", \"DF\", \"P-value\", \"Adj. P-value\")) %&gt;% \n  kableExtra::kable_classic(c(\"hover\"), full_width = FALSE)\n\n\n\n\n\n\n\n\nProgram\nLRT Statistic\nDF\nP-value\nAdj. P-value\n\n\n\n\nRegulation of organogenesis\n6876.4474\n1\n0\n0\n\n\nSecretion\n5167.2046\n1\n0\n0\n\n\nOrganogenesis\n1787.4973\n1\n0\n0\n\n\nPeptide production\n1768.6957\n1\n0\n0\n\n\nDifferentiation\n1562.0034\n1\n0\n0\n\n\nCell cycle\n80.6925\n1\n0\n0\n\n\n\n\n\n\n\n\nTable 4: Significance testing for gene programs across the trajectory"
  },
  {
    "objectID": "tutorials/scLANE_Trajectory_DE.html#dynamic-gene-enrichment",
    "href": "tutorials/scLANE_Trajectory_DE.html#dynamic-gene-enrichment",
    "title": "Interpretable scRNA-seq Trajectory DE with scLANE",
    "section": "6.6 Dynamic gene enrichment",
    "text": "6.6 Dynamic gene enrichment\nLastly, we perform an enrichment analysis for our set of dynamic genes. We’ll focus on terms from the GO biological process (BP) set, as those are generally the easiest to interpret.\n\n\nCode\ndyn_gene_enrich &lt;- enrichDynamicGenes(scLANE_res_tidy, species = \"Mmusculus\")\ndyn_go_bp_terms &lt;- filter(dyn_gene_enrich$result, \n                          source == \"GO:BP\", \n                          p_value &lt; 0.05)\n\n\nOverall, there are 1310 unique significantly-enriched GO:BP terms for our trajectory.\n\n\nCode\nselect(dyn_go_bp_terms, term_name, p_value, source) %&gt;% \n  slice_sample(n = 10) %&gt;% \n  kableExtra::kbl(digits = 3, \n                  booktabs = TRUE, \n                  row.names = FALSE,\n                  col.names = c(\"Term\", \"P-value\", \"Source\")) %&gt;% \n  kableExtra::kable_classic(c(\"hover\"), full_width = FALSE)\n\n\n\n\n\n\n\n\nTerm\nP-value\nSource\n\n\n\n\nepithelial cell development\n0.001\nGO:BP\n\n\nnegative regulation of cellular senescence\n0.020\nGO:BP\n\n\npeptidyl-amino acid modification\n0.000\nGO:BP\n\n\npositive regulation of mitotic cell cycle\n0.000\nGO:BP\n\n\ncell cycle phase transition\n0.000\nGO:BP\n\n\nnegative regulation of insulin secretion\n0.000\nGO:BP\n\n\nsignaling\n0.000\nGO:BP\n\n\nregulation of muscle system process\n0.000\nGO:BP\n\n\ncell-cell junction organization\n0.000\nGO:BP\n\n\npositive regulation of cellular metabolic process\n0.000\nGO:BP\n\n\n\n\n\n\n\n\nTable 5: Random sample of the biological processes enriched for the dynamic gene set"
  },
  {
    "objectID": "tutorials/Seurat_AnnData_Conversion.html",
    "href": "tutorials/Seurat_AnnData_Conversion.html",
    "title": "Converting scRNA-seq Datasets from Seurat to AnnData",
    "section": "",
    "text": "if you’re like me, you switch between using R & Python pretty frequently. Despite all the hype, single cell is still a relatively nascent field, and not every method is implemented in both languages. For example: if you want to run an RNA velocity analysis you’re probably going to need to use Python, but when it comes to making figures for your publication it’s generally easier to go back to R and use ggplot2. Whatever the motivation, it’s useful to be able to seamlessly switch between programming environments - though doing so isn’t always easy.\nThere are several libraries that facilitate conversions from R-based scRNA-seq data structures (typically SingleCellExperiment or Seurat objects) to Python-based ones (AnnData and loom being the most common formats). I’m aware of {zellkonverter}, {SeuratDisk}, {anndata2ri}, {loomR}, & {sceasy}. While these packages generally work fairly well, they don’t always transfer every piece of data that you need, and reading through the source code to try & figure out why your objects are incomplete can take up a lot of time. In addition, some of the more niche / less used dataset attributes are often undocumented or poorly-documented in conversion packages, making it difficult to identify how the data types correspond between formats. As such, sometimes it’s necessary to get through the conversion on your own. This vignette will hopefully make doing so a bit easier by detailing the equivalencies across packages between different types of counts matrices, metadata, graphs, & more. We’ll focus on converting directly from Seurat to AnnData, since converting between formats within the same language e.g., AnnData to loom or SingleCellExperiment to Seurat is generally easier & much better-documented."
  },
  {
    "objectID": "tutorials/Seurat_AnnData_Conversion.html#r",
    "href": "tutorials/Seurat_AnnData_Conversion.html#r",
    "title": "Converting scRNA-seq Datasets from Seurat to AnnData",
    "section": "R",
    "text": "R\n\n\nCode\nlibrary(dplyr)       # data manipulation\nlibrary(Seurat)      # scRNA-seq tools \nlibrary(ggplot2)     # plots\nlibrary(biomaRt)     # gene metadata \nlibrary(paletteer)   # color palettes\nlibrary(patchwork)   # plot alignment\nlibrary(reticulate)  # Python interface"
  },
  {
    "objectID": "tutorials/Seurat_AnnData_Conversion.html#python",
    "href": "tutorials/Seurat_AnnData_Conversion.html#python",
    "title": "Converting scRNA-seq Datasets from Seurat to AnnData",
    "section": "Python",
    "text": "Python\n\n\nCode\nimport numpy as np                   # linear algebra tools\nimport pandas as pd                  # data manipulation\nimport scanpy as sc                  # scRNA-seq tools\nimport scvelo as scv                 # RNA velocity models\nimport anndata as ad                 # scRNA-seq data structures\nfrom scipy.io import mmread          # read .mtx files\nimport matplotlib.pyplot as plt      # matplotlib\nfrom plotnine import theme_classic   # plot themes\nfrom scipy.sparse import csr_matrix  # sparse matrices\n\n\nWe also tweak our matplotlib settings to match ggplot2::theme_classic(), which is my preferred theme.\n\n\nCode\nbase_size = 12\nplt.rcParams.update({\n    # font\n    'font.size': base_size, \n    'font.weight': 'normal',\n    # figure\n    'figure.dpi': 300, \n    'figure.edgecolor': 'white', \n    'figure.facecolor': 'white', \n    'figure.figsize': (6, 4), \n    'figure.constrained_layout.use': True,\n    # axes\n    'axes.edgecolor': 'black',\n    'axes.grid': False,\n    'axes.labelpad': 2.75,\n    'axes.labelsize': base_size * 0.8,\n    'axes.linewidth': 1.5,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.titlelocation': 'left',\n    'axes.titlepad': 11,\n    'axes.titlesize': base_size,\n    'axes.titleweight': 'normal',\n    'axes.xmargin': 0.1, \n    'axes.ymargin': 0.1, \n    # legend\n    'legend.borderaxespad': 1,\n    'legend.borderpad': 0.5,\n    'legend.columnspacing': 2,\n    'legend.fontsize': base_size * 0.8,\n    'legend.frameon': False,\n    'legend.handleheight': 1,\n    'legend.handlelength': 1.2,\n    'legend.labelspacing': 1,\n    'legend.title_fontsize': base_size, \n    'legend.markerscale': 1.25\n})"
  },
  {
    "objectID": "tutorials/Seurat_AnnData_Conversion.html#preprocessing-in-r",
    "href": "tutorials/Seurat_AnnData_Conversion.html#preprocessing-in-r",
    "title": "Converting scRNA-seq Datasets from Seurat to AnnData",
    "section": "Preprocessing in R",
    "text": "Preprocessing in R\nWe’ll start by running the data through a very typical preprocessing pipeline composed of QC, HVG selection, dimension reduction, Leiden graph-based clustering, & visualization. We’ll do a couple things that are non-standard though - first, we make sure to return the fitted UMAP model after running RunUMAP(), which will allow us to later recompute the set of nearest-neighbors that UMAP uses internally. Next, we identify nearest neighbors twice, once with return.neighbor toggled & once without it. The set of NNs identified each time is the same, but the first run returns the NN indices and inter-cell distances, while the second returns the shared nearest-neighbor graph that we use as input to the Leiden clustering algorithm in FindClusters().\n\n\nCode\nbrain &lt;- brain %&gt;% \n         PercentageFeatureSet(pattern = \"^RP[SL]\", col.name = \"percent_ribo\") %&gt;% \n         PercentageFeatureSet(pattern = \"^MT-\", col.name = \"percent_mito\") %&gt;% \n         NormalizeData(verbose = FALSE) %&gt;% \n         ScaleData(verbose = FALSE) %&gt;% \n         FindVariableFeatures(nfeatures = 3000, verbose = FALSE) %&gt;% \n         RunPCA(features = VariableFeatures(.), \n                npcs = 30, \n                verbose = FALSE, \n                seed.use = 312, \n                approx = TRUE) %&gt;% \n         CellCycleScoring(s.features = cc.genes.updated.2019$s.genes, \n                          g2m.features = cc.genes.updated.2019$g2m.genes, \n                          set.ident = FALSE) %&gt;% \n         AddMetaData(.$S.Score - .$G2M.Score, col.name = \"CC_difference\") %&gt;% \n         RunUMAP(reduction = \"pca\", \n                 dims = 1:30, \n                 return.model = TRUE, \n                 n.neighbors = 20, \n                 n.components = 2, \n                 metric = \"cosine\", \n                 n.epochs = 1000, \n                 seed.use = 312, \n                 verbose = FALSE) %&gt;% \n         FindNeighbors(reduction = \"pca\", \n                       dims = 1:30, \n                       k.param = 20, \n                       return.neighbor = TRUE, \n                       nn.method = \"annoy\", \n                       annoy.metric = \"cosine\", \n                       verbose = FALSE) %&gt;% \n         FindNeighbors(reduction = \"pca\", \n                       dims = 1:30, \n                       k.param = 20, \n                       compute.SNN = TRUE, \n                       nn.method = \"annoy\", \n                       annoy.metric = \"cosine\", \n                       verbose = FALSE) %&gt;% \n         FindClusters(resolution = 0.25, \n                      algorithm = 4, \n                      method = \"igraph\", \n                      random.seed = 312, \n                      verbose = FALSE)\n\n\nWe’ll also add & clean up the metadata by creating a cell age variable and sorting the fine celltype labels into slightly coarser categories for visualization. This data is mostly derived from Figure 1 and Supplementary Table 1 in the original paper.\n\n\nCode\nbrain@meta.data &lt;- mutate(brain@meta.data, \n                          age = case_when(Timepoint == \"week_6\" ~ 6,\n                                          Timepoint == \"week_7\" ~ 7,\n                                          Timepoint == \"week_8\" ~ 8,\n                                          Timepoint == \"week_9\" ~ 9,\n                                          Timepoint == \"week_10\" ~ 10,\n                                          Timepoint == \"week_11\" ~ 11,\n                                          TRUE ~ NA_real_), \n                          age = as.factor(age), \n                          broad_celltype = case_when(Cell_type %in% c(\"hOMTN\") ~ \"Oculomotor & Trochlear Nucleus\", \n                                                     Cell_type %in% c(\"hSert\") ~ \"Serotonergic\", \n                                                     Cell_type %in% c(\"hGaba\", \"hNbGaba\") ~ \"Gabaergic\", \n                                                     Cell_type %in% c(\"hDA0\", \"hDA1\", \"hDA2\") ~ \"Dopaminergic\", \n                                                     Cell_type %in% c(\"hRgl1\", \"hRgl2a\", \"hRgl2b\", \"hRgl2c\", \"hRgl3\") ~ \"Radial Glia\", \n                                                     Cell_type %in% c(\"hOPC\") ~ \"Oligodendrocyte Precursor\", \n                                                     Cell_type %in% c(\"hPeric\") ~ \"Pericyte\", \n                                                     Cell_type %in% c(\"hEndo\") ~ \"Endothelial\", \n                                                     Cell_type %in% c(\"hNProg\", \"hProgBP\", \"hProgFPL\", \"hProgFPM\", \"hProgM\") ~ \"Progenitor\", \n                                                     Cell_type %in% c(\"hRN\") ~ \"Red Nucleus\", \n                                                     Cell_type %in% c(\"hNbM\", \"hNbML1\", \"hNbML5\") ~ \"Neuroblast\", \n                                                     Cell_type %in% c(\"hMgl\") ~ \"Microglia\", \n                                                     Cell_type %in% c(\"Unk\") ~ \"Unknown\", \n                                                     TRUE ~ NA_character_), \n                          broad_celltype = as.factor(broad_celltype))\n\n\nLet’s visualize the results. First, we’ll define a color palette for each of our key metadata features using the paletteer package.\n\n\nCode\npalette_cluster &lt;- paletteer_d(\"ggsci::nrc_npg\")\npalette_celltype &lt;- paletteer_d(\"ggthemes::Tableau_20\")\npalette_age &lt;- paletteer_d(\"MetBrewer::Juarez\")\npalette_cc &lt;- paletteer_d(\"ggsci::default_locuszoom\")\n\n\nWe’ll also create a clean theme & legend settings for our dimension reduction plots.\n\n\nCode\ntheme_umap &lt;- function(base.size = 1) {\n  scLANE::theme_scLANE() + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.subtitle = element_text(face = \"italic\"), \n        plot.caption = element_text(face = \"italic\"))\n}\nguide_umap &lt;- function(key.size = 4) {\n  guides(color = guide_legend(override.aes = list(size = key.size, alpha = 1)))\n}\n\n\nWe plot the unsupervised graph-based clustering, the true celltype labels, cell ages, & estimated cell cycle phase on our UMAP embedding.\n\n\nCode\np0 &lt;- DimPlot(brain, \n              group.by = \"seurat_clusters\", \n              cols = alpha(0.75, colour = palette_cluster), \n              shuffle = TRUE, \n              seed = 312, \n              pt.size = 1) + \n      labs(x = \"UMAP 1\", \n           y = \"UMAP 2\", \n           title = \"Leiden Graph-based Clustering\", \n           subtitle = \"k = 20, r = 0.25\") + \n      theme_umap() + \n      guide_umap()\np1 &lt;- DimPlot(brain, \n              group.by = \"broad_celltype\", \n              cols = alpha(0.75, colour = palette_celltype), \n              shuffle = TRUE, \n              seed = 312, \n              pt.size = 1) + \n      labs(x = \"UMAP 1\", \n           y = \"UMAP 2\", \n           title = \"Coarse Celltypes\", \n           subtitle = \"Derived from authors' original annotations\") + \n      theme_umap() + \n      guide_umap()\np2 &lt;- DimPlot(brain, \n              group.by = \"age\", \n              cols = alpha(0.75, colour = palette_age), \n              shuffle = TRUE, \n              seed = 312, \n              pt.size = 1) + \n      labs(x = \"UMAP 1\", \n           y = \"UMAP 2\", \n           title = \"Cell Age\", \n           subtitle = \"Timepoints measured in weeks\") + \n      theme_umap() + \n      guide_umap()\np3 &lt;- DimPlot(brain, \n              group.by = \"Phase\", \n              cols = alpha(0.75, colour = palette_cc), \n              shuffle = TRUE, \n              seed = 312, \n              pt.size = 1) + \n      labs(x = \"UMAP 1\", \n           y = \"UMAP 2\", \n           title = \"Cell Cycle Phase\", \n           subtitle = \"Estimated using genes from Tirosh et al (2016)\") + \n      theme_umap() + \n      guide_umap()\n(p0 | p1) / (p2 | p3)\n\n\n\n\n\n\n\n\nFigure 1: Overview of the neural development dataset"
  },
  {
    "objectID": "tutorials/Seurat_AnnData_Conversion.html#conversion-to-anndata",
    "href": "tutorials/Seurat_AnnData_Conversion.html#conversion-to-anndata",
    "title": "Converting scRNA-seq Datasets from Seurat to AnnData",
    "section": "Conversion to AnnData",
    "text": "Conversion to AnnData\nHaving done all this preprocessing already, when using tools in Python we’d ideally like to be able to keep our clustering, annotations, embeddings, etc. as they are instead of recomputing them with scanpy. That isn’t a terrible worst option of course, but it’s pretty much impossible to get the results of stochastic algorithms such as Leiden clustering and UMAP, and even simpler ones like HVG identification to agree when using differing implementations of differing methods. With that in mind, let’s port our results to Python!\n\n\n\n\n\n\nNote\n\n\n\nThere’s essentially two ways to do this; the first relies on the usage of RMarkdown & reticulate, which allows you to pass objects back & forth between languages. The second, more general method is to write the data to common file formats from R, then read them in with Python & clean up the files afterwards. We’ll show examples of both options, but the best general-practice method is the file-based way. We’ll set up a temporary directory for the conversion files now, then remove it after we’re done.\n\n\n\n\nCode\nif (!dir.exists(\"./conversion_files/\")) {\n  dir.create(\"./conversion_files\")\n}\n\n\n\nRaw Counts\nFirst, we’ll write the counts matrices to a sparse matrix market file, which can be read into Python via scipy. Note: the AnnData ecosystem is built around \\(\\text{cell} \\times \\text{gene}\\) counts matrices, which is transposed from how R-based single cell data structures store them. Make sure to transpose them in either R or Python before creating an AnnData object.\n\n\nCode\nMatrix::writeMM(brain@assays$RNA@layers$counts, file = \"./conversion_files/RNA_counts.mtx\")\n\n\n\n\nCell & Gene Metadata\nWe’ll use CSVs to store the metadata for our cells & genes; these can be read into Python using pandas. The biomaRt package allows us to pull an Ensembl ID & biotype for each gene in our dataset, which we save as well.\n\n\nCode\nreadr::write_csv(brain@meta.data,\n                 file = \"./conversion_files/cell_metadata.csv\",\n                 col_names = TRUE)\nreadr::write_csv(data.frame(cell = colnames(brain)), \n                 file = \"./conversion_files/cells.csv\", \n                 col_names = TRUE)\nreadr::write_csv(data.frame(gene = rownames(brain)), \n                 file = \"./conversion_files/genes.csv\", \n                 col_names = TRUE)\nmart &lt;- useDataset(\"hsapiens_gene_ensembl\", useMart(\"ensembl\"))\ngene_mapping_table &lt;- getBM(filters = \"hgnc_symbol\",\n                            attributes = c(\"hgnc_symbol\", \"ensembl_gene_id\", \"gene_biotype\"),\n                            values = rownames(brain),\n                            mart = mart, \n                            uniqueRows = TRUE)\ngene_mapping_table &lt;- data.frame(hgnc_symbol = rownames(brain)) %&gt;% \n                      left_join(gene_mapping_table, by = \"hgnc_symbol\") %&gt;% \n                      with_groups(hgnc_symbol, \n                                  mutate, \n                                  R = row_number()) %&gt;% \n                      filter(R == 1) %&gt;% \n                      dplyr::select(-R)\nreadr::write_csv(gene_mapping_table, \n                 file = \"./conversion_files/gene_mapping.csv\", \n                 col_names = TRUE)\n\n\n\n\nEmbeddings\nNext, we’ll create CSVs of our PCA & UMAP embeddings. Once we read these into Python they’ll need to be converted to numpy arrays.\n\n\nCode\nreadr::write_csv(as.data.frame(brain@reductions$pca@cell.embeddings),\n                 file = \"./conversion_files/PCA.csv\",\n                 col_names = TRUE)\nreadr::write_csv(as.data.frame(brain@reductions$umap@cell.embeddings),\n                 file = \"./conversion_files/UMAP.csv\",\n                 col_names = TRUE)\n\n\n\n\nGraph Structures\nLastly, we’ll save our nearest-neighbor graph data using a couple different formats - sparse matrices for the NN distance & UMAP connectivity graphs, and a CSV for the NN indices. First we need to actually create the NN distance graph though; in a Seurat object this graph is stored in an \\(n \\times k\\) matrix containing only the distances for each cell’s nearest neighbors, while in an AnnData object the matrix is expanded to also include a value of 0 for all non-neighbor cells i.e., a \\(n \\times n\\) sparse matrix. Luckily this is a fairly quick conversion to make: we simply record the row & column index for each cell’s nearest neighbors along with the accompanying cosine distance, then use that data to build a sparse matrix.\n\n\nCode\nknn_param &lt;- ncol(brain@neighbors$RNA.nn@nn.idx)\nrow_idx &lt;- col_idx &lt;- X &lt;- vector(\"numeric\", length = ncol(brain) * knn_param)\nfor (i in seq(ncol(brain))) {\n  row_idx[(knn_param * i - (knn_param - 1)):(knn_param * i)] &lt;- i\n  col_idx[(knn_param * i - (knn_param - 1)):(knn_param * i)] &lt;- brain@neighbors$RNA.nn@nn.idx[i, ]\n  X[(knn_param * i - (knn_param - 1)):(knn_param * i)] &lt;- brain@neighbors$RNA.nn@nn.dist[i, ]\n  \n}\nknn_dist_mat &lt;- Matrix::sparseMatrix(i = row_idx, \n                                     j = col_idx, \n                                     x = X)\n\n\nThe trickiest bit is the cell-level connectivities. This term is used pretty often in analyses done with scanpy and scvelo, but there isn’t really a direct equivalent in R. In short, scanpy (and the other packages built on top of it) uses part of the UMAP algorithm to both compute the nearest-neighbor graph & embed the cells in one step, rather than computing the neighbors separately from the UMAP embedding as is done in R. More technically, scanpy borrows the computation of the fuzzy simplicial set from UMAP, which is essentially a local approximation of the distances between points for a given distance metric (cosine, in our case). See here, specifically the section entitled “Adapting to Real World Data”, for many more details.\nSince the UMAP embeddings you get from Seurat & scanpy will differ somewhat because of differences in their underlying implementations, we’d like to be able to extract the connectivities from our pre-computed embedding and bring them in Python, instead of recomputing them with scanpy and thus having a set of neighbors that doesn’t exactly correspond to our UMAP. We can do this by refitting the UMAP model using the exact settings that we did when running it through the Seurat wrapper function, but this time we specify the parameter ret_extra = c(\"fgraph\"), which indicates that the fuzzy simplicial set matrix should be returned. Some of the parameters aren’t stored in the returned uwot model, so we need to set them manually. The only tricky one is the initialization; RunUMAP() uses the spectral (normalized Laplacian plus Gaussian noise) initialization as of the time of writing. Note: make sure to use the same random seed as you did when calling RunUMAP() originally.\n\n\nCode\nset.seed(312)\numap_reembed &lt;- uwot::umap(X = brain@reductions$pca@cell.embeddings, \n                           n_neighbors = brain@reductions$umap@misc$model$n_neighbors,\n                           n_components = 2,\n                           metric = \"cosine\", \n                           n_epochs = brain@reductions$umap@misc$model$n_epochs, \n                           init = \"spectral\", \n                           learning_rate = brain@reductions$umap@misc$model$alpha, \n                           min_dist = 0.1, \n                           local_connectivity = brain@reductions$umap@misc$model$local_connectivity, \n                           nn_method = \"annoy\", \n                           negative_sample_rate = brain@reductions$umap@misc$model$negative_sample_rate, \n                           ret_extra = c(\"fgraph\"),\n                           n_threads = 2,\n                           a = brain@reductions$umap@misc$model$a, \n                           b = brain@reductions$umap@misc$model$b, \n                           search_k = brain@reductions$umap@misc$model$search_k, \n                           approx_pow = brain@reductions$umap@misc$model$approx_pow, \n                           verbose = FALSE)\n\n\nNow that we’ve created our data, we write it to files.\n\n\nCode\nMatrix::writeMM(knn_dist_mat, file = \"./conversion_files/KNN_distances.mtx\")\nMatrix::writeMM(umap_reembed$fgraph, file = \"./conversion_files/UMAP_connectivity_matrix.mtx\")\nreadr::write_csv(as.data.frame(brain@neighbors$RNA.nn@nn.idx), \n                 file = \"./conversion_files/KNN_indices.csv\", \n                 col_names = TRUE)"
  },
  {
    "objectID": "tutorials/Seurat_AnnData_Conversion.html#downstream-analysis-in-python",
    "href": "tutorials/Seurat_AnnData_Conversion.html#downstream-analysis-in-python",
    "title": "Converting scRNA-seq Datasets from Seurat to AnnData",
    "section": "Downstream Analysis in Python",
    "text": "Downstream Analysis in Python\nWe read our data into Python, & do some necessary reformatting.\n\n\nCode\n# raw counts \nRNA_counts = mmread('./conversion_files/RNA_counts.mtx').transpose().tocsr().astype('float64')\n# cell metadata\ncell_metadata = pd.read_csv('./conversion_files/cell_metadata.csv') \\\n                  .rename(columns={'Cell_ID': 'cell'}) \\\n                  .set_index('cell', drop=False)\ncell_metadata['seurat_clusters'] = cell_metadata['seurat_clusters'].astype('category')\ncell_metadata['age'] = pd.Categorical(cell_metadata['age'].astype('str'), categories=['6', '7', '8', '9', '10', '11'])\ncat_cols = ['broad_celltype', 'Phase', 'Cell_type']\ncell_metadata[cat_cols] = cell_metadata[cat_cols].apply(lambda x: pd.Categorical(x, categories=list(dict.fromkeys(sorted(x, key=str.lower)))))\ncell_names = pd.read_csv('./conversion_files/cells.csv').set_index('cell', drop=False)\n# gene metadata\ngene_names = pd.read_csv('./conversion_files/genes.csv').set_index('gene', drop=False)\ngene_mapping_table = pd.read_csv('./conversion_files/gene_mapping.csv').set_index('hgnc_symbol', drop=False)\n# reduced dimensional embeddings\npca_embedding = pd.read_csv('./conversion_files/PCA.csv') \\\n                  .set_index(pd.Index(cell_names['cell'])) \\\n                  .to_numpy()\numap_embedding = pd.read_csv('./conversion_files/UMAP.csv') \\\n                   .set_index(pd.Index(cell_names['cell'])) \\\n                   .to_numpy()\n# KNN graphs\nknn_idx = pd.read_csv('./conversion_files/KNN_indices.csv').to_numpy().astype('int32') - 1\nknn_dist_mat = mmread('./conversion_files/KNN_distances.mtx').tocsr().astype('float64')\nknn_conn_mat = mmread('./conversion_files/UMAP_connectivity_matrix.mtx').tocsr().astype('float64')\n\n\nNow we can finally create our AnnData object. We make sure to store the unprocessed version of the data in the AnnData.raw slot.\n\n\nCode\nlayers_dict = {'RNA': RNA_counts}\ndimred_dict = {'X_pca': pca_embedding, 'X_umap': umap_embedding}\ngraph_dict = {'distances': knn_dist_mat, 'connectivities': knn_conn_mat}\nuns_dict = {\n    'broad_celltype_colors': np.array(r.palette_celltype[0:13]), \n    'seurat_clusters_colors': np.array(r.palette_cluster[0:7]), \n    'age_colors': np.array(r.palette_age[0:7]), \n    'Phase_colors': np.array(r.palette_cc[0:3]), \n    'neighbors': {\n        'connectivities_key': 'connectivities', \n        'distances_key': 'distances', \n        'indices': knn_idx, \n        'params': {\n            'n_neighbors': knn_idx.shape[1], \n            'method': 'Seurat::FindNeighbors()', \n            'metric': 'cosine', \n            'n_pcs': pca_embedding.shape[1], \n            'use_rep': 'X_pca'\n        } \n    }\n}\nadata = ad.AnnData(\n    X=layers_dict['RNA'], \n    obs=cell_metadata, \n    var=gene_mapping_table, \n    layers=layers_dict, \n    obsm=dimred_dict, \n    obsp=graph_dict, \n    uns=uns_dict\n)\nadata.raw = adata\nadata\n\n\nAnnData object with n_obs × n_vars = 1977 × 19527\n    obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'cell', 'Cell_type', 'Timepoint', 'percent_ribo', 'percent_mito', 'S.Score', 'G2M.Score', 'Phase', 'CC_difference', 'RNA_snn_res.0.25', 'seurat_clusters', 'age', 'broad_celltype'\n    var: 'hgnc_symbol', 'ensembl_gene_id', 'gene_biotype'\n    uns: 'broad_celltype_colors', 'seurat_clusters_colors', 'age_colors', 'Phase_colors', 'neighbors'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'RNA'\n    obsp: 'distances', 'connectivities'\n\n\nWhen plotting the UMAP embedding, we can see that the coordinates & colors have been preserved.\n\n\nCode\nax = sc.pl.scatter(\n  adata, \n  basis='umap',\n  color='broad_celltype', \n  title='', \n  frameon=True, \n  show=False, \n  legend_fontsize=10, \n)\nax.set_xlabel('UMAP 1')\nax.set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: UMAP Embedding from Seurat\n\n\n\n\n\nNow we can proceed with a typical downstream analysis using the scanpy library. After processing the cells, we estimate a force-directed graph embedding and then a diffusion map embedding, which we’ll use to estimate pseudotime.\n\n\nCode\nsc.pp.filter_genes_dispersion(adata, flavor='seurat', n_top_genes=3000)\nsc.pp.normalize_per_cell(adata)\nsc.pp.log1p(adata)\nsc.pp.scale(adata)\nsc.tl.draw_graph(\n  adata,\n  layout='fr',\n  n_jobs=2,\n  random_state=312\n)\nsc.tl.diffmap(adata, random_state=312)\n\n\n\n\nCode\nax = sc.pl.scatter(\n  adata,\n  basis='diffmap',\n  color='broad_celltype', \n  title='', \n  frameon=True, \n  show=False, \n  legend_fontsize=10\n)\nax.set_xlabel('DC 1')\nax.set_ylabel('DC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Diffusion map embedding colored by celltype\n\n\n\n\n\nWe assign a root cell belonging to the progenitor population, then estimate diffusion pseudotime for each cell, and finally plot the results with our diffusion map embedding.\n\n\nCode\nadata.uns['iroot'] = np.argmin(adata.obsm['X_diffmap'][1])\nsc.tl.dpt(adata)\nax = sc.pl.scatter(\n  adata, \n  color='dpt_pseudotime', \n  basis='diffmap', \n  title='', \n  color_map='gnuplot', \n  legend_fontsize=10, \n  show=False\n)\nax.set_xlabel('DC 1')\nax.set_ylabel('DC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Diffusion pseudotime\n\n\n\n\n\nWhen plotting the distribution of diffusion pseudotime for each coarse celltype, we see that more immature celltypes such as progenitors have lower values, whereas more mature celltypes like endothelial cells & microglia have larger values. This indicates that our DPT estimate is a decent proxy for progression through the underlying biological process.\n\n\nCode\nviolin_order = pd.DataFrame(adata.obs[['broad_celltype', 'dpt_pseudotime']]) \\\n                 .groupby('broad_celltype')['dpt_pseudotime'].mean() \\\n                 .sort_values(ascending=True) \\\n                 .index.tolist()\nax = sc.pl.violin(\n  adata, \n  keys='dpt_pseudotime', \n  groupby='broad_celltype', \n  frameon=True, \n  order=violin_order, \n  show=False, \n  inner='box',\n  scale='width', \n  stripplot=False\n)\nax.set_ylabel('Diffusion Pseudotime')\nax.set_xlabel(None)\nax.set_xticklabels(violin_order, rotation=40, ha='right')\nax.set_position([0.1, 0.375, 0.85, 0.575]) \nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Diffusion pseudotime distribution by celltype"
  },
  {
    "objectID": "tutorials/Seurat_AnnData_Conversion.html#file-cleanup",
    "href": "tutorials/Seurat_AnnData_Conversion.html#file-cleanup",
    "title": "Converting scRNA-seq Datasets from Seurat to AnnData",
    "section": "File Cleanup",
    "text": "File Cleanup\nLastly, we’ll remove the temporary directory we used to store our data while converting it all to AnnData. Note: always be super careful when using rm -rf!\n\n\nCode\nif (dir.exists(\"./conversion_files\")) {\n  system(\"rm -rf ./conversion_files\")\n}"
  },
  {
    "objectID": "tutorials/Seurat_AnnData_Conversion.html#returning-results-to-r",
    "href": "tutorials/Seurat_AnnData_Conversion.html#returning-results-to-r",
    "title": "Converting scRNA-seq Datasets from Seurat to AnnData",
    "section": "Returning Results to R",
    "text": "Returning Results to R\nWe’ll quickly pull the diffusion pseudotime estimate for each cell back into R using reticulate, and add it to our Seurat object. We also add the diffusion map embedding to the reductions slot in the object.\n\n\nCode\ndpt_est &lt;- py$adata$obs$dpt_pseudotime\nnames(dpt_est) &lt;- py$adata$obs$cell\nbrain &lt;- AddMetaData(brain, metadata = dpt_est, col.name = \"scanpy_dpt\")\ndiffmap_embed &lt;- py$adata$obsm[\"X_diffmap\"]\nrownames(diffmap_embed) &lt;- colnames(brain)\ncolnames(diffmap_embed) &lt;- paste0(\"DC_\", c(1:ncol(diffmap_embed)))\nbrain@reductions$diffmap &lt;- CreateDimReducObject(embeddings = diffmap_embed, \n                                                 assay = \"RNA\", \n                                                 key = \"DC_\", \n                                                 global = TRUE)\n\n\nWe can plot the DPT estimates on our UMAP embedding:\n\n\nCode\nFeaturePlot(brain, \n            features = \"scanpy_dpt\", \n            reduction = \"umap\", \n            pt.size = 1) + \n  scale_color_gradientn(colours = viridisLite::inferno(n = 20), \n                        labels = scales::label_number(accuracy = .1)) + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n  theme_umap()\n\n\n\n\n\n\n\n\nFigure 6: Diffusion pseudotime from Scanpy\n\n\n\n\n\nWe can also visualize gene dynamics over pseudotime for a few genes of interest. For example, we see that fibronectin 1 (FN1) is highly expressed towards the end of the biological process, specifically in pericyte & endothelial cells. These two celltypes form part of the blood-brain barrier, & are closely related to one another (source).\n\n\nCode\ndata.frame(cell = colnames(brain), \n           celltype = brain$broad_celltype, \n           dpt = brain$scanpy_dpt, \n           LGALS1 = GetAssayData(brain, \"RNA\")[\"LGALS1\", ], \n           COL4A1 = GetAssayData(brain, \"RNA\")[\"COL4A1\", ], \n           RGS5 = GetAssayData(brain, \"RNA\")[\"RGS5\", ], \n           FN1 = GetAssayData(brain, \"RNA\")[\"FN1\", ]) %&gt;% \n  tidyr::pivot_longer(cols = !c(cell, celltype, dpt), \n                      names_to = \"gene\", \n                      values_to = \"expression\") %&gt;% \n  ggplot(aes(x = dpt, y = expression)) + \n  facet_wrap(~gene, \n             ncol = 2, \n             nrow = 2) + \n  geom_point(aes(color = celltype), \n             size = 1, \n             alpha = 0.75) + \n  geom_smooth(method = \"gam\", \n              se = FALSE, \n              linewidth = 1.25, \n              color = \"black\") + \n  scale_x_continuous(labels = scales::label_number(accuracy = .1)) + \n  scale_y_continuous(limits = c(0, NA), labels = scales::label_number(accuracy = 1)) + \n  scale_color_manual(values = palette_celltype) + \n  labs(x = \"Diffusion Pseudotime\", y = \"Normalized Expression\") + \n  scLANE::theme_scLANE() + \n  theme(legend.title = element_blank(), \n        strip.text.x = element_text(face = \"italic\")) + \n  guides(color = guide_legend(override.aes = list(alpha = 1, size = 4)))\n\n\n\n\n\n\n\n\nFigure 7: Pseudotemporal gene dynamics during human embryonic neurogenesis"
  },
  {
    "objectID": "tutorials/Neurogenesis_with_scLANE.html#celltype-annotation",
    "href": "tutorials/Neurogenesis_with_scLANE.html#celltype-annotation",
    "title": "Investigating Transcription Factor Activity During Neurogenesis",
    "section": "6.1 Celltype annotation",
    "text": "6.1 Celltype annotation\nUsing a Wilcox test, we perform a basic DE analysis in order to identify putative marker genes for each cluster.\n\n\nCode\nseu_brain &lt;- JoinLayers(seu_brain)\npossible_markers &lt;- FindAllMarkers(seu_brain, \n                                   assay = \"RNA\", \n                                   logfc.threshold = 0.3, \n                                   test.use = \"wilcox\", \n                                   slot = \"data\", \n                                   min.pct = 0.1, \n                                   only.pos = TRUE, \n                                   verbose = FALSE, \n                                   random.seed = 312)\ntop5_possible_markers &lt;- arrange(possible_markers, p_val_adj) %&gt;% \n                         with_groups(cluster, \n                                     slice_head, \n                                     n = 5)\n\n\nWe visualize the top 5 markers per cluster using a dotplot. Clear expression patterns emerge - for example, expression of NK2 homeobox 2 (NKX2-2) is clearly specific to cluster 9.\n\n\nCode\np2 &lt;- DotPlot(seu_brain, \n              features = unique(top5_possible_markers$gene),\n              assay = \"RNA\", \n              group.by = \"seurat_clusters\", \n              dot.scale = 5,\n              cols = paletteer::paletteer_d(\"wesanderson::Zissou1\")[c(1, 5)], \n              scale.by = \"radius\") + \n      coord_flip() +\n      labs(y = \"Leiden Cluster\") +\n      theme_scLANE() + \n      theme(axis.title.y = element_blank(), \n            axis.text.y = element_text(face = \"italic\"))\np2\n\n\n\n\n\n\n\n\nFigure 2: Putative marker genes for each cluster\n\n\n\n\n\nBased on the clustering and DE analysis, we assign celltype labels to each cluster.\n\n\n\n\n\n\nNote\n\n\n\nThe celltype annotations we’re assigning are mostly based on Figs. 1-3 and Extended Data Figs. 1-2 & 4 from the original publication.\n\n\n\n\nCode\nseu_brain@meta.data &lt;- mutate(seu_brain@meta.data, \n                              celltype = case_when(seurat_clusters == \"1\" ~ \"Excitatory Neuron\", \n                                                   seurat_clusters == \"2\" ~ \"Interneuron\", \n                                                   seurat_clusters == \"3\" ~ \"Excitatory Neuron\", \n                                                   seurat_clusters == \"4\" ~ \"Neuronal Progenitor\", \n                                                   seurat_clusters == \"5\" ~ \"Astrocyte\", \n                                                   seurat_clusters == \"6\" ~ \"Neuronal Progenitor\", \n                                                   seurat_clusters == \"7\" ~ \"Microglia\", \n                                                   seurat_clusters == \"8\" ~ \"Excitatory Neuron\", \n                                                   seurat_clusters == \"9\" ~ \"Oligodendrocyte Progenitor\", \n                                                   TRUE ~ NA_character_), \n                              celltype_short = case_when(seurat_clusters == \"1\" ~ \"Exc. Neuron\", \n                                                         seurat_clusters == \"2\" ~ \"Interneuron\", \n                                                         seurat_clusters == \"3\" ~ \"Exc. Neuron\", \n                                                         seurat_clusters == \"4\" ~ \"NPC\", \n                                                         seurat_clusters == \"5\" ~ \"Astrocyte\", \n                                                         seurat_clusters == \"6\" ~ \"NPC\", \n                                                         seurat_clusters == \"7\" ~ \"Microglia\", \n                                                         seurat_clusters == \"8\" ~ \"Exc. Neuron\", \n                                                         seurat_clusters == \"9\" ~ \"OPC\", \n                                                         TRUE ~ NA_character_)) %&gt;% \n                              mutate(celltype_short = as.factor(celltype_short))\n\n\nWe plot our new celltype labels on our UMAP embedding.\n\n\nCode\np3 &lt;- data.frame(Embeddings(seu_brain, \"umap\")) %&gt;% \n      mutate(celltype = seu_brain$celltype) %&gt;% \n      ggplot(aes(x = umap_1, y = umap_2, color = celltype)) + \n      geom_point(size = 1.5, \n                 alpha = 0.75, \n                 stroke = 0) + \n      scale_color_manual(values = palette_celltype) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme_scLANE(umap = TRUE) + \n      theme(legend.title = element_blank()) + \n      guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np3\n\n\n\n\n\n\n\n\nFigure 3: UMAP embedding colored by celltype\n\n\n\n\n\nWe re-run our DE testing between our annotated celltypes.\n\n\nCode\nIdents(seu_brain) &lt;- \"celltype_short\"\ncelltype_markers &lt;- FindAllMarkers(seu_brain, \n                                   assay = \"RNA\", \n                                   logfc.threshold = 0.3, \n                                   test.use = \"wilcox\", \n                                   slot = \"data\", \n                                   min.pct = 0.1, \n                                   only.pos = TRUE, \n                                   verbose = FALSE, \n                                   random.seed = 312) %&gt;% \n                    mutate(cluster = factor(cluster, levels = levels(seu_brain$celltype_short)))\ntop5_celltype_markers &lt;- arrange(celltype_markers, p_val_adj) %&gt;% \n                         with_groups(cluster, \n                                     slice_head, \n                                     n = 5)\n\n\nThe expression trends are clear when visualized.\n\n\nCode\np4 &lt;- DotPlot(seu_brain, \n              features = unique(top5_celltype_markers$gene),\n              assay = \"RNA\", \n              group.by = \"celltype_short\", \n              dot.scale = 5,\n              cols = paletteer::paletteer_d(\"wesanderson::Zissou1\")[c(1, 5)], \n              scale.by = \"radius\") + \n      coord_flip() +\n      theme_scLANE() + \n      theme(axis.title = element_blank(), \n            axis.text.y = element_text(face = \"italic\"), \n            axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\np4\n\n\n\n\n\n\n\n\nFigure 4: Putative marker genes for each celltype"
  },
  {
    "objectID": "tutorials/Neurogenesis_with_scLANE.html#trajectory-inference",
    "href": "tutorials/Neurogenesis_with_scLANE.html#trajectory-inference",
    "title": "Investigating Transcription Factor Activity During Neurogenesis",
    "section": "6.2 Trajectory inference",
    "text": "6.2 Trajectory inference\nAs performed by the original authors, we filter out microglial and interneuron cells. This is done because “microglia are mesoderm-derived cells and interneurons are generated in the ganglionic eminence and migrate tangentially to the PFC” i.e., they do not belong to neuronal or glial lineages we’re interested in.\n\n\nCode\nseu_brain &lt;- seu_brain[, !seu_brain$celltype %in% c(\"Interneuron\", \"Microglia\")]\n\n\nSince we’ve subset the data, we now need to re-embed the cells using UMAP.\n\n\nCode\nseu_brain &lt;- RunUMAP(seu_brain, \n                     dims = 1:30, \n                     reduction = \"integrated.cca\", \n                     return.model = TRUE, \n                     n.neighbors = 30, \n                     n.components = 2, \n                     metric = \"cosine\", \n                     seed.use = 312, \n                     verbose = FALSE)\n\n\nWe can already roughly see how the trajectory will be fit - the root population is the neuronal progenitors, with the other three celltypes being terminal cell fates. As such, we should aim to fit three lineages during trajectory estimation.\n\n\nCode\np5 &lt;- data.frame(Embeddings(seu_brain, \"umap\")) %&gt;% \n      mutate(celltype = seu_brain$celltype) %&gt;% \n      ggplot(aes(x = umap_1, y = umap_2, color = celltype)) + \n      geom_point(size = 1.5, \n                 alpha = 0.75, \n                 stroke = 0) + \n      scale_color_manual(values = palette_celltype) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme_scLANE(umap = TRUE) + \n      theme(legend.title = element_blank()) + \n      guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np5\n\n\n\n\n\n\n\n\nFigure 5: UMAP embedding colored by filtered celltypes\n\n\n\n\n\n\n6.2.1 Pseuotime estimation\nWe’ll use the Slingshot package to fit principal curves through our UMAP embedding & estimate lineage-specific pseudotime.\n\n\nCode\nsling_res &lt;- slingshot(Embeddings(seu_brain, \"umap\"), \n                       clusterLabels = seu_brain$celltype, \n                       start.clus = \"Neuronal Progenitor\", \n                       end.clus = c(\"Astrocyte\", \"Oligodendrocyte Progenitor\", \"Excitatory Neuron\"), \n                       approx_points = 1000)\nsling_curves &lt;- slingCurves(sling_res, as.df = TRUE)\nsling_mst &lt;- slingMST(sling_res, as.df = TRUE)\nsling_pt &lt;- slingPseudotime(sling_res) %&gt;% \n            as.data.frame() %&gt;%\n            rowwise() %&gt;% \n            mutate(PT_Overall = mean(c_across(starts_with(\"Lineage\")), na.rm = TRUE)) %&gt;% \n            ungroup() %&gt;% \n            mutate(across(c(starts_with(\"Lineage\"), PT_Overall), \n                          \\(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))\nseu_brain &lt;- AddMetaData(seu_brain, \n                         metadata = sling_pt$PT_Overall, \n                         col.name = \"PT_Overall\")\n\n\nAbove, we created a mean-aggregated pseudotime across all three lineages. This serves as a proxy measurement for overall developmental progression.\n\n\nCode\np6 &lt;- data.frame(Embeddings(seu_brain, \"umap\")) %&gt;% \n      mutate(PT_Overall = seu_brain$PT_Overall) %&gt;% \n      ggplot(aes(x = umap_1, y = umap_2, color = PT_Overall)) + \n      geom_point(size = 1.5, \n                 alpha = 0.75, \n                 stroke = 0) + \n      scale_color_gradientn(colors = palette_heatmap, labels = scales::label_number(accuracy = .01)) + \n      labs(x = \"UMAP 1\", \n           y = \"UMAP 2\", \n           color = \"Mean Pseudotime\") + \n      theme_scLANE(umap = TRUE)\np6\n\n\n\n\n\n\n\n\nFigure 6: UMAP embedding colored by mean-aggregated pseudotime\n\n\n\n\n\nIn general, larger pseudotime values are generated for later developmental stages.\n\n\nCode\np7 &lt;- data.frame(PT_Overall = sling_pt$PT_Overall, \n                 week = seu_brain$week) %&gt;% \n      ggplot(aes(x = week, y = PT_Overall, color = week)) + \n      ggbeeswarm::geom_quasirandom(size = 1.5,\n                                   alpha = 0.75, \n                                   stroke = 0, \n                                   show.legend = FALSE) + \n      stat_summary(geom = \"point\", \n                   fun = \"mean\",\n                   color =  \"black\", \n                   size = 3.5) + \n      scale_color_manual(values = palette_week) + \n      labs(x = \"Gestational Week\", y = \"Mean Pseudotime\") +\n      theme_scLANE()\np7\n\n\n\n\n\n\n\n\nFigure 7: Beeswarm plots of the distribution of mean-aggregated pseudotime for each gestational week\n\n\n\n\n\nDuring pseudotime estimation Slingshot estimates a minimum spanning tree (MST), which serves as a rough graph abstraction of the relationships between celltypes. Our MST looks to correspond well with our exiting biological knowledge.\n\n\nCode\np8 &lt;- data.frame(Embeddings(seu_brain, \"umap\")) %&gt;% \n      mutate(celltype = seu_brain$celltype) %&gt;% \n      ggplot(aes(x = umap_1, y = umap_2, color = celltype)) + \n      geom_point(size = 1.5, \n                 alpha = 0.75, \n                 stroke = 0) + \n      geom_path(data = sling_mst, mapping = aes(x = umap_1, y = umap_2, group = Lineage), \n                linewidth = 1.25, \n                color = \"black\") + \n      geom_point(data = sling_mst, mapping = aes(x = umap_1, y = umap_2, fill = Cluster), \n                color = \"black\", \n                shape = 21, \n                size = 4.5, \n                stroke = 1.25, \n                show.legend = FALSE) + \n      scale_color_manual(values = palette_celltype) + \n      scale_fill_manual(values = palette_celltype) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme_scLANE(umap = TRUE) +  \n      theme(legend.title = element_blank()) + \n      guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np8\n\n\n\n\n\n\n\n\nFigure 8: UMAP embedding with MST from Slinghot overlaid\n\n\n\n\n\nThe three lineage seem to correctly identify the developmental relationships we’re looking for.\n\n\nCode\np9 &lt;- data.frame(Embeddings(seu_brain, \"umap\")) %&gt;% \n      bind_cols(sling_pt) %&gt;% \n      tidyr::pivot_longer(starts_with(\"Lineage\"), \n                          names_to = \"lineage\", \n                          values_to = \"pseudotime\") %&gt;% \n      ggplot(aes(x = umap_1, y = umap_2, color = pseudotime)) + \n      facet_wrap(~lineage, nrow = 3) + \n      geom_point(size = 1.5, \n                 alpha = 0.75, \n                 stroke = 0) + \n      labs(x = \"UMAP 1\", \n           y = \"UMAP 2\", \n           color = \"Pseudotime\") + \n      scale_color_gradientn(colors = palette_heatmap, labels = scales::label_number(accuracy = .01)) + \n      theme_scLANE(umap = TRUE)\np9\n\n\n\n\n\n\n\n\nFigure 9: UMAP embedding colored by lineage-specific pseudotime\n\n\n\n\n\nAs such, we add a celltype label to each lineage corresponding to each lineage’s terminal cell fate.\n\n\nCode\nsling_pt_long &lt;- data.frame(Embeddings(seu_brain, \"umap\")) %&gt;% \n                 bind_cols(sling_pt) %&gt;% \n                 tidyr::pivot_longer(starts_with(\"Lineage\"), \n                                     names_to = \"lineage\", \n                                     values_to = \"pseudotime\") %&gt;% \n                 mutate(lineage_label = case_when(lineage == \"Lineage1\" ~ \"Excitatory Neuron\", \n                                                  lineage == \"Lineage2\" ~ \"Astrocyte\", \n                                                  lineage == \"Lineage3\" ~ \"Oligodendrocyte Progenitor\", \n                                                  TRUE ~ NA_character_))\n\n\nWe can also plot the principal curves from Slingshot on our UMAP embedding. While the curves are a little noisy, they overall seem to recapitulate the underlying biology well.\n\n\nCode\np10 &lt;- data.frame(Embeddings(seu_brain, \"umap\")) %&gt;% \n       mutate(celltype = seu_brain$celltype) %&gt;% \n       ggplot(aes(x = umap_1, y = umap_2, color = celltype)) + \n       geom_point(size = 1.5, \n                  alpha = 0.75, \n                  stroke = 0) + \n       geom_path(data = sling_curves,\n                 mapping = aes(x = umap_1, y = umap_2, group = Lineage), \n                 color = \"black\", \n                 linewidth = 1.5, \n                 alpha = 0.75, \n                 lineend = \"round\") + \n       scale_color_manual(values = palette_celltype) + \n       labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n       theme_scLANE(umap = TRUE) + \n       theme(legend.title = element_blank()) + \n       guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np10\n\n\n\n\n\n\n\n\nFigure 10: UMAP embedding with principal curves from Slinghot overlaid\n\n\n\n\n\n\n\n6.2.2 Trajectory DE testing\nNow that we have a pseudotime estimate for each cell across each lineage, we can perform trajectory differential expression (DE) testing using our package scLANE. We test only the top 3,000 HVGs, a heuristic which is motivated by the fact that the UMAP embedding was generated using just expression values from the HVG set. As such, non-HVGs don’t have a direct association with pseudotime, and for simple analyses it’s usually OK to refrain from testing them.\n\n\nCode\ncell_offset &lt;- createCellOffset(seu_brain)\npt_df &lt;- data.frame(PT1 = sling_pt$Lineage1, \n                    PT2 = sling_pt$Lineage2, \n                    PT3 = sling_pt$Lineage3)\nscLANE_models &lt;- testDynamic(seu_brain, \n                             pt = pt_df, \n                             genes = VariableFeatures(seu_brain), \n                             size.factor.offset = cell_offset, \n                             n.cores = 6L, \n                             verbose = FALSE)\n\n\nscLANE testing completed for 3000 genes across 3 lineages in 14.509 mins\n\n\nCode\nscLANE_de_res &lt;- getResultsDE(scLANE_models)\n\n\nAfter running the getResultsDE() function we now have a tidy table of differential expression statistics.\n\n\nCode\nselect(scLANE_de_res, \n       Gene, \n       Lineage, \n       Test_Stat, \n       P_Val, \n       P_Val_Adj,\n       Gene_Dynamic_Overall) %&gt;% \n  mutate(Lineage = case_when(Lineage == \"A\" ~ \"Excitatory Neuron\", \n                             Lineage == \"B\" ~ \"Astrocyte\", \n                             Lineage == \"C\" ~ \"Oligodendrocyte Progenitor\", \n                             TRUE ~ NA_character_), \n         Gene_Dynamic_Overall = if_else(Gene_Dynamic_Overall == 1, \"Dynamic\", \"Static\")) %&gt;% \n  slice_head(n = 10) %&gt;% \n  kableExtra::kbl(digits = 4, \n                  booktabs = TRUE, \n                  col.names = c(\"Gene\", \"Lineage\", \"LRT stat.\", \"P-value\", \"Adj. p-value\", \"Predicted gene status\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nGene\nLineage\nLRT stat.\nP-value\nAdj. p-value\nPredicted gene status\n\n\n\n\nGTSE1\nAstrocyte\n67619.12\n0\n0\nDynamic\n\n\nGTSE1\nOligodendrocyte Progenitor\n49494.67\n0\n0\nDynamic\n\n\nC21orf58\nOligodendrocyte Progenitor\n43007.99\n0\n0\nDynamic\n\n\nZWINT\nAstrocyte\n31441.23\n0\n0\nDynamic\n\n\nCKAP2L\nOligodendrocyte Progenitor\n29424.94\n0\n0\nDynamic\n\n\nPLK1\nOligodendrocyte Progenitor\n27513.28\n0\n0\nDynamic\n\n\nLOC100505715\nAstrocyte\n26932.25\n0\n0\nDynamic\n\n\nZWINT\nOligodendrocyte Progenitor\n23814.28\n0\n0\nDynamic\n\n\nKIF20A\nAstrocyte\n23448.91\n0\n0\nDynamic\n\n\nHIST1H1D\nOligodendrocyte Progenitor\n21592.17\n0\n0\nDynamic\n\n\n\n\n\n\n\n\nTable 1: Sample DE test output from scLANE\n\n\n\n\n\n\n6.2.3 Gene dynamics\nHere we visualize the fitted dynamics and knots for three transcription factors (TFs) investigated in the original paper: SRY-box transcription factor 2 (SOX2), paired box 6 (PAX6), and oligodendrocyte transcription factor 1 (OLIG1). All three TFs have well-annotated roles in regulating neurogenesis in humans and other species e.g., Mus musculus. Interestingly, while SOX2 and OLIG1 have markedly different dynamics and knots (transcriptional switches) across lineages, the knots chosen for PAX6 are pretty similar for all three lineages - even though the dynamics differ.\n\n\nCode\np11 &lt;- getFittedValues(scLANE_models, \n                       genes = c(\"SOX2\", \"PAX6\", \"OLIG1\"), \n                       pt = pt_df, \n                       expr.mat = seu_brain, \n                       size.factor.offset = cell_offset, \n                       cell.meta.data = select(seu_brain@meta.data, celltype)) %&gt;% \n       mutate(lineage_label = case_when(lineage == \"A\" ~ \"Excitatory Neuron\", \n                                        lineage == \"B\" ~ \"Astrocyte\", \n                                        lineage == \"C\" ~ \"Oligodendrocyte Progenitor\", \n                                        TRUE ~ NA_character_)) %&gt;% \n       ggplot(aes(x = pt, y = rna_log1p)) + \n       facet_grid(lineage_label~gene) + \n       geom_point(aes(color = celltype), \n                  size = 2, \n                  alpha = 0.75, \n                  stroke = 0) + \n       geom_vline(data = data.frame(gene = scLANE_models$SOX2$Lineage_A$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Excitatory Neuron\", \n                                    knot = scLANE_models$SOX2$Lineage_A$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_vline(data = data.frame(gene = scLANE_models$SOX2$Lineage_B$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Astrocyte\", \n                                    knot = scLANE_models$SOX2$Lineage_B$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_vline(data = data.frame(gene = scLANE_models$SOX2$Lineage_C$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Oligodendrocyte Progenitor\", \n                                    knot = scLANE_models$SOX2$Lineage_C$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_vline(data = data.frame(gene = scLANE_models$PAX6$Lineage_A$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Excitatory Neuron\", \n                                    knot = scLANE_models$PAX6$Lineage_A$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_vline(data = data.frame(gene = scLANE_models$PAX6$Lineage_B$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Astrocyte\", \n                                    knot = scLANE_models$PAX6$Lineage_B$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_vline(data = data.frame(gene = scLANE_models$PAX6$Lineage_C$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Oligodendrocyte Progenitor\", \n                                    knot = scLANE_models$PAX6$Lineage_C$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_vline(data = data.frame(gene = scLANE_models$OLIG1$Lineage_A$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Excitatory Neuron\", \n                                    knot = scLANE_models$OLIG1$Lineage_A$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_vline(data = data.frame(gene = scLANE_models$OLIG1$Lineage_B$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Astrocyte\", \n                                    knot = scLANE_models$OLIG1$Lineage_B$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_vline(data = data.frame(gene = scLANE_models$OLIG1$Lineage_C$MARGE_Slope_Data$Gene, \n                                    lineage_label = \"Oligodendrocyte Progenitor\", \n                                    knot = scLANE_models$OLIG1$Lineage_C$MARGE_Slope_Data$Breakpoint), \n                  mapping = aes(xintercept = knot), \n                  linetype = \"dashed\", \n                  color = \"grey20\") + \n       geom_ribbon(aes(ymin = scLANE_ci_ll_log1p, ymax = scLANE_ci_ul_log1p), \n                   linewidth = 0, \n                   fill = \"grey70\", \n                   alpha = 0.75) + \n       geom_line(aes(y = scLANE_pred_log1p), \n                 color = \"black\", \n                 linewidth = 0.75) + \n       scale_x_continuous(labels = scales::label_number(accuracy = 0.01)) + \n       scale_color_manual(values = palette_celltype) + \n       labs(x = \"Pseudotime\", \n            y = \"Normalized Expression\") + \n       theme_scLANE() + \n       theme(legend.title = element_blank(), \n             strip.text.x = element_text(face = \"italic\"), \n             legend.position = \"bottom\") + \n       guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np11\n\n\n\n\n\n\n\n\nFigure 11: Gene dynamics for three transcription factors of interest\n\n\n\n\n\n\n\n6.2.4 Distribution of knots from scLANE\nNext, we use the getKnostDist() function to pull the location of every identified knot for genes dynamic across any of the three lineages.\n\n\nCode\ndyn_genes &lt;- filter(scLANE_de_res, Gene_Dynamic_Overall == 1) %&gt;% \n             distinct(Gene) %&gt;% \n             pull(Gene)\nknot_dist &lt;- getKnotDist(scLANE_models, dyn.genes = dyn_genes)\n\n\nThe output of the function is rather simple:\n\n\nCode\nmutate(knot_dist, \n       lineage = case_when(lineage == \"Lineage_A\" ~ \"Excitatory Neuron\", \n                           lineage == \"Lineage_B\" ~ \"Astrocyte\", \n                           lineage == \"Lineage_C\" ~ \"Oligodendrocyte Progenitor\", \n                           TRUE ~ NA_character_)) %&gt;% \n  slice_sample(n = 5) %&gt;% \n  kableExtra::kable(digits = 4, row.names = FALSE, col.names = c(\"Gene\", \"Lineage\", \"Knot\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nGene\nLineage\nKnot\n\n\n\n\nFN1\nOligodendrocyte Progenitor\n0.7306\n\n\nPLD4\nOligodendrocyte Progenitor\n0.7063\n\n\nCCDC112\nOligodendrocyte Progenitor\n0.7306\n\n\nPPFIBP1\nOligodendrocyte Progenitor\n0.5774\n\n\nTRIB2\nExcitatory Neuron\n0.1349\n\n\n\n\n\n\n\n\nTable 2: Sample output displaying per-gene, per-lineage knot locations in pseudotime\n\n\n\n\nWe can visualize the differences between lineages using a combination histogram and density plot. Interestingly, the oligodendrocyte progenitor lineage has a large number of knots just prior to ~0.75 pseudotime. This is about where the “break” in the UMAP embedding occurs (see e.g., Figure 10), hence the placement of knots there by scLANE makes sense.\n\n\nCode\np12 &lt;- mutate(knot_dist, \n              lineage_label = case_when(lineage == \"Lineage_A\" ~ \"Excitatory Neuron\", \n                                        lineage == \"Lineage_B\" ~ \"Astrocyte\", \n                                        lineage == \"Lineage_C\" ~ \"Oligodendrocyte Progenitor\", \n                                        TRUE ~ NA_character_)) %&gt;% \n       ggplot(aes(x = knot)) + \n       facet_wrap(~lineage_label, nrow = 3) + \n       geom_histogram(aes(y = after_stat(density)), \n                      fill = \"white\", color = \"navy\", linewidth = 0.5) + \n       geom_density(fill = \"steelblue3\", color = \"steelblue4\", alpha = 0.5, linewidth = 0.75) + \n       labs(x = \"Knot Location\", y = \"Density\") + \n       theme_scLANE() + \n       theme(axis.ticks.y = element_blank(), \n             axis.text.y = element_blank())\np12\n\n\n\n\n\n\n\n\nFigure 12: Empirical distribution of knots across pseudotime for each lineage\n\n\n\n\n\n\n\n6.2.5 Gene-level embeddings\nAfter generating three lineage-specific matrices of gene dynamics using the smoothedCountsMatrix() function, we use embedGenes() to cluster the gene dynamics within each lineage and embed them in dimension-reduced space with PCA & UMAP.\n\n\nCode\nsmoothed_counts &lt;- smoothedCountsMatrix(scLANE_models, \n                                        size.factor.offset = cell_offset, \n                                        pt = pt_df, \n                                        genes = dyn_genes, \n                                        n.cores = 4L)\ngene_embed_neuron &lt;- embedGenes(log1p(smoothed_counts$Lineage_A))\ngene_embed_astro &lt;- embedGenes(log1p(smoothed_counts$Lineage_B))\ngene_embed_opc &lt;- embedGenes(log1p(smoothed_counts$Lineage_C))\n\n\nWe can visualize the differing UMAP embeddings, showing heterogeneity across lineages:\n\n\nCode\np13 &lt;- ggplot(gene_embed_neuron, aes(x = umap1, y = umap2, color = leiden)) + \n       geom_point(size = 2, \n                  alpha = 0.75, \n                  stroke = 0) + \n       scale_color_manual(values = palette_week[1:length(unique(gene_embed_neuron$leiden))]) + \n       labs(x = \"UMAP 1\",\n            y = \"UMAP 2\", \n            color = \"Leiden\", \n            subtitle = \"Interneuron Lineage\") + \n       theme_scLANE(umap = TRUE) + \n       guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np14 &lt;- ggplot(gene_embed_astro, aes(x = umap1, y = umap2, color = leiden)) + \n       geom_point(size = 2, \n                  alpha = 0.75, \n                  stroke = 0) + \n       scale_color_manual(values = palette_week[-c(1:length(unique(gene_embed_neuron$leiden)))]) + \n       labs(x = \"UMAP 1\", \n            y = \"UMAP 2\", \n            color = \"Leiden\", \n            subtitle = \"Astrocyte Lineage\") + \n       theme_scLANE(umap = TRUE) + \n       guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np15 &lt;- ggplot(gene_embed_opc, aes(x = umap1, y = umap2, color = leiden)) + \n       geom_point(size = 2, \n                  alpha = 0.75, \n                  stroke = 0) + \n       scale_color_manual(values = palette_week[-c(1:(length(unique(gene_embed_neuron$leiden)) + length(unique(gene_embed_astro$leiden))))]) + \n       labs(x = \"UMAP 1\", \n            y = \"UMAP 2\", \n            color = \"Leiden\", \n            subtitle = \"Oligodendrocyte Progenitor Lineage\") + \n       theme_scLANE(umap = TRUE) + \n       guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np16 &lt;- p13 + p14 + p15\np16\n\n\n\n\n\n\n\n\nFigure 13: Lineage-specific embeddings of gene dynamics\n\n\n\n\n\n\n\n6.2.6 Gene set enrichment analysis\nLastly, we can compare the dynamic gene sets for each lineage by performing enrichment analysis on each. The enrichDynamicGenes() function does this using the gprofiler2 package internally.\n\n\nCode\nenrich_neuron &lt;- enrichDynamicGenes(scLANE_de_res, lineage = \"A\")\nenrich_astro &lt;- enrichDynamicGenes(scLANE_de_res, lineage = \"B\")\nenrich_opc &lt;- enrichDynamicGenes(scLANE_de_res, lineage = \"C\")\n\n\nExamining the top biological processes enriched in each lineage gives us some idea as to what each dynamic gene set is doing. For example, the interneuron lineage is correctly characterized by terms related to neurogenesis & neuronal differentiation.\n\n\nCode\ntop5_terms &lt;- filter(enrich_neuron$result, source == \"GO:BP\") %&gt;% \n              mutate(lineage = \"Interneuron\", .before = 1) %&gt;% \n              bind_rows((filter(enrich_astro$result, source == \"GO:BP\") %&gt;% \n                         mutate(lineage = \"Astrocyte\", .before = 1))) %&gt;% \n              bind_rows((filter(enrich_opc$result, source == \"GO:BP\") %&gt;% \n                         mutate(lineage = \"Oligodendrocyte Progenitor\", .before = 1))) %&gt;% \n              select(lineage, term_name, p_value) %&gt;% \n              arrange(p_value) %&gt;% \n              with_groups(lineage, \n                          slice_head, \n                          n = 5)\nkableExtra::kable(top5_terms, digits = 3, row.names = FALSE, col.names = c(\"Lineage\", \"Term\", \"P-value\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nLineage\nTerm\nP-value\n\n\n\n\nAstrocyte\nmitotic cell cycle\n0\n\n\nAstrocyte\ncell cycle\n0\n\n\nAstrocyte\nmitotic cell cycle process\n0\n\n\nAstrocyte\nchromosome segregation\n0\n\n\nAstrocyte\ncell division\n0\n\n\nInterneuron\nnervous system development\n0\n\n\nInterneuron\nneurogenesis\n0\n\n\nInterneuron\nsystem development\n0\n\n\nInterneuron\nneuron differentiation\n0\n\n\nInterneuron\ngeneration of neurons\n0\n\n\nOligodendrocyte Progenitor\nmitotic cell cycle\n0\n\n\nOligodendrocyte Progenitor\nmitotic cell cycle process\n0\n\n\nOligodendrocyte Progenitor\nchromosome segregation\n0\n\n\nOligodendrocyte Progenitor\ncell division\n0\n\n\nOligodendrocyte Progenitor\ncell cycle\n0\n\n\n\n\n\n\n\n\nTable 3: Top 5 GO:BP terms for the dynamic genes from each lineage\n\n\n\n\nWe can also identify sets of terms specific to each lineage; here we do so for the oligodendrocyte lineage.\n\n\nCode\nspec_terms_opc &lt;- filter(enrich_opc$result, source == \"GO:BP\", \n                         !term_id %in% (filter(enrich_neuron$result, p_value &lt; 0.05) %&gt;% pull(term_id)), \n                         !term_id %in% (filter(enrich_astro$result, p_value &lt; 0.05) %&gt;% pull(term_id)), \n                         p_value &lt; 0.05) %&gt;% \n                  mutate(lineage = \"Oligodendrocyte Progenitor\", .before = 1)\n\n\nWe see several terms significantly enriched in the lineage that are specific to OPC development. This serves as confirmatory evidence that our trajectory was fit correctly and that our trajectory DE results are biologically meaningful.\n\n\nCode\nfilter(spec_terms_opc, grepl(\"oligodendrocyte\", term_name)) %&gt;% \n  select(lineage, term_name, p_value) %&gt;% \n  kableExtra::kable(digits = 3, row.names = FALSE, col.names = c(\"Lineage\", \"Term\", \"P-value\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nLineage\nTerm\nP-value\n\n\n\n\nOligodendrocyte Progenitor\npositive regulation of oligodendrocyte differentiation\n0.010\n\n\nOligodendrocyte Progenitor\noligodendrocyte cell fate commitment\n0.032\n\n\nOligodendrocyte Progenitor\noligodendrocyte cell fate specification\n0.032\n\n\n\n\n\n\n\n\nTable 4: Significantly enriched OPC-related biological process"
  },
  {
    "objectID": "tutorials/GEE_Benchmarking.html",
    "href": "tutorials/GEE_Benchmarking.html",
    "title": "Benchmarking Negative Binomial GEE Model Backends",
    "section": "",
    "text": "In the course of my recent work on trajectory differential expression, I’ve spent a fair amount of time fitting generalized estimating equation (GEE) models. These models are like classical GLMs in that they can handle non-normally distributed response variables via some specified link function & iterative estimation of coefficients, but they differ in that they can account for the variation inherent to longitudinal or otherwise clustered datasets in which observations are measured repeatedly from multiple subjects & are thus not independent. Since scRNA-seq experiments are almost always composed of samples from multiple subjects nowadays, GEEs can be of great use when building models of gene expression, as they allow us to be more confident that our standard errors are accurate. Note that unlike (generalized) linear mixed models, the GEE is a marginal model and thus provides only a population-level fit - GLMMs are conditional models as they provide subject-specific fits.\nSince single cell mRNA abundance follows the negative binomial distribution, it’s necessary that whatever GEE fitting algorithm we use support that distribution. Since the negative binomial is a two-parameter distribution, this does complicate things; the most commonly-used R packages for GEEs do not support NB models. We do have a couple options in R though, the Python statsmodels ecosystem supports NB GEEs as well, and there’s a Julia package too. Our goal here will be to benchmark the available options in terms of runtime and model accuracy in terms of the fitted values & the estimated parameters. We’ll keep our simulated data simple, with a negative binomial response \\(Y\\) and a single continuously-valued covariate \\(X\\); thus we’ll estimate the coefficient vector \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T\\) using each method for each set of simulations."
  },
  {
    "objectID": "tutorials/GEE_Benchmarking.html#r",
    "href": "tutorials/GEE_Benchmarking.html#r",
    "title": "Benchmarking Negative Binomial GEE Model Backends",
    "section": "R",
    "text": "R\n\n\nCode\nlibrary(dplyr)       # data manipulation\nlibrary(ggplot2)     # plots \nlibrary(JuliaCall)   # call Julia from R\nlibrary(reticulate)  # call Python from R"
  },
  {
    "objectID": "tutorials/GEE_Benchmarking.html#python",
    "href": "tutorials/GEE_Benchmarking.html#python",
    "title": "Benchmarking Negative Binomial GEE Model Backends",
    "section": "Python",
    "text": "Python\nWe’ll call the Python code from R using reticulate, but we’ll need to make sure to use the virtual environment I set up previously that has the statsmodels and pandas libraries (and their various dependencies) installed.\n\n\nCode\nuse_virtualenv(\"~/Desktop/Python/science/venv/\", required = TRUE)\nsm &lt;- import(\"statsmodels.api\")\npd &lt;- import(\"pandas\")"
  },
  {
    "objectID": "tutorials/GEE_Benchmarking.html#julia",
    "href": "tutorials/GEE_Benchmarking.html#julia",
    "title": "Benchmarking Negative Binomial GEE Model Backends",
    "section": "Julia",
    "text": "Julia\nWe’ll also need to activate a Julia environment, into which I’ve installed the GEE.jl package and its necessary dependencies.\n\n\nCode\njulia_setup(verbose = FALSE)\njulia_command('using Pkg; Pkg.activate(\"/Users/jack/Desktop/Julia/science/\");')\njulia_command(\"using Distributions, GLM, GEE, DataFrames;\")"
  },
  {
    "objectID": "tutorials/GEE_Benchmarking.html#helper-functions",
    "href": "tutorials/GEE_Benchmarking.html#helper-functions",
    "title": "Benchmarking Negative Binomial GEE Model Backends",
    "section": "Helper Functions",
    "text": "Helper Functions\nUsing the same general simulation procedure from the example above, we’ll write a function to simulate negative binomial data suitable for using GEEs. We’ll generate multiple observations per subject, though the resulting data will be in the same format, albeit with an added subject ID column. We’ll keep things simple by allocating our total sample size evenly across subjects & keeping \\(\\boldsymbol{\\beta}\\) constant across all subjects.\n\n\nCode\nsim_gee_data &lt;- function(n.subjects = NULL, \n                         n.per.subject = NULL, \n                         dropout.rate = 0.1, \n                         theta.y = 3) {\n  # simulate true coefficients\n  sim_beta &lt;- rnorm(2, mean = 0, sd = 0.5)\n  # simulate X & Y per subject\n  subject_names &lt;- paste0(\"S\", seq(n.subjects))\n  subject_sims &lt;- purrr::map(subject_names, function(s) {\n    sim_df &lt;- data.frame(ID = s, \n                         X = rnorm(n.per.subject, mean = 0, sd = 1)) %&gt;% \n              mutate(Beta_X = sim_beta[1] + sim_beta[2] * X, \n                     Exp_Beta_X = exp(Beta_X)) %&gt;% \n              rowwise() %&gt;% \n              mutate(Y = rnbinom(1, mu = Exp_Beta_X, size = theta.y)) %&gt;% \n              ungroup()\n  })\n  subject_sim_df &lt;- purrr::reduce(subject_sims, rbind) %&gt;% \n                    mutate(ID = as.factor(ID))\n  # add stochastic dropout\n  dropout_idx &lt;- sample(seq(nrow(subject_sim_df)), size = round(dropout.rate * nrow(subject_sim_df)))\n  subject_sim_df$Y[dropout_idx] &lt;- 0\n  res_list &lt;- list(beta = sim_beta, \n                   sim_df = subject_sim_df)\n  return(res_list)\n}\n\n\nNext, we’ll write a function to run all of our different model types. If we use the R implementations of the GEE framework, we need to provide an estimate for \\(\\theta\\), since the estimation procedure depends on \\(\\theta\\) being “known”. To approximate this, we provide the value of \\(\\theta\\) estimated using the intercept-only model, which in my experience has been a good enough approximation for simpler datasets like this one.\n\n\nCode\nrun_model &lt;- function(sim.data = NULL, model.type = NULL) {\n  # run correct model framework\n  if (model.type == \"NB GLM\") {\n    start_time &lt;- Sys.time()\n    model_fit &lt;- MASS::glm.nb(Y ~ X, \n                              data = sim.data, \n                              link = \"log\")\n    diff_time &lt;- Sys.time() - start_time\n    model_preds &lt;- predict(model_fit, type = \"link\")\n    model_coef &lt;- coef(model_fit)\n    model_est_alpha &lt;- NA_real_\n  } else if (model.type == \"geeM\") {\n    start_time &lt;- Sys.time()\n    theta_hat &lt;- MASS::theta.mm(y = sim.data$Y,\n                                mu = mean(sim.data$Y),\n                                dfr = nrow(sim.data) - 1)\n    model_fit &lt;- geeM::geem(Y ~ X, \n                            id = sim.data$ID, \n                            data = sim.data, \n                            family = MASS::negative.binomial(theta = theta_hat, link = \"log\"), \n                            corstr = \"exchangeable\", \n                            sandwich = TRUE)\n    diff_time &lt;- Sys.time() - start_time\n    model_preds &lt;- predict(model_fit, type = \"link\")\n    model_coef &lt;- coef(model_fit)\n    model_est_alpha &lt;- model_fit$alpha\n  } else if (model.type == \"mmmgee\") {\n    theta_hat &lt;- MASS::theta.mm(y = sim.data$Y,\n                                mu = mean(sim.data$Y),\n                                dfr = nrow(sim.data) - 1)\n    start_time &lt;- Sys.time()\n    model_fit &lt;- mmmgee::geem2(Y ~ X, \n                               id = sim.data$ID, \n                               data = sim.data, \n                               family = MASS::negative.binomial(theta = theta_hat, link = \"log\"), \n                               corstr = \"exchangeable\", \n                               sandwich = TRUE)\n    diff_time &lt;- Sys.time() - start_time\n    model_preds &lt;- predict(model_fit, type = \"link\")\n    model_coef &lt;- coef(model_fit)\n    model_est_alpha &lt;- model_fit$alpha\n  } else if (model.type == \"statsmodels\") {\n    start_time &lt;- Sys.time()\n    nb_family &lt;- sm$families$NegativeBinomial(link = sm$genmod$families$links$log)\n    cor_structure &lt;- sm$cov_struct$Exchangeable()\n    py_gee &lt;- sm$GEE$from_formula(\"Y ~ X\", \n                                  \"ID\", \n                                  data = sim.data, \n                                  cov_struct = cor_structure, \n                                  family = nb_family)\n    model_fit &lt;- py_gee$fit()\n    diff_time &lt;- Sys.time() - start_time\n    model_preds &lt;- model_fit$get_prediction()$linpred$predicted_mean\n    model_coef &lt;- model_fit$params\n    model_est_alpha &lt;- model_fit$cov_struct$dep_params\n  } else if (model.type == \"GEE.jl\") {\n    start_time &lt;- Sys.time()\n    JuliaCall::julia_assign(\"sim_data\", sim.data)\n    JuliaCall::julia_command('model_fit = gee(@formula(Y ~ X), sim_data, sim_data.ID, NegativeBinomial(), ExchangeableCor(), LogLink(), cov_type=\"robust\");')\n    diff_time &lt;- Sys.time() - start_time\n    model_est_alpha &lt;- JuliaCall::julia_eval(\"GEE.corparams(model_fit)\")\n    model_coef &lt;- JuliaCall::julia_eval(\"GEE.coef(model_fit)\")\n    model_preds &lt;- log(JuliaCall::julia_eval(\"GEE.predict(model_fit)\"))  # GEE.predict() gives fitted values \n  }\n  # format results & compute model error\n  names(model_coef) &lt;- c(\"B0\", \"B1\")\n  model_rmse &lt;- yardstick::rmse_vec(truth = sim.data$Y, \n                                    estimate = exp(model_preds))\n  model_huber_loss &lt;- yardstick::huber_loss_vec(truth = sim.data$Y, \n                                                estimate = exp(model_preds), \n                                                delta = 2)\n  res_list &lt;- list(model_type = model.type, \n                   model_runtime = as.numeric(diff_time), \n                   model_runtime_units = attributes(diff_time)$units, \n                   model_rmse = model_rmse, \n                   model_huber_loss = model_huber_loss, \n                   model_beta = model_coef, \n                   model_alpha = model_est_alpha)\n  return(res_list)\n}"
  },
  {
    "objectID": "tutorials/GEE_Benchmarking.html#running-the-experiment",
    "href": "tutorials/GEE_Benchmarking.html#running-the-experiment",
    "title": "Benchmarking Negative Binomial GEE Model Backends",
    "section": "Running the Experiment",
    "text": "Running the Experiment\nWe fit each model over 100 iterations, and save the results in a list (purrr is great for this kind of thing).\n\n\nCode\nsim_list &lt;- purrr::map(seq(100), function(i) {\n  set.seed(i)\n  sim_data &lt;- sim_gee_data(n.subjects = 3, \n                           n.per.subject = 400, \n                           dropout.rate = 0.1, \n                           theta.y = 5)\n  glm_res &lt;- run_model(sim.data = sim_data$sim_df, model.type = \"NB GLM\")\n  geeM_res &lt;- run_model(sim.data = sim_data$sim_df, model.type = \"geeM\")\n  mmmgee_res &lt;- run_model(sim.data = sim_data$sim_df, model.type = \"mmmgee\")\n  statsmodels_res &lt;- run_model(sim.data = sim_data$sim_df, model.type = \"statsmodels\")\n  julia_GEE_res &lt;- run_model(sim.data = sim_data$sim_df, model.type = \"GEE.jl\")\n  all_models &lt;- list(glm_res, \n                     geeM_res, \n                     mmmgee_res, \n                     statsmodels_res, \n                     julia_GEE_res)\n  overall_res_df &lt;- data.frame(Iter = i, \n                               Beta_0 = sim_data$beta[1], \n                               Beta_1 = sim_data$beta[2], \n                               Method = purrr::map_chr(all_models, \\(x) x$model_type), \n                               Est_Beta_0 = purrr::map_dbl(all_models, \\(x) x$model_beta[1]), \n                               Est_Beta_1 = purrr::map_dbl(all_models, \\(x) x$model_beta[2]), \n                               Est_Alpha = purrr::map_dbl(all_models, \\(x) x$model_alpha), \n                               RMSE = purrr::map_dbl(all_models, \\(x) x$model_rmse), \n                               Huber_Loss = purrr::map_dbl(all_models, \\(x) x$model_huber_loss), \n                               Runtime = purrr::map_dbl(all_models, \\(x) x$model_runtime), \n                               Runtime_Units = purrr::map_chr(all_models, \\(x) x$model_runtime_units))\n  return(overall_res_df)\n})\n\n\nLet’s coerce the results to a dataframe & add a couple more features.\n\n\nCode\nsim_res_df &lt;- purrr::reduce(sim_list, rbind) %&gt;% \n              mutate(Abs_Error_Beta0 = abs(Beta_0 - Est_Beta_0), \n                     Abs_Error_Beta1 = abs(Beta_1 - Est_Beta_1), \n                     Runtime_Seconds = if_else(Runtime_Units == \"secs\", Runtime, Runtime * 60))"
  },
  {
    "objectID": "knowledge_base/scholarly_writing.html",
    "href": "knowledge_base/scholarly_writing.html",
    "title": "A Reproducibility-focused Workflow for Writing Scholarly Articles",
    "section": "",
    "text": "Scholarly writing can be a difficult, complex, and occasionally expensive process.It’s hard enough to simply get ideas on paper in a coherent manner - much less do so while adhering to journal formatting requirements, keeping track of references, and ensuring the reproducibility of your results. Collaboration with other authors introduces a whole host of other issues including version control, tracking reviewer comments, and differences in preferred writing tools. The purpose of this writeup is to lay out some thoughts on how to best address some of these issues in a way that focuses on reproducibility and ease of collaboration. No single tool is able to ameliorate very concern, but this workflow has - for the most part - worked well for me as of late. In general, the options fall along two axes: reproducibility and ease of use / collaboration. The goal is to find an appropriate (which is, to be fair, highly subjective) balance between the two. Lastly, we’ll place a small amount of importance on the aesthetics of the final output. Some would posit that this should not be a primary concern, and that view does have some merit. I am guilty of spending too much time on making figures pretty, etc. (just ask my advisor), but to some degree the aesthetic properties of the final paper do matter. People pay more attention to well-formatted, visually attractive figures and text, thus if the goal of each paper is in part to reach a wide audience the final product should be aesthetically pleasing. Anyways, off we go."
  },
  {
    "objectID": "knowledge_base/scholarly_writing.html#microsoft-word",
    "href": "knowledge_base/scholarly_writing.html#microsoft-word",
    "title": "A Reproducibility-focused Workflow for Writing Scholarly Articles",
    "section": "2.1 Microsoft Word",
    "text": "2.1 Microsoft Word\nThe classic and most widely-used option here is of course Microsoft Word. Word is high up on the ease of use axis, and low on the reproducibility axis.\nPros: per-collaborator change tracking is possible, it integrates with several citation managers, and pretty much everyone has it installed on their computer regardless of what operating system they use. In addition, along with PDFs Word documents are what journals usually accept for initial & final submissions.\nCons: figure and table placement around text can be a huge pain, citation integration is slow, and since it’s impossible to check Word docs into GitHub you usually end up with dozens of different manuscript versions floating around across various machines. in addition, once figures, references, and dozens of pages of text have been placed in the document the app tends to slow to a crawl, which makes writing and editing painful.\nI understand why researchers use Word, I really do. In terms of setup it has the lowest barrier to entry and it tends to be the easiest way to get multiple authors collaborating on the same manuscript. In addition, you can pretty easily convert your Word doc to PDF using Adobe Acrobat if that’s what the journal you’re submitting to requires. However, it does, in my opinion, come with some pretty significant drawbacks. If you need to reformat for a journal submission you must do so manually, which is time-consuming and might affect the placement of your figures & tables. In addition, the equation editor is an absolute nightmare to use compared to writing equations in LaTeX."
  },
  {
    "objectID": "knowledge_base/scholarly_writing.html#quarto",
    "href": "knowledge_base/scholarly_writing.html#quarto",
    "title": "A Reproducibility-focused Workflow for Writing Scholarly Articles",
    "section": "2.2 Quarto",
    "text": "2.2 Quarto\nThe successor to the widely-popular RMarkdown, Quarto is a Pandoc-based, fully-featured technical writing suite that allows for code execution, automatic figure placement, Markdown & LaTeX usage, and more. I’ve been using Quarto for over a year now, and while there have been some growing pains (as there are with any new software), in general I have vastly enjoyed the experience and prefer it to RMarkdown.\nPros: you can compile to multiple output formats simultaneously, including code is quite simple, and it’s possible to use GitHub, GitLab, etc. to version-control your document - which makes collaboration a bit easier. In addition, Zotero support is included, and the output of each format is quite visually appealing & customizeable. Quarto supports a wide variety of document types, including articles (Word, PDF, HTML), websites such as this one, presentations (PowerPoint, Beamer, RevealJS), books, and as of v1.4 web-based manuscripts. In addition, the framework is agnostic with respect to the type of software used to edit the text. I use VS Code, but other options include RStudio and JupyterLab. Finally, since Pandoc is used under the hood, figure, table, algorithm, etc. placement is handled algorithmically, which makes adding or deleting non-text content super simple.\nCons: collaboration can be a bit tricky. This is, in my opinion, the main drawback. Non-technical collaborators almost certainly don’t have Quarto installed, and even some technical colleagues might not have made the switch from RMarkdown. The other primary pain point I’ve run into is that tracking word-level changes isn’t really possible unless you feel up to going through git commits to see who changed what. In that respect specifically, Word is a much better option for editing and reviewing your manuscript. A solution to this I’ve used lately is to write the first couple versions using Quarto in VS Code, then use a Word doc to perform the final edits and changes with my advisor / coauthors. This isn’t a perfect option, but it does allow you to mostly maintain the formatting advantages offered by Quarto."
  },
  {
    "objectID": "knowledge_base/scholarly_writing.html#command-line-editors-latex",
    "href": "knowledge_base/scholarly_writing.html#command-line-editors-latex",
    "title": "A Reproducibility-focused Workflow for Writing Scholarly Articles",
    "section": "2.3 Command line editors & LaTeX",
    "text": "2.3 Command line editors & LaTeX\nLastly, there will always a subset of scientists who have a highly-customized installation of Vim, Neovim, Emacs, or some other command line text editor that they cling to. They tend to prefer to write their manuscripts in LaTeX and later compile to PDF or Word using a tool like Pandoc (if the journal doesn’t allow LaTeX submissions). This is the most customizeable writing approach, and is also the hardest to use.\nPros: most of the aforementioned text editors have various plugins to make writing LaTeX documents easier, the IDEs are lightweight, and it’s easy to version-control a text document by checking it into git. Citations are supported via BibTex or something else like it.\nCons: almost no one else wants to write an entire manuscript in a text editor, and non-technical collaborators will almost certainly balk at a LaTeX-only approach. Tangentially, LaTeX has a very steep learning curve; it’s incredibly versatile and the results are usually quite aesthetically pleasing, but learning it is much harder than opening Word or using Markdown."
  },
  {
    "objectID": "knowledge_base/scholarly_writing.html#programmatic-solutions",
    "href": "knowledge_base/scholarly_writing.html#programmatic-solutions",
    "title": "A Reproducibility-focused Workflow for Writing Scholarly Articles",
    "section": "3.1 Programmatic solutions",
    "text": "3.1 Programmatic solutions\nAs far as programming languages go, R is my primary choice for graphics creation. ggplot2 is an incredibly rich library and can make pretty much any type of plot you need - except for RNA velocity streamline plots, which I have tried and failed to make work several times (irksome). I use ggplot2 to create the individual figure components, and as needed I combine them using the wonderful patchwork package. patchwork uses an arithmetical syntax to arrange figures, and in my opinion is much easier to use than alternatives such as cowplot or gridExtra.\nSome scientists prefer to work in Python, and for those of us that do matplotlib is king. A slightly more user-friendly matplotlib-based alternative is seaborn, which makes creating basic figures simpler but lacks the extensibility of matplotlib. While ggplot2 makes it easier to create most types of figure, matplotlib allows for super granular control over each plot element in a way that is often difficult with ggplot2. It also includes built-in functionality for arranging multiple subfigures which is nice.\nThe main positive aspect of using programmatic solutions is that they are much more reproducible: you simply save the code you used to generate a figure, then re-run it if necessary. In addition, code & date are shareable via platforms like Zenodo, which is starting to become a more widespread requirement for publication. Lastly, if you use a scientific writing system like Quarto or JupyterLab, you can embed the figure-generation code in your document - the ultimate form of reproducibility.\nThe downsides are that some figures just require manual editing in a GUI-based software e.g., adding text in between subfigures or drawing arrows within a plot. Also, not all collaborators are capable of reading or writing figure-generating code, which can hamper collaboration."
  },
  {
    "objectID": "knowledge_base/scholarly_writing.html#adobe-products",
    "href": "knowledge_base/scholarly_writing.html#adobe-products",
    "title": "A Reproducibility-focused Workflow for Writing Scholarly Articles",
    "section": "3.2 Adobe products",
    "text": "3.2 Adobe products\nThe main alternative (or perhaps extension) to programmatic approaches are GUI softwares such as Photoshop, InDesign, and Illustrator. Obviously some programmatic aspect is required to process the data and plot it, but these programs can be extraordinarily helpful when it comes to downstream processing. Specifically, adding text and other annotations and aligning multiple subfigures are both generally easier when using a point-and-click software. The downsides are mainly that GUI workflows are not reproducible or shareable, and manipulating figure post-hoc must be re-done every time you need to change a figure."
  },
  {
    "objectID": "knowledge_base/R_Development.html",
    "href": "knowledge_base/R_Development.html",
    "title": "R Development Resources",
    "section": "",
    "text": "Introduction\nThis document houses a list of resources I’ve put together that have helped me in my journey from novice R user to experienced (kinda) R package developer. Most of the resources will be centered around developing packages specifically, but some pertain to other types of R projects.\n\n\nPackage Development\n\nR Packages\n\nThe official R package development book, written by Hadley Wickham & Jenny Bryan. Contains a startup guide and sections on metadata, dependencies, unit testing, and documentation. Good comprehensive resource, but not the quickest way to get up & running. This package does have non-R dependencies, but they’re free & easy to install.\n\nThe usethis package\n\nThis R package removes a lot of headaches from setting up a new R package or other project. It contains functions to create a new R package, set up continuous integration (CI) tools, connect your package to a GitHub repository, and much more. Essentially, it removes a lot of the point-and-click steps that used to be necessary to set up an R package & get it version-controlled, and generally just makes development a lot easier.\n\nWriting R Extensions\n\nThis book, written by the R Core Team, is a lower-level guide to creating R packages, writing R documentation, debugging, linking R with C / C++, and other advanced topics. It’s useful if you’re getting deeper into package development & already have a very solid handle on the basics.\n\n\n\n\nComputational Efficiency\n\nCode profiling with proffer\n\nCode profiling allows you to see a breakdown in graph or table form of which functions in a piece of code are taking the longest to execute. This is done at varying hierarchical levels of resolution, which means you can see that e.g., a function you wrote called run_analysis() is taking up 50% of your computation time, and that within that function the main culprit is the subfunction load_large_dataset(). As such, it becomes very simple & quick to identify the best targets for further optimization of your code with respect to runtime.\n\nBenchmarking runtime using microbenchmark\n\nThis package is a very light benchmarking utility. It allows you to compare a list of different functions’ runtime by executing each function a given number of times e.g., 100 times, and returning the distribution of runtime for each function. This makes it very easy to quickly see which implementation of a given task is better with respect to runtime. The source repository can be found here.\n\n\n\n\nReproducible Research\n\nReproducible pipelines in R using targets\n\nThe targets package is one of my favorite R tools, & the well-written docs above show how to create version-controlled pipelines entirely using R. This framework is an absolute godsend for large projects, simulation studies, etc., and I’ve used it on every longterm computational project I’ve worked on in the past 2 years. It makes tracking, reproducing, & parallelizing the execution of large codebases very easy, and makes reproducible research accessible to anyone with a good handle on R.\n\nScientific writing via quarto\n\nquarto is a more fully-featured successor to RMarkdown, and enables the user to combine code, Markdown-formatted text, images, equations, etc. in a single document. Citations via supported as well, which makes technical writing relatively easy. There is support for LaTeX, which is great for homework assignments, proofs, and Methods sections. In addition, you can use quarto to create websites, books, presentations, and more. This site itself is actually run using quarto. I’d highly recommend it over RMarkdown at this point, both for its breadth of features & its expanded support of languages (Python, Julia, R, etc.) & IDEs (RStudio, VS Code, Jupyter notebooks). Full documentation can be found here.\n\nHow to control stochasticity when processing in parallel\n\nThis presentation describes how to produce reproducible random number streams when using one of R’s several parallel processing frameworks. It’s pretty applied (not theoretical), and several useful code examples are shown.\n\n\n\n\nDevelopment Best Practices\n\nWhat They Forgot to Teach You About R\n\nThis online book is less about package development & more about R’s idiosyncrasies, but it contains a bunch of useful tips & tricks on how to make your code more reproducible & less brittle. The debugging section is clear & concise, and contains links to other, more detailed resources as well.\n\n\n\n\nInterfacing with Other Languages\n\nreticulate\n\nSeamlessly interact with Python from your R session. Super easy to use & fairly full-featured, this package makes things like performing scRNA-seq analysis with both R & Python libraries a breeze. This package is developed by the team at RStudio & is actively maintained.\n\nJuliaCall\n\nAllows you to use the Julia language from within an R session. A great vignette can be found here.\n\n\n\n\nMiscellaneous\n\nThe R Inferno\n\nThis amusingly-titled & engagingly-written book covers a variety of oddities & frustrating idiosyncrasies that R has, many of which are holdovers from when R was being first developed. If you’re having an annoyingly difficult low-level problem, this book might have the answer.\n\nradian: an improved R console\n\nIf you often use R in the terminal, it can be a frustrating experience when compared to all the features (syntax highlighting, autocomplete, bracket-matching, etc.) that RStudio has. radian is a command line tool that adds all these aesthetic features to your R console. Simply install the Python-based library, then call radian to open a console session from the terminal."
  },
  {
    "objectID": "derivations/Large_Sample_Practice.html",
    "href": "derivations/Large_Sample_Practice.html",
    "title": "Assorted Large Sample Theory Practice Problems",
    "section": "",
    "text": "Here are a couple practice problems I wrote up while studying for a course in statistical asymptotics. The main focus is on the application of influence functions & U-statistics, though there are some simulation examples of statistical properties with code as well. Sources for the problems are listed throughout; some are taken directly from the original source while others I have modified slightly."
  },
  {
    "objectID": "derivations/Large_Sample_Practice.html#exercise-3.1---asymptotic-statistics",
    "href": "derivations/Large_Sample_Practice.html#exercise-3.1---asymptotic-statistics",
    "title": "Assorted Large Sample Theory Practice Problems",
    "section": "Exercise 3.1 - Asymptotic Statistics",
    "text": "Exercise 3.1 - Asymptotic Statistics\nThis is exercise 3.1 from A.W. van der Vaart’s textbook. The setup is as follows:\n\\[\n\\begin{aligned}\n  & X_1, ..., X_n \\overset{\\small{\\text{IID}}}{\\sim} F \\\\\n  &\\bar{X}_n = n^{-1} \\sum_{i=1}^n X_i \\\\\n  & S^2_n = n^{-1} \\sum_{i=1}^n \\left( X_i - \\bar{X}_n \\right)^2 \\\\\n  & \\mu_4 = \\mathbb{E} \\left[ (X - \\mu)^4 \\right];\\: \\mu_4 \\in \\mathbb{R} \\\\\n\\end{aligned}\n\\]\nWe’re interested in finding the joint asymptotic distribution of the following, and in determining what assumptions are necessary for the two quantities to be considered asymptotically independent:\n\\[\n\\begin{pmatrix}\n  \\sqrt{n}(\\bar{X}_n - \\mu) \\\\\n  \\sqrt{n}(S^2_n - \\sigma^2)\n\\end{pmatrix}\n\\]\nWe’ll start by defining the basics; we know that the sample mean converges in expectation to the population mean, and thus converges in probability as well.\n\\[\n\\mathbb{E} \\left[ \\bar{X}_n \\right]\\mu \\implies \\bar{X}_n \\overset{p}{\\to} \\mu\n\\]\nOn the other hand, \\(S^2_n\\) is not an unbiased estimator, with expectation:\n\\[\n\\mathbb{E} \\left[ S^2_n \\right] = \\frac{n-1}{n} \\sigma^2\n\\]\nHowever, we can show that it converges to \\(\\sigma^2\\) in probability.\n\n\n\n\n\n\nNote\n\n\n\nGoing forwards, raw moments will be denoted \\(m_k\\), and central moments \\(\\mu_k\\).\n\n\n\\[\n\\begin{aligned}\n  S^2_n\n    &= n^{-1} \\sum_{i=1}^n \\left( X_i - \\bar{X}_n \\right)^2 \\\\\n    &= n^{-1} \\sum_{i=1}^n X_i^2 - 2X_i\\bar{X}_n + \\bar{X}_n^2 \\\\\n    &= \\bar{X^2_n} - \\bar{X}^2_n \\\\\n  \\underset{n \\to \\infty}{\\text{lim}} S^2_n\n    &= m_2 - m_1^2 \\\\\n    &= \\sigma^2 \\\\\n  \\implies S^2_n &\\overset{p}{\\to} \\sigma^2 \\\\\n\\end{aligned}\n\\]\nSince both estimators converge in probability to their population parameters, they converge in distribution as well thanks to the following property of stochastic convergence:\n\\[\nX_n \\overset{p}{\\to} X \\implies X_n \\overset{d}{\\to} X\n\\]\nThe asymptotic variance of the sample mean is derived like so:\n\\[\n\\begin{aligned}\n  \\text{Var}(\\bar{X}_n)\n    &= \\text{Var} \\left( n^{-1} \\sum_{i=1}^n X_i \\right) \\\\\n    &= n^{-2} \\sum_{i=1}^n \\text{Var}(X_i) \\\\\n    &= n^{-1}\\sigma^2 \\\\\n\\end{aligned}\n\\]\nThus, the asymptotic distribution of the sample mean is:\n\\[\n\\sqrt{n} \\left( \\bar{X}_n - \\mu \\right) \\overset{d}{\\to} \\mathcal{N}\\left( 0, \\sigma^2 \\right)\n\\]\nDeriving the asymptotic variance for \\(S^2_n\\) is slightly trickier. Going forward, we’ll assume that the observations \\(X_i\\) have been centered around \\(\\mu\\), and thus have zero mean. This will simplify the following derivation somewhat:\n\\[\n\\begin{aligned}\n\\text{Var} \\left( S^2_n \\right)\n  &= \\text{Var} \\left( n^{-1} \\sum_{i=1}^n X_i^2 - \\bar{X}_n^2  \\right) \\\\\n  &= \\text{Var} \\left( n^{-1} \\sum_{i=1}^n X_i^2 \\right) \\\\\n  &= n^{-2} \\sum_{i=1}^n \\text{Var} \\left( X_i^2 \\right) \\\\\n  &= n^{-2} \\sum_{i=1}^n \\mathbb{E}\\left[ X_i^4 \\right] - \\left( \\mathbb{E}[X_i^2] \\right)^2 \\\\\n  &= n^{-2} \\sum_{i=1}^n m_4 - m_2^2 \\\\\n  &= n^{-1} \\left( m_4 - m_2^2 \\right) \\\\\n\\end{aligned}\n\\]\nThus, the asymptotic distribution for the sample variance is:\n\\[\n\\sqrt{n} \\left( S^2_n - \\sigma^2 \\right) \\overset{d}{\\to} \\mathcal{N} \\left( 0, m_4 - m_2^2 \\right)\n\\]\nNow all we need is the covariance between the two estimators. We’ll derive this using the multivariate Delta Method. First, we’ll need to define the sample mean and variance as a functional. Then, we’ll derive the asymptotic distribution of that quantity, the general form of which will be:\n\\[\n\\sqrt{n} \\left( \\hat{\\theta}_n - \\theta \\right) \\overset{d}{\\to} \\mathcal{N}_2(\\mathbf{0}, \\boldsymbol{\\Sigma})\n\\]\nThe multivariate Delta Method then allows us to formulate the joint asymptotic distribution of the original estimators like so:\n\\[\n\\sqrt{n} \\left( \\phi(\\hat{\\theta}_n) - \\phi(\\theta) \\right) \\overset{d}{\\to} \\mathcal{N}_2 \\left( \\mathbf{0},\\: \\phi^\\prime(\\theta) \\boldsymbol{\\Sigma} \\phi^\\prime(\\theta)^T \\right)\n\\]\nWe can derive the covariance between the sample mean & sample variance in the following fashion, making use of some properties of summation:\n\\[\n\\begin{aligned}\n  \\text{Cov} \\left( \\bar{X}_n, S^2_n \\right)\n    &= \\mathbb{E} \\left[ \\left( \\bar{X}_n  - \\mathbb{E} \\left[ \\bar{X}_n \\right] \\right) \\left( S^2_n  - \\mathbb{E} \\left[ S^2_n \\right] \\right) \\right] \\\\\n    &= \\mathbb{E} \\left[ \\left( \\bar{X}_n  - m_1 \\right) \\left( S^2_n  - m_2 \\right) \\right] \\\\\n    &= \\mathbb{E} \\left[ \\bar{X}_n S^2_n -m_2\\bar{X}_n -m_1S^2_n + m_1m_2 \\right] \\\\\n    &= \\mathbb{E} \\left[ \\bar{X}_nS^2_n \\right] - m_2m_1 - m_1m_2 +m_1m_2 \\\\\n    &= \\mathbb{E} \\left[ \\bar{X}_nS^2_n \\right] - m_1m_2 \\\\\n    &= \\mathbb{E} \\left[ \\left( n^{-1} \\sum_{i=1}^n X_i \\right) \\left( n^{-1} \\sum_{i=1}^n X_i^2 \\right)  \\right] - m_1m_2 \\\\\n    &= \\mathbb{E} \\left[ n^{-2} \\sum_{i=1}^n X_i \\sum_{i=1}^n X_i^2 \\right] - m_1m_2 \\\\\n    &= \\mathbb{E} \\left[ n^{-2} \\sum_{i=1}^n \\sum_{j=1}^n X_i X_j^2 \\right] - m_1m_2 \\\\\n    &= n^{-2} \\sum_{i=1}^n \\sum_{j=1}^n \\mathbb{E} \\left[ X_j^3 \\right] -m_1m_2 \\\\\n    &= n^{-2} (n^2m_3) -m_1m_2 \\\\\n    &= m_3 - m_1m_2 \\\\\n\\end{aligned}\n\\]\nThus we have the asymptotic distribution for the first and second moments (this can be verified by checking p.27 of the van der Vaart textbook). For expediency’s sake, we’ll refer to the variance-covariance matrix of the asymptotic distribution as \\(\\boldsymbol{\\Sigma^*}\\) going forwards.\n\\[\n\\sqrt{n} \\left( \\begin{pmatrix} \\bar{X}_n \\\\ \\bar{X^2}_n \\end{pmatrix} - \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix} \\right)\n\\overset{d}{\\to}\n\\mathcal{N}_2 \\left( \\mathbf{0},\\: \\begin{pmatrix} m_2 - m_1^2 & m_3 - m_1m_2 \\\\ m_3 - m_1m_2 & m_4 - m_2^2 \\end{pmatrix} \\right)\n\\]\nNext we need to set up our function, which we do like so:\n\\[\n\\begin{aligned}\n  \\phi(x, y) &= \\begin{pmatrix} x \\\\ y - x^2 \\end{pmatrix} \\\\\n  \\phi \\left( m_1, m_2 \\right) &= \\begin{pmatrix} \\bar{X}_n \\\\ S^2_n \\end{pmatrix} \\\\\n\\end{aligned}\n\\]\nThe gradient of \\(\\phi\\) is then:\n\\[\n\\begin{aligned}\n  \\phi^\\prime_{m_1, m_2}\n    &= \\begin{pmatrix}\n         \\frac{\\partial}{\\partial m_1} \\phi_1(m_1, m_2) & \\frac{\\partial}{\\partial m_1} \\phi_2(m_1, m_2) \\\\\n         \\frac{\\partial}{\\partial m_2} \\phi_1(m_1, m_2) & \\frac{\\partial}{\\partial m_2} \\phi_2(m_1, m_2) \\\\\n       \\end{pmatrix} \\\\\n    &= \\begin{pmatrix}\n         1 & -2m_1 \\\\\n         0 & 1 \\\\\n       \\end{pmatrix}\n\\end{aligned}\n\\]\nUsing the multivariate Delta Method, we have thus arrived at the joint asymptotic distribution of the sample mean & sample variance:\n\\[\n\\sqrt{n} \\left( \\begin{pmatrix} \\bar{X}_n \\\\ S^2_n \\end{pmatrix} - \\begin{pmatrix} \\mu \\\\ \\sigma^2 \\end{pmatrix} \\right)\n\\overset{d}{\\to}\n\\mathcal{N}_2 \\left( \\mathbf{0},\\: \\phi^\\prime_{m_1, m_2} \\boldsymbol{\\Sigma^*} \\left( \\phi^\\prime_{m_1, m_2} \\right)^T \\right)\n\\]\nIn deriving the above quantity, we’ve also figured out what conditions are necessary for \\(\\bar{X}_n\\) and \\(S^2_n\\) to be independent. We defined the covariance between the two estimators above:\n\\[\n\\text{Cov} \\left( \\bar{X}_n, S^2_n \\right) = m_3 - m_1m_2\n\\]\nKeeping in mind that we have centered our data, we know that our raw moments are equivalent to central moments. For symmetric distributions, the expectations of the odd moments are all equal to zero, and thus the covariance as defined above will go to zero if the distribution function \\(F\\) is symmetric. In order for us to strictly say that \\(\\bar{X}_n\\) and \\(S^2_n\\) are independent and not just uncorrelated, the two quantities must be jointly normally distributed, which we have shown above. Thus, as long as \\(F\\) is symmetric, the sample mean and sample variance are independent."
  },
  {
    "objectID": "derivations/Large_Sample_Practice.html#exercise-12.3---asymptotic-statistics",
    "href": "derivations/Large_Sample_Practice.html#exercise-12.3---asymptotic-statistics",
    "title": "Assorted Large Sample Theory Practice Problems",
    "section": "Exercise 12.3 - Asymptotic Statistics",
    "text": "Exercise 12.3 - Asymptotic Statistics\nThis problem is also pulled from the van der Vaart book; the goal is simply to determine an appropriate kernel for a U-statistic for the third central moment.\n\\[\n\\mu_3 = \\mathbb{E} \\left[ (X - \\mathbb{E}[X])^3 \\right]\n\\]\nWe could attempt to define the kernel as we would when creating a U-statistic for the sample variance, as shown below:\n\\[\nh(X_i, X_j) = (X_i - X_j)^3\n\\]\nHowever, this kernel is no symmetric, which is a highly desirable property in U-statistic kernels i.e., \\(h(x_i, x_j) \\neq h(x_j, x_i)\\) as shown below:\n\\[\n\\begin{aligned}\n  h(X_i, X_j)\n    &= (X_i - X_j)^3 \\\\\n    &= X_i^3 -3X_jX_i^2 + 3X_j^2X_i - X_j^3 \\\\\n  h(X_j, X_i)\n    &= (X_j - X_i)^3 \\\\\n    &= X_j^3 -3X_iX_j^2 + 3X_i^2X_j - X_i^3 \\\\\n\\implies h(X_i, X_j) &= -h(X_j, X_i) \\\\\n\\end{aligned}\n\\]\nHowever, any asymmetric U-statistic of degree \\(r\\) may be symmetrized by averaging over all possible input permutations using the following technique:\n\\[\ng(X_1, ..., X_r) = (r!)^{-1} \\sum_{i_1, ..., i_r} h(X_{i_1}, ..., X_{i_r})\n\\]\nWe’ll thus use the following symmetric kernel of degree 3 as detailed in Locke & Spurrier (1978):\n\\[\nh(X_1, X_2, X_3) = \\frac{1}{3} \\sum_{i=1}^3 \\left( X_i - \\frac{1}{3} (X_1 + X_2 + X_3) \\right)^3\n\\]\nThis in turn leads to the following U-statistic for \\(\\mu_k\\):\n\\[\n\\begin{aligned}\n  U_n\n    &= \\binom{n}{3}^{-1} \\sum_{i=1}^n \\sum_{i&lt;j} \\sum_{j&lt;k} h(X_i, X_j, X_k) \\\\\n    &= \\binom{n}{3}^{-1} \\sum_{i=1}^n \\sum_{i&lt;j} \\sum_{j&lt;k} \\frac{1}{3} \\left( \\left( X_i - \\frac{1}{3}(X_i + X_j + X_k) \\right)^3 + \\left( X_j - \\frac{1}{3}(X_i + X_j + X_k) \\right)^3 + \\left( X_k - \\frac{1}{3}(X_i + X_j + X_k) \\right)^3\\right)\n\\end{aligned}\n\\]\nWe can show this empirically by testing it via simulation. Here we simulate \\(n = 1000\\) realizations from \\(X_1, \\dots, X_n \\overset{\\small{\\text{IID}}}{\\sim} \\mathcal{N}(3,\\: 1)\\). Since we’re sampling from a normal distribution, which is symmetric about \\(\\mu\\), the expectation of the third central moment is equal to zero. In the interest of showing how the U-statistic converges towards \\(\\mu_3\\) as sample size grows, we’ll compute the statistic for several values of \\(n\\) and then visualize the results. First, we’ll need to define a helper function for our kernel:\n\n\nCode\nh_ijk &lt;- function(x_i, x_j, x_k) {\n  sum_ijk &lt;- x_i + x_j + x_k\n  mean_ijk &lt;- 1/3 * ((x_i - 1/3 * sum_ijk)^3 + (x_j - 1/3 * sum_ijk)^3 + (x_k - 1/3 * sum_ijk)^3)\n  return(mean_ijk)\n}\n\n\nNow we iterate over sample sizes & save the results. We’ll perform a few replications per sample size value for reproducibility reasons.\n\n\nCode\nn_vals &lt;- purrr::map(c(5, 10, 15, 25, 40, 50, 75, 100, 150, 200, 250, 300, 400, 500), \\(x) rep(x, 3)) %&gt;% \n          purrr::reduce(c)\nU_n_vals &lt;- numeric(length = length(n_vals)) \nfor (n in seq(n_vals)) {\n  set.seed(n)\n  sample_n &lt;- n_vals[n]\n  mu &lt;- 3\n  sigma &lt;- 1\n  x &lt;- rnorm(sample_n, mean = mu, sd = sigma)\n  i &lt;- 1\n  U_sum &lt;- 0\n  while (i &lt;= sample_n) {\n    j &lt;- i + 1\n    while (j &lt;= sample_n) {\n      k &lt;- j + 1\n      while (k &lt;= sample_n) {\n        h_val &lt;- h_ijk(x_i = x[i], x_j = x[j], x_k = x[k])\n        U_sum &lt;- U_sum + h_val\n        k &lt;- k + 1\n      }\n      j &lt;- j + 1\n    }\n    i &lt;- i + 1\n  }\n  U_n &lt;- choose(sample_n, 3)^(-1) * U_sum\n  U_n_vals[n] &lt;- U_n\n}\n\n\nWe can see that the U-statistic grows very close to the true value of zero as \\(n\\) increases. The drawback of this approach is the computational cost; even for this relatively small sample size the runtime is several minutes, and grows on the order of \\(O(n^3)\\).\n\n\nCode\ndata.frame(U = U_n_vals, \n           N = n_vals) %&gt;% \n  ggplot(aes(x = N, y = abs(U))) + \n  geom_point() + \n  geom_smooth(color = \"forestgreen\", se = FALSE) + \n  labs(x = latex2exp::TeX(r\"($\\textit{n}$)\"), \n       y = latex2exp::TeX(r\"($|\\textit{U_n} - \\theta|$)\")) + \n  theme_classic(base_size = 14)"
  },
  {
    "objectID": "derivations/Large_Sample_Practice.html#example-1.3.2---u-statistics-theory-and-practice",
    "href": "derivations/Large_Sample_Practice.html#example-1.3.2---u-statistics-theory-and-practice",
    "title": "Assorted Large Sample Theory Practice Problems",
    "section": "Example 1.3.2 - U-Statistics: Theory and Practice",
    "text": "Example 1.3.2 - U-Statistics: Theory and Practice\nThis problem is a slight modification of one of the examples from Lee (1990) seen on page 13. Instead of deriving the asymptotic distribution of the U-statistic for the sample variance, we’ll do so for the second raw moment \\(m_2\\).\n\\[\n\\theta = E[X^2] = m_2\n\\]\nFrom the definition of variance, we know that \\(m_2 = \\mu_2 + m_1^2\\). Our kernel, which is symmetric, will thus be a combination of the kernels for the sample variance and for the expectation squared:\n\\[\nh(X_1, X_2) = \\frac{(X_1 - X_2)^2}{2} + X_1X_2\n\\]\nhaving expectation:\n\\[\n\\begin{aligned}\n  \\mathbb{E}[h(X_1, X_2)]\n    &= \\mathbb{E} \\left[ \\frac{(X_1 - X_2)^2}{2} + X_1X_2 \\right] \\\\\n    &= \\frac{1}{2}(\\mathbb{E} \\left[ (X_1 - X_2)^2 \\right]) - \\mathbb{E}[X_1X_2] \\\\\n    &= \\frac{1}{2}(m_2 - 2\\mu^2 + m_2) + \\mu^2 \\\\\n    &= m_2 \\\\\n\\end{aligned}\n\\]\nThis leads us to the following U-statistic:\n\\[\nU_n = \\binom{n}{2}^{-1} \\sum_{i=1}^n \\sum_{i&lt;j} \\frac{(X_i - X_j)^2}{2} + X_iX_j\n\\]\nWe define the following:\n\\[\n\\begin{aligned}\n  h_1(X_1, X_2) &= \\mathbb{E}[h(X_1, X_2) | X_1] \\\\\n  h_1^c(X_1, X_2)\n    &= \\mathbb{E}[h(X_1, X_2) | X_1] - \\theta \\\\\n    &= \\mathbb{E}[h(X_1, X_2) | X_1] - m_2 \\\\\n  \\zeta_1\n    &= \\mathbb{E} \\left[ (h_1^c(X_1, X_2))^2 \\right] \\\\\n    &= \\mathbb{E} \\left[ (\\mathbb{E}[h(X_1, X_2) | X_1] - m_2)^2 \\right] \\\\\n    &= \\text{Var} \\left( \\mathbb{E}[h(X_1, X_2) | X_1] \\right) \\\\\n  \\text{Var}(U_n) &\\overset{p}{\\to} \\frac{r^2}{n} \\zeta_1 \\\\\n  \\sqrt{n}(U_n - m_2) &\\overset{d}{\\to} \\mathcal{N}(0,\\: r^2\\zeta_1) \\\\\n\\end{aligned}\n\\]\nWe begin with the expectation of the kernel conditional on \\(X_1\\):\n\\[\n\\begin{aligned}\n  h_1(X_1, X_2)\n    &= \\mathbb{E} \\left[ \\frac{(X_1 - X_2)^2}{2} + X_1X_2 | X_1 \\right] \\\\\n    &= \\frac{1}{2} \\mathbb{E}[X_1^2 -2X_1X_2 + X_2^2 | X_1] + \\mathbb{E}[X_1X_2 | X_1] \\\\\n    &= \\frac{1}{2}(X_1^2 - 2X_1\\mu + m_2) + X_1\\mu \\\\\n    &= \\frac{X_1^2 + m_2}{2}\n\\end{aligned}\n\\]\nFrom which we can derive \\(\\zeta_1\\):\n\\[\n\\begin{aligned}\n  \\zeta_1\n    &= \\text{Var} \\left( \\frac{X_1^2 + m_2}{2} \\right) \\\\\n    &= \\frac{1}{4} \\text{Var}(X_1^2 + m_2) \\\\\n    &= \\frac{1}{4} \\left( \\text{Var}(X_1^2) + \\text{Var}(m_2) \\right) \\\\\n    &= \\frac{1}{4} \\text{Var}(X_1^2 \\\\\n    &= \\frac{1}{4} \\mathbb{E} \\left[ (X_1^2 - m_2)^2 \\right] \\\\\n    &= \\frac{1}{4}(m_4 - 2m_2m_2 + m_2^2) \\\\\n    &= \\frac{m_4 - m_2^2}{4} \\\\\n\\end{aligned}\n\\]\nFinally, since our kernel has degree \\(r = 2\\):\n\\[\n\\sqrt{n}(U_n - m_2) \\overset{d}{\\to} \\mathcal{N}(0,\\: m_4 - m_2^2)\n\\]\nWe can confirm this using simulation as well. First, we define a new function that computes our kernel:\n\n\nCode\nh_ij &lt;- function(x_i, x_j) {\n  m2_ij &lt;- ((x_i - x_j)^2) / 2 + x_i * x_j\n  return(m2_ij)\n}\n\n\nWe’ll simulate observations from the following distribution. From the definition of variance, we know that \\(m_2 = \\text{Var}(X) + \\left( \\mathbb{E}(X) \\right)^2\\), which in our case is: \\(m_2 = 1 + 2^2 = 5\\). This is the value against which we’ll compare our U-statistics, whose expectation is \\(m_2\\).\n\\[\nX_1, \\dots, X_n \\overset{\\small{\\text{IID}}}{\\sim} \\mathcal{N}(2,\\: 1)\n\\]\nNow we iterate over a range of possible values for \\(n\\), running the simulation 3x per value, and recording the U-statistics that we estimate.\n\n\nCode\nn_vals &lt;- purrr::map(c(5, 10, 15, 25, 40, 50, 75, 100, 150, 200, 250, 300, 400, 500, 750, 1000, 1250), \\(x) rep(x, 3)) %&gt;% \n          purrr::reduce(c)\nU_n_vals &lt;- numeric(length = length(n_vals)) \nfor (n in seq(n_vals)) {\n  set.seed(n)\n  sample_n &lt;- n_vals[n]\n  mu &lt;- 2\n  sigma &lt;- 1\n  x &lt;- rnorm(sample_n, mean = mu, sd = sigma)\n  i &lt;- 1\n  U_sum &lt;- 0\n  while (i &lt;= sample_n) {\n    j &lt;- i + 1\n    while (j &lt;= sample_n) {\n      h_val &lt;- h_ij(x_i = x[i], x_j = x[j])\n      U_sum &lt;- U_sum + h_val\n      j &lt;- j + 1\n    }\n    i &lt;- i + 1\n  }\n  U_n &lt;- choose(sample_n, 2)^(-1) * U_sum\n  U_n_vals[n] &lt;- U_n\n}\n\n\nPlotting the results, we see a monotonically decreasing trend of the absolute error of \\(U_n\\) when compared to the true value \\(m_2 = 5\\).\n\n\nCode\ndata.frame(U = U_n_vals, \n           N = n_vals) %&gt;% \n  ggplot(aes(x = N, y = abs(U - 5))) + \n  geom_point() + \n  geom_smooth(color = \"forestgreen\", se = FALSE) + \n  labs(x = latex2exp::TeX(r\"($\\textit{n}$)\"), \n       y = latex2exp::TeX(r\"($|\\textit{U_n} - \\theta|$)\")) + \n  theme_classic(base_size = 14)"
  },
  {
    "objectID": "derivations/Large_Sample_Practice.html#example-3.2---zepeda-tello-r.-et-al",
    "href": "derivations/Large_Sample_Practice.html#example-3.2---zepeda-tello-r.-et-al",
    "title": "Assorted Large Sample Theory Practice Problems",
    "section": "Example 3.2 - Zepeda-Tello, R. et al",
    "text": "Example 3.2 - Zepeda-Tello, R. et al\nThis example is pulled from The delta-method and influence function in medical statistics: A reproducible tutorial, a preprint from June 2022. Section 3 includes several examples of asymptotic distributions of estimators derived using the functional delta method. Example 3.2 shows the derivation of the asymptotic distribution of the sample mean using the influence function instead of the Central Limit Theorem; they use a discrete distribution for simplicity, but here we’ll use a continuous distribution instead.\nWe assume that observations are generated from the following distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\):\n\\[\nX_1, \\dots, X_n \\overset{\\small{\\text{IID}}}{\\sim} F\n\\]\nThe estimator can be formulated as a functional, with \\(\\phi(\\cdot)\\) simply being the identity function:\n\\[\n\\begin{aligned}\n  \\Psi\n    &= \\phi(\\mathbb{P}_X) \\\\\n    &= \\phi(\\theta) \\\\\n    &= \\mu \\\\\n  \\widehat{\\Psi}_n\n    &= \\phi(\\widehat{P}_X) \\\\\n    &= \\phi(\\hat{\\theta}_n) \\\\\n    &= \\bar{X}_n \\\\\n\\end{aligned}\n\\]\nVia a Taylor expansion, we have:\n\\[\n\\begin{aligned}\n\\widehat{\\Psi}_n &\\approx \\Psi + IF(X) \\\\\n\\implies IF(X) &= \\bar{X}_n - \\mu + o_p(1) \\\\\n\\end{aligned}\n\\]\nThe asymptotic distribution is as follows; we note that the expectation of the influence function is always equal to zero, and thus its variance is equal to its second raw moment.\n\\[\n\\phi(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} \\mathcal{N} \\left( 0,\\: \\text{Var} \\left( IF(X) \\right) \\right)\n\\]\nWe derive the variance of the influence function, and obtain the same result as we would have in using the CLT:\n\\[\n\\begin{aligned}\n  \\text{Var} \\left( IF(X) \\right)\n    &= \\text{Var} \\left( \\bar{X}_n - \\mu \\right) \\\\\n    &= \\text{Var} \\left( n^{-1} \\sum_{i=1}^n X_i \\right) \\\\\n    &= n^{-2} \\sum_{i=1}^n \\text{Var}(X_i) \\\\\n    &= n^{-1}\\sigma^2 \\\\\n\\end{aligned}\n\\]\nThus the asymptotic distribution of the sample mean is:\n\\[\n\\sqrt{n} \\left( \\bar{X}_n - \\mu \\right) \\overset{d}{\\to} \\mathcal{N}(0,\\: \\sigma^2)\n\\]"
  },
  {
    "objectID": "derivations/Large_Sample_Practice.html#exercise-12.7---asymptotic-statistics",
    "href": "derivations/Large_Sample_Practice.html#exercise-12.7---asymptotic-statistics",
    "title": "Assorted Large Sample Theory Practice Problems",
    "section": "Exercise 12.7 - Asymptotic Statistics",
    "text": "Exercise 12.7 - Asymptotic Statistics\nI’m modifying this question slightly; the original asks for the asymptotic distribution of the U-statistic for \\(\\mu^2\\), and in addition we’ll derive its joint distribution with the U-statistic for \\(m_2\\) that we found previously using influence functions.\nFirst, we define the given quantities:\n\\[\n\\begin{aligned}\n  X_1, \\dots, X_n &\\overset{\\small{\\text{IID}}}{\\sim} F \\\\\n  \\mathbb{E} \\left[ X^2_1 \\right] &&lt; \\infty \\\\\n\\end{aligned}\n\\]\nWe define the following symmetric kernel to use in the U-statistic for \\(\\mu^2\\):\n\\[\nh(X_1, X_2) = X_1X_2\n\\]\nThe expectation of that kernel is given by:\n\\[\n\\begin{aligned}\n  \\mathbb{E} \\left[ h(X_1, X_2) \\right]\n    &= \\mathbb{E}[X_1X_2] \\\\\n    &= \\mathbb{E}[X_1]\\mathbb{E}[X_2] \\\\\n    &= \\mu^2 \\\\\n\\end{aligned}\n\\]\nThe U-statistic is thus:\n\\[\nU_n = \\binom{n}{2}^{-1} \\sum_{i=1}^n \\sum_{i&lt;j} X_iX_j\n\\]\nNext we derive the asymptotic distribution of the U-statistic:\n\\[\n\\begin{aligned}\n  h_1(X_1, X_2)\n    &= \\mathbb{E} \\left[ h(X_1, X_2) | X_1 \\right] \\\\\n    &= \\mathbb{E}[X_1X_2 | X_1] \\\\\n    &= X_1\\mu \\\\\n  \\implies \\zeta_1\n    &= \\mathbb{E} \\left[ (h_1^c(X_1, X_2))^2 \\right] \\\\\n    &= \\mathbb{E} \\left[(h_1(X_1, X_2) - \\mu^2)^2 \\right] \\\\\n    &= \\text{Var}(h_1(X_1, X_2)) \\\\\n    &= \\text{Var}(X_1\\mu) \\\\\n    &= \\mu^2\\text{Var}(X_1) \\\\\n    &= \\mu^2\\sigma^2 \\\\\n  \\implies \\sqrt{n}(U_n - \\mu^2) &\\overset{d}{\\to} \\mathcal{N}(0,\\: 4\\mu^2\\sigma^2)\n\\end{aligned}\n\\]\nThe influence function of a U-statistic is given by:\n\\[\nIF_U(X) = r h_1^c(X_1, \\dots, X_r)\n\\]\nThus the influence function for our U-statistic is:\n\\[\nIF_U(X) = 2\\mu(X_1 - \\mu)\n\\]\nRemembering the U-statistic we derived earlier for \\(m_2\\), which we’ll now refer to as \\(U^*\\) in order to distinguish it from the other statistic:\n\\[\nU_n^* = \\binom{n}{2}^{-1} \\sum_{i=1}^n \\sum_{i&lt;j} \\frac{(X_i - X_j)^2}{2} + X_iX_j\n\\]\nWe now define its influence function as:\n\\[\n\\begin{aligned}\n  IF_{U^*}(X)\n    &= 2 \\left( \\frac{X_1^2 + m_2}{2} - m_2 \\right) \\\\\n    &= X_1^2 - m_2 \\\\\n\\end{aligned}\n\\]\nThus, the joint distribution of the two U-statistics is:\n\\[\n\\sqrt{n} \\begin{pmatrix} U_n - \\mu^2 \\\\ U_n^* - m_2 \\end{pmatrix}\n\\overset{d}{\\to}\n\\boldsymbol{\\mathcal{N}}_2 \\left(\\mathbf{0},\\:\n  \\begin{pmatrix}\n    \\text{Var} \\left( IF_U \\right) & \\text{Cov} \\left( IF_U, IF_{U^*} \\right) \\\\\n    \\text{Cov} \\left( IF_U, IF_{U^*} \\right) & \\text{Var} \\left( IF_{U^*} \\right) \\\\\n  \\end{pmatrix} \\right)\n\\]\nThe variance of an influence function is equal to its second raw moment, as its first raw moment is always equal to zero. Ergo, the variances of the two U-statistics are as follows (they should match the asymptotic variances from earlier):\n\\[\n\\begin{aligned}\n  \\text{Var} \\left( IF_U(X) \\right)\n    &= \\mathbb{E} \\left[ IF_U(X)^2 \\right] \\\\\n    &= \\mathbb{E} \\left[ (2\\mu(X_1 - \\mu))^2 \\right] \\\\\n    &= \\mathbb{E} \\left[ 4\\mu^2(X_1 - \\mu)^2 \\right] \\\\\n    &= 4\\mu^2\\sigma^2 \\\\\n  \\implies \\text{Var} \\left( IF_{U^*}(X) \\right)\n    &= \\mathbb{E} \\left[ (X_1^2 - m_2)^2 \\right] \\\\\n    &= \\text{Var} \\left( X_1^2 \\right) \\\\\n    &= \\mathbb{E} \\left[ (X_1^2)^2 \\right] - \\left( \\mathbb{E} \\left[ X_1^2 \\right] \\right)^2 \\\\\n    &= m_4 - m_2^2 \\\\\n\\end{aligned}\n\\]\nLastly, the covariance term:\n\\[\n\\begin{aligned}\n  \\text{Cov} \\left( IF_U, IF_{U^*} \\right)\n    &= \\mathbb{E} \\left[ IF_U IF_{U^*} \\right] - \\mathbb{E}[IF_U]\\mathbb{E}[IF_{U^*}] \\\\\n    &= \\mathbb{E} \\left[ 2\\mu(X_1 - \\mu)(X_1 - m_2) \\right] - \\mathbb{E}[2\\mu(X_1 - \\mu)] \\mathbb{E}[X_1 - m_2] \\\\\n    &= \\mathbb{E}[2\\mu(X_1^2 - X_1m_2 - \\mu X_1 + \\mu m_2)] - 2\\mu \\left( \\mathbb{E}[X_1] - \\mu \\right)(\\mu - m_2) \\\\\n    &= 2\\mu(m_2 - \\mu m_2 - \\mu^2 + \\mu m_2) \\\\\n    &= 2\\mu(m_2 - \\mu^2) \\\\\n\\end{aligned}\n\\]\nThus we have arrived at the joint asymptotic distribution:\n\\[\n\\sqrt{n} \\begin{pmatrix} U_n - \\mu^2 \\\\ U_n^* - m_2 \\end{pmatrix}\n\\overset{d}{\\to}\n\\boldsymbol{\\mathcal{N}}_2 \\left(\\mathbf{0},\\:\n  \\begin{pmatrix}\n    4\\mu^2\\sigma^2 & 2\\mu(m_2 - \\mu^2) \\\\\n    2\\mu(m_2 - \\mu^2) & m_4 - m_2^2 \\\\\n  \\end{pmatrix} \\right)\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Background",
    "section": "",
    "text": "My undergrad degree is in statistics, which I did at the University of North Carolina at Chapel Hill (Go Heels!). After graduating in 2020, I spent some of the next two years earning a Master’s in biostatistics from the University of Florida. During my second year in that program, I applied to PhD programs and ended up deciding to continue at UF and start a PhD in the same department. I’m currently entering the first semester of my second year. My most relevant studies include courses on generalized linear models, machine learning theory, convex optimization, probability theory, mathematical statistics, and statistical software development."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Background",
    "section": "",
    "text": "My undergrad degree is in statistics, which I did at the University of North Carolina at Chapel Hill (Go Heels!). After graduating in 2020, I spent some of the next two years earning a Master’s in biostatistics from the University of Florida. During my second year in that program, I applied to PhD programs and ended up deciding to continue at UF and start a PhD in the same department. I’m currently entering the first semester of my second year. My most relevant studies include courses on generalized linear models, machine learning theory, convex optimization, probability theory, mathematical statistics, and statistical software development."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Background",
    "section": "Work Experience",
    "text": "Work Experience\nIn the summer after my junior year at UNC, I joined Dr. Jen Jen Yeh’s lab at Lineberger Comprehensive Cancer Center. There I learned how to process and analyze genomic and transcriptomic data, and in Fall 2019 started focusing on developing methods for scRNA-seq analysis. My primary contribution there was the development of reproducible bioinformatics pipelines for bulk & single cell RNA-seq, ChIP-seq, and whole genome & exome sample processing. In addition, I developed a robust, evidence-based downstream analysis workflow for the lab’s scRNA-seq samples.\nAfter leaving the Yeh Lab in July 2020, I moved to Jacksonville and started working full-time as a junior data analyst at Blue Cross Blue Shield of Florida. While there, I completed a year-long rotational program during which I applied regression and clustering methods to several complex business problems. A highlight was my usage of graph-based clustering and dimension reduction algorithms (concepts lifted directly from my experience with scRNA-seq data) to identify well-performing subsets of specialist doctors. After my rotations ended, I became a full analyst and spent the next year working in care analytics, where I tested and deployed machine learning algorithms used to identify people to be targeted for various preventive healthcare programs. During this time, I also completed a manuscript started while I was at the Yeh Lab detailing a method for improved clustering of single cell data containing rare cell types.\nWhile all this was going on, I also started doing research work (once again focused on scRNA-seq method development) in Dr. Rhonda Bacher’s group at UF. Our original focus was in profiling, testing, and improving existing methods for RNA velocity and trajectory inference through simulations and analysis of publicly available single cell datasets.\nAfter two years at Blue Cross, and graduating with my master’s in May of 2022, I decided to join Dr. Bacher as a PhD student in Fall 2022. My first semester was spent pursuing scRNA-seq research, with a focus on improving the interpretability of trajectory differential expression methods. I also provided downstream analysis of single cell data generated by Dr. Phillip Efron’s lab containing different types of peripheral blood mononuclear cells from patients presenting with sepsis at UF’s emergency room. In addition, I was the instructor for the online section of PHC 6790 (Biostatistical Computing with SAS), a master’s-level course developed by fellow Tar Heel Dr. John Kairalla."
  },
  {
    "objectID": "about.html#personal-interests",
    "href": "about.html#personal-interests",
    "title": "Background",
    "section": "Personal Interests",
    "text": "Personal Interests\nOutside of research, I enjoy reading (my favorite genres are nautical adventure, weird history, and narrative science), riding my bike, cooking for my friends, and rock climbing."
  },
  {
    "objectID": "about.html#r-packages",
    "href": "about.html#r-packages",
    "title": "Background",
    "section": "R Packages",
    "text": "R Packages\n\nSCISSORS\n\nAn extension of the Seurat framework that implements a semi-supervised scRNA-seq reclustering method built around the silhouette scores. Useful for identifying rare cell types & annotating cell subtypes.\n\nscLANE\n\nEver wished scRNA-seq trajectory differential expression models were more interpretable? This package implements piecewise linear GLMs, GEEs, & GLMMs in order to generate easier-to-understand models that perform just as well as GAM-based methods at classifying temporally dynamic genes."
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "Background",
    "section": "Publications",
    "text": "Publications\n\nJack R. Leary, Yi Xu, Ashley Morrison, Chong Jin, Emily C. Shen, Ye Su, Naim U. Rashid, Jen Jen Yeh, Xianlu Laura Peng. Sub-cluster identification through semi-supervised optimization of rare-cell silhouettes (SCISSORS) in single-cell sequencing. Bioinformatics (2023).\nMatthew E. Berginski, Madison R. Jenner, Chinmaya U. Joisa, Silvia G.Herrera Loeza, Brian T. Golitz, Matthew B. Lipner, Jack R. Leary, Naim U. Rashid, Gary L. Johnson, Jen Jen Yeh, Shawn M. Gomez. Kinome state is predictive of cell viability in pancreatic cancer tumor and stroma cell lines. BioRxiv (2021).\nJack R. Leary, Rhonda Bacher. Interpretable trajectory inference with single-cell Linear Adaptive Negative-binomial Expression (scLANE) testing. BioRxiv (2023).\nXiaoru Dong, Jack R. Leary, Chuanhao Yang, Maigan A. Brusko, Todd M. Brusko, Rhonda Bacher. Data-driven selection of analysis decisions in single-cell RNA-seq trajectory inference. BioRxiv (2023)."
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html",
    "href": "derivations/Intercept_Interpretation.html",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "",
    "text": "Code\nlibrary(dplyr)    # data manipulation\nlibrary(ggplot2)  # plots"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#multiplication",
    "href": "derivations/Intercept_Interpretation.html#multiplication",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Multiplication",
    "text": "Multiplication\n\nTheory\nFirst define two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), each with 2 rows and 2 columns:\n\\[\n\\begin{aligned}\n\\mathbf{A} &=\n    \\begin{bmatrix}\n      a_{11} & a_{21} \\\\\n      a_{12} & a_{22} \\\\\n    \\end{bmatrix} \\\\\n\\mathbf{B} &=\n  \\begin{bmatrix}\n    b_{11} & b_{21} \\\\\n    b_{12} & b_{22} \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nTheir product, another matrix \\(C\\), also has 2 rows and 2 columns, and its elements are defined like so, with \\(i\\) specifying the row and \\(j\\) the column of each element. What we’re doing is finding the dot product of the \\(i^{\\text{th}}\\) row of \\(\\mathbf{A}\\) and the \\(j^{\\text{th}}\\) column of \\(\\mathbf{B}\\), the expanded definition of which is below.\n\\[\n\\begin{aligned}\nc_{ij} &= \\mathbf{A}_{i*} \\cdot \\mathbf{B}_{*j} \\\\\nc_{ij} &= \\sum_{k=1}^n a_{ik}b_{kj} \\\\\nc_{ij} &= a_{i1}b_{1j} + \\dots + a_{n1}b_{nj} \\\\\n\\end{aligned}\n\\]\nAs such, we can define the product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) like so:\n\\[\n\\begin{aligned}\n\\mathbf{C} &=  \\mathbf{A} \\mathbf{B} \\\\\n\\mathbf{C} &=\n  \\begin{bmatrix}\n    \\mathbf{A}_{1*} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*1} \\\\\n    \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*2} \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{C} &=\n  \\begin{bmatrix}\n    a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n    a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nImportant Note: To multiply two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) together, the number of rows of \\(\\mathbf{B}\\) must be equal to the number of columns in \\(\\mathbf{A}\\). To generalize:\n\\[\n\\mathbf{A}_{m \\times n} \\cdot \\mathbf{B}_{n \\times p} = \\mathbf{C}_{m \\times p}\n\\]\n\n\nExample\nLet’s define two matrices:\n\\[\n\\begin{aligned}\n\\mathbf{A} &=\n  \\begin{bmatrix}\n    3 & 2 \\\\\n    0 & 7 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{B} &=\n  \\begin{bmatrix}\n    1 & 4 \\\\\n    1 & 2 \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nTheir product \\(\\mathbf{C}\\) is defined as:\n\\[\n\\begin{aligned}\n\\mathbf{C} &=\n  \\begin{bmatrix}\n    3 \\times 1 + 2 \\times 1 & 3 \\times 4 + 2 \\times 2 \\\\\n    0 \\times 1 + 7 \\times 1 & 0 \\times 4 + 7 \\times 2 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{C} &=\n  \\begin{bmatrix}\n    5 & 16 \\\\\n    7 & 14 \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nWe can check this using R:\n\n\nCode\nA_mat &lt;- matrix(c(3, 2, 0, 7), \n                nrow = 2, \n                ncol = 2, \n                byrow = TRUE)\nB_mat &lt;- matrix(c(1, 4, 1, 2), \n                nrow = 2, \n                ncol = 2, \n                byrow = TRUE)\nC_mat &lt;- A_mat %*% B_mat\nC_mat\n\n\n     [,1] [,2]\n[1,]    5   16\n[2,]    7   14"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#transposition",
    "href": "derivations/Intercept_Interpretation.html#transposition",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Transposition",
    "text": "Transposition\n\nTheory\nVery simply, the transpose of a matrix can be thought of as simply flipping the rows & columns. The transpose of an \\(m \\times n\\) matrix is thus \\(n \\times m\\). A matrix that is its own transpose is symmetric. Notation-wise, some write the matrix transpose as \\(A^T\\), others as \\(A^\\mathsf{T}\\), and still others denote it by \\(A^\\intercal\\), but I personally prefer \\(A^\\prime\\). The matrix transpose is more formally defined as:\n\\[\n\\begin{aligned}\n  & \\mathbf{A}_{m \\times n} \\\\\n  & \\mathbf{B}_{n \\times n} = A^\\prime \\\\\n  & \\mathbf{B}_{i, j} = \\mathbf{A}_{j, i} \\\\\n\\end{aligned}\n\\]\n\n\nExample\nIf we define the following matrix \\(\\mathbf{A}_{2 \\times 3}\\), we would expect its transpose to be the \\(3 \\times 2\\) matrix \\(\\mathbf{A}^\\prime\\):\n\\[\n\\begin{aligned}\n\\mathbf{A} &=\n  \\begin{bmatrix}\n    0 & -2 & 2 \\\\\n    3 & -10 & 0 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{A}^\\prime &=\n  \\begin{bmatrix}\n    0 & 3 \\\\\n    -2 & -10 \\\\\n    2 & 0 \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n\\]\nWe can confirm this in R using the t() function:\n\n\nCode\nA &lt;- matrix(c(0, -2, 2, 3, -10, 0), \n            nrow = 2, \n            ncol = 3, \n            byrow = TRUE)\nt(A)\n\n\n     [,1] [,2]\n[1,]    0    3\n[2,]   -2  -10\n[3,]    2    0"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#inversion",
    "href": "derivations/Intercept_Interpretation.html#inversion",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Inversion",
    "text": "Inversion\n\nTheory\nThe last matrix operation we’ll go over is inversion - this one is very important & is applied all throughout statistics & computing in general. The inverse of a matrix is simply another matrix that, when multiple by the first matrix, returns the identity matrix (a \\(n \\times n\\) matrix with 1s on the diagonal and 0s everywhere else). Matrices must be square to be invertible, but not even all square matrices are invertible; the ones that aren’t are called singular. We’ll gloss over that fact though, & simply assume / fervently hope that the matrices we encounter will have an easily-computable (approximate) inverse. See this article for more information on inverse computation. We define the inverse of \\(\\mathbf{A}_{n \\times n}\\) as:\n\\[\n\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}_n\n\\]\n\n\nExample\nIn R, the solve() function is used most frequently to compute the matrix inverse; since the result is an approximation, we round the results to show that the result is the identity matrix:\n\n\nCode\nA &lt;- matrix(rnorm(9), nrow = 3, ncol = 3)\nA_inv &lt;- solve(A)\nround(A %*% A_inv)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nThere are many other ways to compute a matrix inverse, as well as several different types of pseudoinverses. We can compute the Moore-Penrose pseudoinverse using the MASS package:\n\n\nCode\nA_mp_inv &lt;- MASS::ginv(A)\nround(A %*% A_mp_inv)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#the-identity-matrix",
    "href": "derivations/Intercept_Interpretation.html#the-identity-matrix",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "The Identity Matrix",
    "text": "The Identity Matrix\n\nTheory\nAs mentioned previously, the identity matrix \\(\\mathbf{I}_{n}\\) is a square matrix composed entirely of zeroes except along the diagonal, which is composed of ones. This matrix carries some unique properties (which are listed below) that will be helpful to us later on.\n\\[\n\\begin{aligned}\n\\mathbf{I}_{n} &=\n  \\begin{bmatrix}\n    1 & 0 & \\cdots & 0 \\\\\n    0 & 1 & \\cdots & 0 \\\\\n    \\vdots & \\vdots & \\ddots & 0 \\\\\n    0 & 0 & 0 & 1 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{I}_{n}^\\prime &= \\mathbf{I}_{n} \\\\\n\\mathbf{I}_{n}^{-1} &= \\mathbf{I}_{n} \\\\\n\\end{aligned}\n\\]\n\n\nExample\nWe can set up a \\(3 \\times 3\\) identity matrix \\(\\mathbf{I}_{3}\\) in R using the diag() function:\n\n\nCode\nident_mat &lt;- diag(nrow = 3)\nident_mat\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nThe transpose is also equal to \\(\\mathbf{I}_{3}\\):\n\n\nCode\nt(ident_mat)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nAs is the inverse:\n\n\nCode\nsolve(ident_mat)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#setup",
    "href": "derivations/Intercept_Interpretation.html#setup",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Setup",
    "text": "Setup\nFor now, we’ll take it for granted that the solution to a linear regression problem is defined as follows:\n\\[\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y}\n\\]"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#the-intercept-only-model",
    "href": "derivations/Intercept_Interpretation.html#the-intercept-only-model",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "The Intercept-only Model",
    "text": "The Intercept-only Model\nThe intercept-only model (also sometimes called the null model) is defined as linear regression when \\(\\mathbf{X}\\) is simply a column vector of ones:\n\\[\n\\mathbf{X} =\n  \\begin{bmatrix}\n    1 \\\\\n    \\vdots \\\\\n    1 \\\\\n  \\end{bmatrix}\n\\]\nWe know the intercept-only model produces the mean as the one predicted value, as the mean minimizes the sum of squared errors in the absence of any other covariates. We can check this using R - we’ll first generate a vector \\(\\mathbf{y}\\) consisting of 5 realizations of a random variable, such that \\(Y \\sim \\mathcal{N}(0, 3)\\).\n\n\nCode\ny &lt;- rnorm(5, mean = 0, sd = 3)\ny &lt;- matrix(y, ncol = 1)\ny\n\n\n          [,1]\n[1,] -5.415350\n[2,]  4.356184\n[3,] -2.261118\n[4,]  3.439089\n[5,] -2.163296\n\n\nThe mean of \\(\\mathbf{y}\\) is:\n\n\nCode\nmean(y)\n\n\n[1] -0.4088981\n\n\nWe can use R to fit an intercept-only model. We can see that the intercept coefficient \\(\\beta_0\\) is equal to the mean of \\(\\mathbf{y}\\).\n\n\nCode\nnull_mod &lt;- lm(y ~ 1)\ncoef(null_mod)\n\n\n(Intercept) \n -0.4088981 \n\n\nLet’s use linear algebra to figure out why this is true. Once again, we know that the linear regression closed-form solution is given by the following:\n\\[\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y}\n\\]\nLet’s first define \\(\\mathbf{X}\\) - just a column vector of 1s with \\(n = 5\\) rows:\n\n\nCode\nX &lt;- c(1, 1, 1, 1, 1)\nX &lt;- matrix(X, ncol = 1)\nX\n\n\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    1\n[5,]    1\n\n\nThe value of \\(\\mathbf{X}^\\prime \\mathbf{X}\\) is given by the following - note that this is equal to our sample size \\(n = 5\\). We knew that this quantity would be a scalar (a \\(1 \\times 1\\) matrix) since \\(\\mathbf{X}^\\prime\\) has 1 row and 5 columns, and \\(\\mathbf{X}\\) has 5 rows and 1 column, thus by the rule we defined above their product has 1 row and 1 column.\n\n\nCode\nt(X) %*% X\n\n\n     [,1]\n[1,]    5\n\n\nThe inverse of which, \\(\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1}\\), is of course \\(n^{-1}\\):\n\n\nCode\nsolve(t(X) %*% X)\n\n\n     [,1]\n[1,]  0.2\n\n\nWe multiply the above by \\(\\mathbf{X}^\\prime\\) again to obtain \\(\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime\\), which gives us a constant vector of length \\(n\\) with all values being equal to \\(n^{-1}\\):\n\n\nCode\nsolve(t(X) %*% X) %*% t(X)\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.2  0.2  0.2  0.2  0.2\n\n\nLastly, we multiply the above by \\(\\mathbf{y}\\). Remember how multiplying vectors works - in this case we are multiplying each element of the above vector \\(\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime\\) with each element of \\(\\mathbf{y}\\) and adding them together. We’ll define \\(\\mathbf{Z} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime\\) for convenience of notation:\n\\[\n\\mathbf{Z} \\mathbf{y} = \\sum_{i=1}^n \\mathbf{Z}_i \\mathbf{y}_i\n\\]\nSince each element of \\(\\mathbf{Z}\\) is the same, \\(n^{-1}\\), by the transitive property the above quantity is equivalent to:\n\\[\n\\begin{aligned}\n  \\mathbf{Z} \\mathbf{y} &= \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y} \\\\\n  \\mathbf{Z} \\mathbf{y} &= \\sum_{i=1}^n \\mathbf{Z}_i \\mathbf{y}_i \\\\\n  \\mathbf{Z} \\mathbf{y} &= n^{-1} \\sum_{i=1}^n \\mathbf{y}_i \\\\\n\\end{aligned}\n\\]\nThis is simply the sum of all the elements of \\(\\mathbf{y}\\) divided by \\(n\\) - the mean! We can verify this with R by using linear algebra to compute the OLS solution:\n\n\nCode\nsolve(t(X) %*% X) %*% t(X) %*% y\n\n\n           [,1]\n[1,] -0.4088981\n\n\nThis is equal to simply taking the mean of \\(\\mathbf{y}\\):\n\n\nCode\nmean(y)\n\n\n[1] -0.4088981"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-categorical-predictors",
    "href": "derivations/Intercept_Interpretation.html#models-with-categorical-predictors",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Categorical Predictors",
    "text": "Models with Categorical Predictors\nIn practice of course we usually build models with predictors of interest outside of the intercept. Categorical variables are composed of discrete values, each with a different meaning e.g., we could have a variable containing the type of treatment a patient has received. In order to fit models with categorical variables, it’s necessary to expand a categorical variable into multiple indicator variables - variables composed of 1s and 0s depending on whether a certain observation belongs to a certain category. This is a little confusing, so let’s show an example. We’ll start by creating a categorical variable .\n\n\nCode\nX &lt;- sample(c(\"A\", \"B\"), size = 10, replace = TRUE)\nX &lt;- matrix(X, ncol = 1)\nX\n\n\n      [,1]\n [1,] \"B\" \n [2,] \"B\" \n [3,] \"B\" \n [4,] \"A\" \n [5,] \"B\" \n [6,] \"B\" \n [7,] \"A\" \n [8,] \"B\" \n [9,] \"B\" \n[10,] \"A\" \n\n\nTo convert \\(\\mathbf{X}\\) into a numeric variable that we can use in a model, we use the model.matrix() function. To use this function though, we need to define the model we’re interested in using R’s formula syntax. The output we see shows an intercept column, which we understand, and another column composed of 1s and 0s called XB. This column is an indicator variable that tells us whether each observation belongs to category B or not - thus when XB is equal to 0, we know that the observation belongs to category A. This process of converting non-numeric categorical data to indicator variables has many names (one-hot encoding, dummifying, etc.), and there’s many ways of doing it. You can read more about the various ways of doing so here, but for now we’ll take it for granted that this is how it works under the hood in the lm() function that we use to fit linear models.\n\n\nCode\nX_2 &lt;- model.matrix(~X)\nX_2\n\n\n   (Intercept) XB\n1            1  1\n2            1  1\n3            1  1\n4            1  0\n5            1  1\n6            1  1\n7            1  0\n8            1  1\n9            1  1\n10           1  0\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$X\n[1] \"contr.treatment\"\n\n\nFrom what we already know about matrix multiplication, we can see that the B group is going to have a different predicted average than the A group. We’ll also need to generate a new response variable \\(\\mathbf{y}\\), since we’ve increased our sample size to \\(n = 10\\).\n\n\nCode\ny &lt;- rnorm(10, mean = 0, sd = 3)\ny &lt;- matrix(y, ncol = 1)\ny\n\n\n            [,1]\n [1,] -5.8375020\n [2,] -1.8924219\n [3,]  2.9715616\n [4,] -1.5104106\n [5,] -1.1778186\n [6,]  1.3690104\n [7,] -0.5117578\n [8,]  1.8279983\n [9,]  1.9796838\n[10,] -4.7857809\n\n\nThe mean of \\(\\mathbf{y}\\) for each treatment group can be computed as follows. We’re going to use a little dplyr code to perform the summarization, as I find it a little more readable & replicable than base R. This will necessitate creating a data.frame to hold our \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) variables. Note that we’ve switched back to the categorical representation of \\(\\mathbf{X}\\), as it’s more interpretable than the indicator variable version for summaries such as this.\n\n\nCode\ndata.frame(X = X, y = y)\n\n\n\n\n\n\nX\ny\n\n\n\n\nB\n-5.8375020\n\n\nB\n-1.8924219\n\n\nB\n2.9715616\n\n\nA\n-1.5104106\n\n\nB\n-1.1778186\n\n\nB\n1.3690104\n\n\nA\n-0.5117578\n\n\nB\n1.8279983\n\n\nB\n1.9796838\n\n\nA\n-4.7857809\n\n\n\n\n\n\nHere’s the mean for each group:\n\n\nCode\ndata.frame(X = X, y = y) %&gt;% \n  with_groups(X, \n              summarise, \n              mu = mean(y))\n\n\n\n\n\n\nX\nmu\n\n\n\n\nA\n-2.2693164\n\n\nB\n-0.1084983\n\n\n\n\n\n\nLet’s use the OLS formula to solve for \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]\\). Note that we’re once again using the design matrix version of \\(\\mathbf{X}\\) with the intercept column and indicator variable for group. We see that the intercept \\(\\beta_0\\) is equal to the mean of the A group - but the coefficient for the B group \\(\\beta_1\\) isn’t! This is because \\(\\beta_1\\) doesn’t have the same interpretation as \\(\\beta_0\\). While \\(\\beta_0\\) is equal to the mean of the reference group (i.e., the first level of the categorical variable, in our case group A), \\(\\beta_1\\) represents the difference between the mean for group A and the mean for group B.\n\n\nCode\ncat_mod_beta &lt;- solve(t(X_2) %*% X_2) %*% t(X_2) %*% y\ncat_mod_beta\n\n\n                 [,1]\n(Intercept) -2.269316\nXB           2.160818\n\n\nThis becomes easier to understand when we sum the coefficients and get the average for group B, which is -0.1084983.\n\n\nCode\nsum(cat_mod_beta)\n\n\n[1] -0.1084983\n\n\nThis is validated by fitting a linear model with lm() and checking the output, which matches our manual solution:\n\n\nCode\ncat_mod &lt;- lm(y ~ X)\ncoef(cat_mod)\n\n\n(Intercept)          XB \n  -2.269316    2.160818 \n\n\nTo summarize: when categorical variables are used in an ordinary linear regression, the intercept represents the mean of the response variable when each of the categorical variables is at its reference level. When running regressions like this, it’s important to make sure that 1) you know what the reference levels are for each of your categorical variables and 2) that those reference levels make sense. Sometimes it doesn’t matter what order the categorical variables are in, but it often does - so check! A final note is that this interpretation holds when only categorical variables are used in your model. When continuous variables are included too, the interpretation changes. More on that in a bit.\n\n\n\n\n\n\nNote\n\n\n\nWorking with categorical variables (or factors, as R labels them) can be confusing. If you want to gain a deeper understanding of how factors work, check out the chapter on them in the R for Data Science book. For factor-related tools, try the forcats R package, which is part of the tidyverse ecosystem and makes dealing with factors a lot simpler."
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-continuous-predictors",
    "href": "derivations/Intercept_Interpretation.html#models-with-continuous-predictors",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Continuous Predictors",
    "text": "Models with Continuous Predictors\nContinuous predictors differ from categorical ones in that they do not have a discrete set of possible values. The interpretation of the intercept thus differs. For any regression, the intercept interpretation is the value of the response when all predictors are equal to zero. What “all predictors equal to zero” means depends on the types of predictors you’re using; when predictors are categorical this implies that all predictors are at their reference levels (thanks to the indicator variable encoding that’s done in the background). With continuous variables, being equal to zero might have a reasonable interpretation or it might not, depending on what the variable is. In this case, think of the intercept like you would in middle school when learning \\(y = mx + b\\). The intercept, \\(b\\), is the value of \\(y\\) where \\(x = 0\\), like the plot below.\n\n\nCode\ndata.frame(x = rnorm(500, sd = 2)) %&gt;% \n  mutate(y = x + rnorm(500, sd = 0.5)) %&gt;% \n  ggplot(aes(x = x, y = y)) + \n  geom_point(alpha = 0.8) + \n  geom_vline(xintercept = 0, color = \"forestgreen\", size = 1) +\n  labs(x = \"X\", y = \"Y\") + \n  theme_classic(base_size = 14)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nIn some cases, this value might be meaningful - for example, if your covariate of interest was the expression of a certain gene, then zero expression has a biological meaning. In other situations it makes little sense, such as when the covariate of interest is the age of each patient in your dataset. It’s unlikely that age being equal to zero would really mean much, as newborns aren’t often part of clinical trials. There’s ways to remedy this difficulty in interpretation, but we’ll focus first on why the interpretation is the way it is.\nWe’ll start by generating some data. The predictor variable we’re interested in will be negative-binomially distributed, with \\(X \\sim \\text{NB}(10, 0.7)\\). Note that we’re using the parameterization of the negative binomial used in the rnbinom() function defaults, with the size and probability parameters. Our response variable \\(\\mathbf{y}\\) will be a function of \\(\\mathbf{X}\\) with added normally-distributed noise. Since we’ve increased our sample size to \\(n = 500\\), we’ll only look at the first few rows of each variable.\n\n\nCode\nX &lt;- rnbinom(500, size = 10, prob = 0.7)\ny &lt;- 2 * X + rnorm(500, mean = 0, sd = 2)\nX &lt;- matrix(X, ncol = 1)\ny &lt;- matrix(y, ncol = 1)\ndata.frame(X = X[, 1], y = y[, 1]) %&gt;% \n  slice_head(n = 5)\n\n\n\n\n\n\nX\ny\n\n\n\n\n6\n12.495948\n\n\n7\n15.185192\n\n\n4\n8.957864\n\n\n6\n10.231077\n\n\n3\n4.752311\n\n\n\n\n\n\nWe can plot the data using ggplot2 to get an idea of what the relationship between the two variables is.\n\n\nCode\ndata.frame(X = X[, 1], y = y[, 1]) %&gt;% \n  ggplot(aes(x = X, y = y)) + \n  geom_point() + \n  labs(x = \"X\", y = \"y\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\n\n\nUsing dplyr, we can find the mean of \\(\\mathbf{y}\\) when \\(\\mathbf{X} = 0\\).\n\n\nCode\ndata.frame(X = X[, 1], y = y) %&gt;% \n  filter(X == 0) %&gt;% \n  summarise(mu = mean(y))\n\n\n\n\n\n\nmu\n\n\n\n\n0.6471092\n\n\n\n\n\n\nLet’s fit a linear model manually and see what the coefficients are. We’ll first need to create the design matrix again using model.matrix(). This gives us a two column matrix, with the first column being the intercept (all 1s), and the second column being the negative binomial random variable \\(\\mathbf{X}\\) we just simulated. Unlike models with categorical predictors, the intercept is not simply equal to the expected value when \\(\\mathbf{X} = 0\\). Instead, the intercept is the expected value of the response variable conditional on the line of best fit that has been obtained i.e., conditional on the rest of the data in \\(\\mathbf{X}\\). See this StackOverflow post for another example.\n\n\nCode\nX_3 &lt;- model.matrix(~X)\ncont_mod_beta &lt;- solve(t(X_3) %*% X_3) %*% t(X_3) %*% y\ncont_mod_beta\n\n\n                  [,1]\n(Intercept) -0.1207059\nX            2.0299096\n\n\nWe can validate the above by fitting a linear model with lm() and checking the coefficients \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]\\), which are equal to our manually-computed coefficients.\n\n\nCode\ncont_mod &lt;- lm(y ~ X)\ncoef(cont_mod)\n\n\n(Intercept)           X \n -0.1207059   2.0299096 \n\n\n\nCentering Continuous Predictors\nOne way to make models like this more interpretable is to center continuous variables around their means. Doing so ensures that the centered version of continuous variable is equal to zero when the original version of the variable is at its mean. This can give a better interpretation to some models e.g., if the continuous variable of interest was patient age, then the intercept would be the expected value of the response for a patient of mean age. Since centering doesn’t change the units of the continuous variable, only the intercept \\(\\beta_0\\) will change i.e., the coefficient for our predictor of interest will stay the same. We can validate this by creating a centered version of \\(\\mathbf{X}\\) and re-running the regression. The scale() function centers (subtracts the mean) and standardizes (divides by the standard deviation) by default, so we need to set scale = FALSE in order to only center the data.\n\n\nCode\nX_cent &lt;- scale(X, scale = FALSE)\ncont_mod_centered &lt;- lm(y ~ X_cent)\ncoef(cont_mod_centered)\n\n\n(Intercept)      X_cent \n    8.74188     2.02991 \n\n\n\n\nStandardizing Continuous Predictors\nStandardizing (or scaling, as R calls it) can also occasionally be useful. Dividing by the standard deviation in addition to centering results in our continuous random variable having mean 0 and standard deviation 1. This does change the units of the variable though, which is important to remember. It does not, however, change the interpretation of the intercept - which remains unchanged from the model we fit above with only centering. The coefficient \\(\\beta_1\\) differs though, and now represents the change in \\(\\mathbf{y}\\) given a one standard deviation increase in \\(\\mathbf{X}\\). For more on standardization see this StatLect post and this blog post from Columbia’s Dr. Andrew Gelman.\n\n\nCode\nX_scaled &lt;- scale(X)\ncont_mod_scaled &lt;- lm(y ~ X_scaled)\ncoef(cont_mod_scaled)\n\n\n(Intercept)    X_scaled \n   8.741880    5.010312 \n\n\nLastly, with respect to standardization at least, it’s important to note that if we standardize the response variable \\(\\mathbf{y}\\) in addition to standardizing the predictor matrix \\(\\mathbf{X}\\), the intercept will disappear i.e., it will become zero. This is because after standardization, the means of both the predictor and response variables are equal to zero. Since the intercept is the mean of the response when the predictor is zero, the intercept is also zero. Note that because of how integers work in computer memory the value of the intercept shown below isn’t quite zero, but it is very close. For another example of this, see this Stackoverflow post.\n\n\nCode\ny_scaled &lt;- scale(y)\ncont_mod_resp_scaled &lt;- lm(y_scaled ~ X_scaled)\ncoef(cont_mod_resp_scaled)\n\n\n  (Intercept)      X_scaled \n-1.233443e-16  9.276516e-01"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-categorical-continuous-predictors",
    "href": "derivations/Intercept_Interpretation.html#models-with-categorical-continuous-predictors",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Categorical & Continuous Predictors",
    "text": "Models with Categorical & Continuous Predictors\nFinally, let’s put it all together. In most real-life modeling scenarios you’ll have a mix of categorical and continuous predictors, and thus care must be taken when preparing the data. You generally will want your intercept to be meaningful - whatever that means for the data you’re analyzing. In this case, we’ll simulate data under the following scenario: our response variable \\(\\mathbf{y}\\) is the expression of some gene of interest in each of \\(n = 500\\) patients, and our predictor matrix \\(\\mathbf{X}\\) is composed of one categorical variable representing a treatment, a continuous variable representing patient age in years, and another categorical variable representing the facility at which each patient was treated.\n\n\nCode\nX_df &lt;- data.frame(age = rpois(500, lambda = 30), \n                   treatment = sample(c(\"A\", \"B\"), 500, replace = TRUE), \n                   facility = sample(c(\"Hosp1\", \"Hosp2\"), 500, replace = TRUE)) %&gt;% \n        mutate(y = 2 * age + rpois(500, lambda = 10), \n               y = case_when(treatment == \"A\" ~ 0.7 * y - rpois(1, lambda = 20), \n                             TRUE ~ y), \n               y = case_when(facility == \"Hosp2\" ~ y + rpois(1, lambda = 10), \n                             TRUE ~ y))\n\n\nThe above code to might be a bit confusing - we simulate age as a Poisson random variable with a mean of 30 years, and randomly assign one of two treatments and one of two facilities to each patient. Our response variable \\(\\mathbf{y}\\) is a function of all three predictors. We start by multiplying age by two and then adding Poisson-distributed random noise. We change the slope and subtract Poisson noise for treatment group A, and add Poisson-distributed noise for facility group Hosp2. Visualizing the data should help make sense of this process:\n\n\nCode\nggplot(X_df, aes(x = age, y = y, color = treatment)) + \n  facet_wrap(~facility) + \n  geom_point(alpha = 0.8) + \n  geom_smooth(mapping = aes(group = treatment), \n              color = \"black\",\n              method = \"lm\", \n              show.legend = FALSE) + \n  labs(x = \"Age (Years)\", \n       y = \"Gene Expression\", \n       color = \"Treatment\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nWe can see that the lowest value of age in our dataset is 15, and thus it doesn’t really make sense to have our intercept correspond to age being equal to 0. Instead, we should center age, which will ensure that our intercept represents the expected value of \\(\\mathbf{y}\\) for a patient of mean age that was given treatment A at facility Hosp1. The reference groups for each categorical variable are known since R sorts categorical variables alphabetically by default (though this can be overridden through functions like relevel()).\n\n\nCode\nX_df &lt;- mutate(X_df, \n               age_centered = scale(age, scale = FALSE))\n\n\nNow that we have our centered variable, we can set up our design matrix. We use the formula syntax to specify which predictors we’re interested in.\n\n\nCode\nX_design_mat &lt;- model.matrix(~age_centered + treatment + facility, data = X_df)\nhead(X_design_mat)\n\n\n  (Intercept) age_centered treatmentB facilityHosp2\n1           1        -7.25          0             0\n2           1         1.75          1             0\n3           1        -7.25          1             1\n4           1        -0.25          0             1\n5           1         3.75          1             0\n6           1         0.75          1             1\n\n\nWe can now solve the ordinary linear regression problem by hand to obtain the coefficient vector \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]\\).\n\n\nCode\nfull_mod_beta &lt;- solve(t(X_design_mat) %*% X_design_mat) %*% t(X_design_mat) %*% X_df$y\nfull_mod_beta\n\n\n                   [,1]\n(Intercept)   33.434628\nage_centered   1.706063\ntreatmentB    37.137926\nfacilityHosp2 11.861502\n\n\nWe can verify the result once again using lm(). The interpretation for \\(\\beta_0\\) is the expected response for a patient of mean age who is taking treatment A at facility Hosp1.\n\n\nCode\nfull_mod &lt;- lm(y ~ age_centered + treatment + facility, X_df)\ncoef(full_mod)\n\n\n  (Intercept)  age_centered    treatmentB facilityHosp2 \n    33.434628      1.706063     37.137926     11.861502"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#setup-1",
    "href": "derivations/Intercept_Interpretation.html#setup-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Setup",
    "text": "Setup\nWe’ll next move to the more complicated case of the generalized linear model (GLM). We’ll start by defining the basic form of a GLM; the main difference from an ordinary linear model is that the model’s response variable is a transformation of the actual response variable. This transformation is taken via what we call a link function. There are some detail here I’m skipping over, but in practice the link function is usually the natural log, and can be others such as the logit (for logistic regression). The link function is usually denoted \\(g(\\cdot)\\), which gives us the following general form of a GLM with \\(p\\) covariates:\n\\[\ng(\\mathbb{E}[\\mathbf{y}]) = \\beta_0 + \\beta_1 \\mathbf{X}_1 + \\dots + \\beta_p\\mathbf{X}_p\n\\]\nLike ordinary linear models, GLMs are linear in their covariates (as shown above), which is what gives them their relatively easy interpretations. However, unlike with ordinary linear regression, there is no simple closed-form solution to the above equation. Instead, a solution is estimated using something like iteratively reweighted least squares or Newton’s method. As such, we won’t be able to provide exact calculations of the GLM solutions like we did above with the ordinary linear models, so we’ll stick to first providing the interpretation and then checking to make sure the fitted model matches that interpretation."
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#the-intercept-only-model-1",
    "href": "derivations/Intercept_Interpretation.html#the-intercept-only-model-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "The Intercept-only Model",
    "text": "The Intercept-only Model\nIn the intercept-only model, the GLM formula becomes:\n\\[\n\\begin{aligned}\ng(\\mathbb{E}[\\mathbf{y}]) &= \\beta_0 \\\\\n\\mathbb{E}[\\mathbf{y}] &= g^{-1}(\\beta_0) \\\\\n\\end{aligned}\n\\]\nIf we’re using \\(g(\\cdot) = \\text{log}(\\cdot)\\) (the log-link), then the inverse of \\(g(\\cdot)\\) is \\(g^{-1}(\\cdot) = \\text{exp}(\\cdot)\\). In this case, it’s easy to see that the intercept \\(\\beta_0\\) is actually the natural log of the mean of the response variable \\(\\mathbf{y}\\).\nWe can verify this in R. In this example we’ll use a Poisson GLM with a log link function, as it’s probably the simplest to understand. We’ll start by simulating a Poisson-distributed response \\(\\mathbf{y} \\sim \\text{Poisson}(5)\\) with \\(n = 10\\).\n\n\nCode\ny &lt;- rpois(10, lambda = 5)\ny &lt;- matrix(y, ncol = 1)\ny\n\n\n      [,1]\n [1,]    4\n [2,]   10\n [3,]    4\n [4,]    6\n [5,]    8\n [6,]    4\n [7,]    3\n [8,]    3\n [9,]    5\n[10,]    6\n\n\nThe mean of \\(\\mathbf{y}\\) is 5.3, and the log of that quantity is:\n\n\nCode\nlog(mean(y))\n\n\n[1] 1.667707\n\n\nWe can fit a Poisson GLM like so. We can clearly see that \\(\\beta_0 = \\text{log}(\\bar{\\mathbf{y}})\\).\n\n\nCode\nnull_mod_pois &lt;- glm(y ~ 1, family = poisson(link = \"log\"))\ncoef(null_mod_pois)\n\n\n(Intercept) \n   1.667707 \n\n\nThus, when we use the inverse of the link function (exponentiation) on the estimated value of \\(\\beta_0\\), we get the value of \\(\\mathbb{E}[\\mathbf{y}] = 5.3\\).\n\n\nCode\nexp(coef(null_mod_pois))\n\n\n(Intercept) \n        5.3"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-categorical-predictors-1",
    "href": "derivations/Intercept_Interpretation.html#models-with-categorical-predictors-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Categorical Predictors",
    "text": "Models with Categorical Predictors\nThe extension to categorical predictors is much the same as we saw before with ordinary linear models, with the primary change being that we’re now working on the log scale. Let’s add a categorical predictor to our model. First we simulate a categorical \\(\\mathbf{X}\\):\n\n\nCode\nX &lt;- sample(c(\"A\", \"B\"), size = 10, replace = TRUE)\nX &lt;- matrix(X, ncol = 1)\nX\n\n\n      [,1]\n [1,] \"A\" \n [2,] \"B\" \n [3,] \"A\" \n [4,] \"A\" \n [5,] \"B\" \n [6,] \"A\" \n [7,] \"A\" \n [8,] \"B\" \n [9,] \"B\" \n[10,] \"B\" \n\n\nAgain using dplyr, we can find \\(\\bar{\\mathbf{y}}\\) and \\(\\text{log}(\\bar{\\mathbf{y}})\\) for each group in our categorical variable:\n\n\nCode\ndata.frame(y = y, \n           X = X[, 1]) %&gt;% \n  with_groups(X, \n              summarise, \n              mu = mean(y), \n              log_mu = log(mean(y)))\n\n\n\n\n\n\nX\nmu\nlog_mu\n\n\n\n\nA\n4.2\n1.435085\n\n\nB\n6.4\n1.856298\n\n\n\n\n\n\nWe fit a Poisson GLM using the glm() function, again using the natural log as our link function. We see that the intercept is equal to the log of the mean of our response variable for group A.\n\n\nCode\ncat_mod_pois &lt;- glm(y ~ X, family = poisson(link = \"log\"))\ncoef(cat_mod_pois)\n\n\n(Intercept)          XB \n  1.4350845   0.4212135 \n\n\nNext, if we sum the coefficients, we get the log of the response variable for group B:\n\n\nCode\nsum(coef(cat_mod_pois))\n\n\n[1] 1.856298\n\n\nIf we exponentiate the coefficients the intercept becomes simply the mean of \\(\\mathbf{y}\\) for group A.\n\n\nCode\nexp(coef(cat_mod_pois))\n\n\n(Intercept)          XB \n    4.20000     1.52381 \n\n\nLastly, if we exponentiate the sum of the coefficients (the order of these operations is important), we get the mean of \\(\\mathbf{y}\\) for group B:\n\n\nCode\nexp(sum(coef(cat_mod_pois)))\n\n\n[1] 6.4"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-continuous-predictors-1",
    "href": "derivations/Intercept_Interpretation.html#models-with-continuous-predictors-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Continuous Predictors",
    "text": "Models with Continuous Predictors\nAs with ordinary linear models, in the presence of a continuous predictor the intercept becomes the expected value of the response variable conditional on the line of best fit that we generate. First we generate another \\(\\mathbf{y}\\) with a larger sample size and a slightly lower value of \\(\\lambda\\). Then we generate Gamma random noise, and define the predictor \\(\\mathbf{X} = 3.1y + \\epsilon\\).\n\n\nCode\ny &lt;- rpois(300, lambda = 3)\ny &lt;- matrix(y, ncol = 1)\nX &lt;- 3.1 * y + rgamma(300, shape = 20, rate = 4)\nX &lt;- matrix(X, ncol = 1)\n\n\nWe can plot \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to get an idea of what their relationship is. Since \\(\\mathbf{X}\\) is a Gamma random variable, it doesn’t actually have any zero values; the minimum value we’ve generated is actually 3.8055922. As such, the intercept will be an extrapolation of the data that we do have.\n\n\nCode\ndata.frame(y = y, \n           X = X[, 1]) %&gt;% \n  ggplot(aes(x = X, y = y)) + \n  geom_point(alpha = 0.8) + \n  labs(x = \"X\", y = \"Y\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\n\n\nWe fit another Poisson GLM and check the coefficients. The intercept in this case is interpreted as the log of the expected value of the response when our predictor variable is equal to zero. However - since \\(\\mathbf{X}\\) is essentially a Gamma random variable, it can actually never take a value of zero, as its support is \\((0, \\infty)\\). As such, the interpretation of \\(\\beta_0\\) is somewhat useless here, and thus it makes sense to center \\(\\mathbf{X}\\) as we did earlier with the ordinary linear model.\n\n\nCode\ncont_mod_pois &lt;- glm(y ~ X, family = poisson(link = \"log\"))\ncoef(cont_mod_pois)\n\n\n(Intercept)           X \n-0.37029176  0.09311951 \n\n\nAfter centering and refitting the Poisson GLM, the intercept takes on a new interpretation - the log of \\(\\mathbf{y}\\) when \\(\\mathbf{X}\\) is at its mean. Note that the coefficient for \\(\\mathbf{X}\\) does not change, as centering does not change the units of \\(\\mathbf{X}\\).\n\n\nCode\nX_cent &lt;- scale(X, scale = FALSE)\ncont_mod_pois_centered &lt;- glm(y ~ X_cent, family = poisson(link = \"log\"))\ncoef(cont_mod_pois_centered)\n\n\n(Intercept)      X_cent \n 0.95596361  0.09311951 \n\n\nLet’s plot both the raw data and the fitted values obtained from our model. The blue horizontal line shows \\(\\text{exp}(\\beta_0)\\) i.e., the expected value of \\(\\mathbf{y}\\) when \\(\\mathbf{X}\\) is at its mean value (again, conditional on the line of best fit that we obtained). The vertical yellow line shows where the mean of \\(\\mathbf{X}\\) is; since we centered \\(\\mathbf{X}\\) this is equal to zero. Lastly, the green line shows the predicted values from our model, and this line intersects nicely with the value of the intercept as we would expect.\n\n\nCode\nintercept_exp &lt;- exp(coef(cont_mod_pois_centered)[1])\ndata.frame(y = y, \n           y_pred = fitted(cont_mod_pois_centered), \n           X = X_cent[, 1]) %&gt;% \n  ggplot(aes(x = X, y = y)) + \n  geom_point(alpha = 0.8) + \n  geom_hline(yintercept = intercept_exp, color = \"dodgerblue\", size = 1) + \n  geom_vline(xintercept = 0, color = \"goldenrod\", size = 1) + \n  geom_line(aes(y = y_pred), color = \"forestgreen\", size = 1) + \n  labs(x = \"X (centered)\", y = \"Y\") + \n  theme_classic()"
  },
  {
    "objectID": "derivations/Intercept_Interpretation.html#models-with-categorical-continuous-predictors-1",
    "href": "derivations/Intercept_Interpretation.html#models-with-categorical-continuous-predictors-1",
    "title": "Understanding Intercepts in Linear Regression Models",
    "section": "Models with Categorical & Continuous Predictors",
    "text": "Models with Categorical & Continuous Predictors\nLastly, we’ll again examine the most common real-life situation - models with both categorical and continuous predictors. Consider a dataset where the response is a Poisson-distributed random variable, say the number of copies of viral RNA in a sample taken from a patient. First, let’s say that our sample size is set to a generous \\(n = 1000\\). Next, imagine that we have two continuous predictors; the first being patient age in years, and the second being the number of months that the patient has been in treatment. We’ll simulate age as a Poisson random variable with a mean of 25 years, and months in treatment as a negative binomial random variable with a mean of 3 months and overdispersion parameter (denoted size in the rnbinom() function) of 4. Lastly, let’s define a categorical predictor with 3 possible treatment categories. We store all 3 predictors in a data.frame.\n\n\nCode\nX_age &lt;- rpois(1000, 25)\nX_months &lt;- rnbinom(1000, size = 4, mu = 3)\nX_treat &lt;- sample(c(\"Drug_A\", \"Drug_B\", \"Drug_C\"), \n                  size = 1000, \n                  replace = TRUE)\nmod_df &lt;- data.frame(age = X_age, \n                     months = X_months, \n                     treat = X_treat)\nslice_head(mod_df, n = 5)\n\n\n\n\n\n\nage\nmonths\ntreat\n\n\n\n\n17\n2\nDrug_B\n\n\n24\n0\nDrug_C\n\n\n24\n2\nDrug_A\n\n\n18\n1\nDrug_C\n\n\n36\n4\nDrug_C\n\n\n\n\n\n\nFinally, we’ll define \\(\\mathbf{y}\\) to be a function of all 3 predictor variables along with a large amount of Poisson-distributed random noise following the distribution \\(\\epsilon \\sim \\text{Poisson}(50)\\). We use the handy dplyr::case_when() function to create \\(\\mathbf{y}\\) as a piecewise function whose relationship to the predictor variable changes based on the treatment each patient was given. After generating \\(\\mathbf{y}\\), we round it to ensure that it is integer-valued, since we’re focused here on using Poisson GLMs.\n\n\nCode\nepsilon &lt;- rpois(1000, 50)\nmod_df &lt;- mod_df %&gt;% \n          mutate(y = case_when(treat == \"Drug_A\" ~ 2.25 * age - 1.2 * months, \n                               treat == \"Drug_B\" ~ 2 * age - 3 * months, \n                               treat == \"Drug_C\" ~ 1.75 * age - 5 * months), \n                 y = round(y + epsilon))\nslice_head(mod_df, n = 5)\n\n\n\n\n\n\nage\nmonths\ntreat\ny\n\n\n\n\n17\n2\nDrug_B\n79\n\n\n24\n0\nDrug_C\n86\n\n\n24\n2\nDrug_A\n104\n\n\n18\n1\nDrug_C\n84\n\n\n36\n4\nDrug_C\n86\n\n\n\n\n\n\nWe know that for our two continuous predictors, the intercept will indicate the predicted value of \\(\\mathbf{y}\\) for patients with values of zero for those two variables. Since we also have a categorical variable, the intercept will refer to the reference group of that variable - in this case patients who were assigned Drug_A. Lastly, since we’re using a Poisson GLM, we know that the intercept will be the natural log of that quantity. Our model will be of the following form:\n\\[\n\\text{log}(\\mathbb{E}[\\text{Viral RNA}]) = \\beta_0 + \\beta_{\\text{age}} X_{\\text{age}} + \\beta_{\\text{months}} X_{\\text{months}} + \\beta_{\\text{treat}} X_{\\text{treat}} + \\epsilon\n\\]\nLet’s visualize the data so that we can get some idea of what the relationships between our variables are, and what the intercept of the model we’re going to fit will tell us. First let’s look at the relationship between age and the response, splitting by treatment group. We add a purple vertical line showing where the overall mean of age is.\n\n\nCode\nggplot(mod_df, aes(x = age, y = y, color = treat)) + \n  geom_vline(aes(xintercept = mean(age)), color = \"purple\", size = 1) + \n  geom_point(alpha = 0.8) + \n  labs(x = \"Age\", y = \"Viral RNA\", color = \"Treatment\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\n\n\nWe repeat the visualization for months.\n\n\nCode\nggplot(mod_df, aes(x = months, y = y, color = treat)) + \n  geom_vline(aes(xintercept = mean(months)), color = \"purple\", size = 1) + \n  geom_point(alpha = 0.8) + \n  labs(x = \"Months Treated\", y = \"Viral RNA\", color = \"Treatment\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\n\n\nLastly, let’s simply compare the distribution of the response between the three treatment groups using a beeswarm plot, which is a variation on the classic violin plot that I’ve preferred recently. Check out the ggbeeswarm package docs for more. We also add a horizontal line showing \\(\\bar{\\mathbf{y}}\\) for each group.\n\n\nCode\nggplot(mod_df, aes(x = treat, y = y, color = treat)) + \n  ggbeeswarm::geom_quasirandom() + \n  stat_summary(fun = \"mean\",\n               geom = \"crossbar\", \n               width = 0.5,\n               size = 0.75, \n               color = \"black\") + \n  labs(x = \"Treatment\", y = \"Viral RNA\", color = \"Treatment\") + \n  theme_classic(base_size = 14)\n\n\n\n\n\n\n\n\n\nLet’s fit a model. As previously noted, we use the log link function. Since our treatment variable has three categories, we now have two coefficients for each of the non-reference group levels. The intercept \\(\\beta_0\\) gives us the log of the expected value of the response for a patient that is 0 years old, has spent 0 months in treatment, and is assigned to be treated with Drug_A. We can tell right away that this isn’t the most useful interpretation, as there are no patients in our study who are zero years old (and even if there were, it probably wouldn’t be that useful to know that quantity).\n\n\nCode\nfull_mod_pois &lt;- glm(y ~ age + months + treat, \n                     data = mod_df, \n                     family = poisson(link = \"log\"))\ncoef(full_mod_pois)\n\n\n(Intercept)         age      months treatDrug_B treatDrug_C \n 4.18370839  0.02170154 -0.03522072 -0.12013892 -0.25924502 \n\n\nSince we’re using the log-link, exponentiating the intercept gives us the expected value of \\(\\mathbf{y}\\) under the conditions we described just above.\n\n\nCode\nexp(coef(full_mod_pois))[1]\n\n\n(Intercept) \n   65.60871 \n\n\nClearly, the age variable is a prime target for centering. While it could be useful to center months (depending on the goals of the study), there are a total of 113 patients in the study who have spent zero months in treatment, so it’s at least somewhat anchored in reality. We center age, then refit the model. We see that the estimate of \\(\\beta_0\\) changes, while none of the coefficients for our predictors do (as expected).\n\n\nCode\nmod_df &lt;- mutate(mod_df, \n                 age_cent = scale(age, scale = FALSE))\nfull_mod_pois_centered &lt;- glm(y ~ age_cent + months + treat, \n                              data = mod_df, \n                              family = poisson(link = \"log\"))\ncoef(full_mod_pois_centered)\n\n\n(Intercept)    age_cent      months treatDrug_B treatDrug_C \n 4.72589977  0.02170154 -0.03522072 -0.12013892 -0.25924502 \n\n\nExponentiating \\(\\beta_0\\) now gives us the expected value of \\(\\mathbf{y}\\) for a patient of average age (which is 24.984 years) who was assigned treatment Drug_A and has spent zero months being treated. This quantity is useful because it tells us the baseline pre-treatment value for the average patient who takes Drug_A.\n\n\nCode\nexp(coef(full_mod_pois_centered))[1]\n\n\n(Intercept) \n    112.832"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a second-year PhD student at the University of Florida under Dr. Rhonda Bacher. My primary research interest are genomics and genetics, and particularly single cell RNA-seq computational method development. I mostly develop tools related to trajectory inference, clustering, and differential expression analysis. In addition, I also focus on scientific reproducibility, computational scalability, and effective data visualization."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Florida | PhD Biostatistics\nAug 2022 - Current\nUniversity of Florida | MS Biostatistics\nAug 2020 - May 2022\nUniversity of North Carolina | BS Statistics\nAug 2016 - May 2020"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience\nUniversity of Florida | Graduate Research & Teaching Assistant\nMay 2021 - present\nUniversity of North Carolina | Research Collaborator\nAugust 2020 - present\nBlue Cross Blue Shield of Florida | Data Analyst\nJuly 2020 - July 2022\nUniversity of North Carolina | Undergraduate Research Assistant\nJune 2019 - July 2020"
  },
  {
    "objectID": "knowledge_base/scRNAseq.html",
    "href": "knowledge_base/scRNAseq.html",
    "title": "scRNA-seq Resources",
    "section": "",
    "text": "Here I’ll catalog useful papers, preprints, method vignettes, Twitter discussions, etc. that I’ve found helpful while learning how to process and analyze single cell RNA-seq data. When possible, I’ll link to static versions of things in the hopes that links don’t break. I’ll categorize resources according to which problem they address e.g., raw data processing, clustering, visualization, etc., though some resources will of course touch on multiple topics."
  },
  {
    "objectID": "knowledge_base/scRNAseq.html#via-pseudotime",
    "href": "knowledge_base/scRNAseq.html#via-pseudotime",
    "title": "scRNA-seq Resources",
    "section": "Via Pseudotime",
    "text": "Via Pseudotime\n\nSlingshot: cell lineage and pseudotime inference for single-cell transcriptomics\n\nThe slingshot package is my currently preferred method for estimating a pseudotemporal cellular ordering. A decent vignette can be found here. I would absolutely recommend using principal components as input to the algorithm instead of UMAP / t-SNE components.\n\nTrajectory-based differential expression analysis for single-cell sequencing data\n\nThis paper describes the development of the tradeSeq R package, which uses generalized additive models (GAMs) to perform differential expression over an inferred cellular trajectory. The package has some limitations, but provides a variety of different tests of different patterns of gene expression, and overall strikes a good balance between running quickly & providing accurate results. A good vignette can be accessed here. A nice characteristic of the method is that it is agnostic with respect to the type of pseudotime estimation used, meaning the user can derive their cellular ordering using any pseudotime or RNA velocity method prior to running tradeSeq."
  },
  {
    "objectID": "knowledge_base/scRNAseq.html#via-rna-velocity",
    "href": "knowledge_base/scRNAseq.html#via-rna-velocity",
    "title": "scRNA-seq Resources",
    "section": "Via RNA Velocity",
    "text": "Via RNA Velocity\n\nRNA velocity unraveled\nOn the mathematics of RNA velocity I: Theoretical analysis\nOn the mathematics of RNA velocity II: Algorithmic aspects\nUnified fate mapping in multiview single-cell data\n\nThis preprint presents the CellRank 2 Python package, which implements a flexible, modular set of tools for the analysis of cell state composition and transition. The package is an extension of CellRank (paper), and is built around the concept of kernels based on pseudotime, RNA velocity, graph connectivity, time-series, etc. that are used to create estimators that classify cells into initial, intermediate, and terminal cell states. While RNA velocity is a possible input, it’s no longer necessary to run the estimation routine like it was in the original CellRank package. The great thing about this package is that you can combine kernels i.e., you can base the cell state estimation on weighted combinations of pseudotime, RNA velocity, graph structure, etc. based on how confident you are in each input."
  },
  {
    "objectID": "tutorials/CellRank_Tutorial.html",
    "href": "tutorials/CellRank_Tutorial.html",
    "title": "Analyzing Transcriptional Dynamics with CellRank",
    "section": "",
    "text": "In this analysis we’ll use CellRank to identify multiple cell fates in pancreatic endocrinogenesis data by combining RNA velocity, pseudotime, & developmental potential information. The goal is to showcase a more advanced analysis that incorporates multiple modalities to infer a confident final trajectory."
  },
  {
    "objectID": "tutorials/CellRank_Tutorial.html#preprocessing",
    "href": "tutorials/CellRank_Tutorial.html#preprocessing",
    "title": "Analyzing Transcriptional Dynamics with CellRank",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nHVG Selection & Normalization\nWe begin by filtering out cells that have fewer than 20 total counts between the spliced & unspliced assays. Next, we select the top 3,000 most highly variable genes (HVGs), after which we normalize the counts using a log1p-transform. Lastly, we use the cell cycle gene sets from Tirosh et al (2016) to assign S-phase and G2M-phase scores to each cell in order to identify populations of cycling cells. Cell cycle variation is not always of biological interest, and can prevent trajectory structures from being as clean or simple as we’d like.\n\n\nCode\nscv.pp.filter_and_normalize(\n    ad_panc, \n    min_shared_counts=20, \n    n_top_genes=3000,\n    flavor='seurat',  \n    subset_highly_variable=False, \n    log=False\n)\nsc.pp.log1p(ad_panc)\nscv.tl.score_genes_cell_cycle(ad_panc)\n\n\nFiltered out 20801 genes that are detected 20 counts (shared).\nNormalized count data: X, spliced, unspliced.\nExtracted 3000 highly variable genes.\ncalculating cell cycle phase\n--&gt;     'S_score' and 'G2M_score', scores of cell cycle phases (adata.obs)\n\n\n\n\nPCA Embedding\nAfter centering & scaling the normalized spliced mRNA counts matrix, we use the HVGs identified earlier to generate a 30-dimensional principal component analysis (PCA) embedding.\n\n\nCode\nsc.pp.scale(ad_panc)\nsc.tl.pca(\n    ad_panc, \n    n_comps=30, \n    random_state=312, \n    use_highly_variable=True\n)\n\n\nVisualizing the PCA embedding shows us a rather simple trajectory from endocrine precursors to mature endocrine cells.\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='pca', \n    color='clusters',\n    title='', \n    frameon=True, \n    size=20, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('PC 1')\nplt.gca().set_ylabel('PC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: PCA embedding\n\n\n\n\n\n\n\nGraph-based Clustering\nAfter identifying the 20 nearest neighbors (NNs) for each cell in PCA space, we use the Leiden algorithm to partition the NN graph into discrete clusters.\n\n\nCode\nsc.pp.neighbors(\n    ad_panc, \n    use_rep='X_pca', \n    n_pcs=30, \n    n_neighbors=20, \n    metric='cosine', \n    random_state=312\n)\nsc.tl.leiden(\n    ad_panc, \n    resolution=0.3, \n    random_state=312\n)\n\n\nThe clusters generally correspond to our celltypes, with some minor differences e.g., overclustering in the ductal cell population (most likely due to cell cycle variation).\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='pca', \n    color='leiden',\n    title='', \n    frameon=True, \n    size=20, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('PC 1')\nplt.gca().set_ylabel('PC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Leiden clusters in PCA space\n\n\n\n\n\nPlotting the cell cycle scores on the PCA embedding shows us that the two ductal cell clusters are in fact split by enrichment for cell cycle genes.\n\n\nCode\nscv.pl.scatter(\n  ad_panc, \n  basis='pca', \n  color_gradients=['S_score', 'G2M_score'], \n  smooth=True,\n  perc=[5, 95], \n  frameon=True, \n  show=False\n)\nplt.gca().set_xlabel('PC 1')\nplt.gca().set_ylabel('PC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Cell cycle scores\n\n\n\n\n\n\n\nUMAP Embedding\nWe generate a UMAP embedding in two dimensions.\n\n\nCode\nsc.tl.umap(ad_panc, random_state=312)\n\n\nUpon visual inspection, the UMAP embedding appears to preserve both the transitions between celltypes and the heterogeneity of the mature endocrine cells better than the PCA embedding.\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='umap', \n    color='clusters',\n    title='', \n    frameon=True, \n    size=20, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: UMAP embedding\n\n\n\n\n\n\n\nDiffusion Map Embedding\nWe’ll also generate a 2D diffusion map embedding, since that algorithm often works well for trajectory manifolds.\n\n\nCode\nsc.tl.diffmap(\n  ad_panc, \n  n_comps=10, \n  random_state=312\n)\n\n\nWe visualize the 2nd and 3rd components - for some reason there’s a bug in the scanpy source code that incorrectly arranges the returned array for the embedding. While the overall trajectory structure is preserved, the local structure of the mature endocrine celltypes is messy, making this embedding less than ideal for our purposes.\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='diffmap', \n    color='clusters',\n    title='', \n    frameon=True, \n    size=20, \n    alpha=0.75, \n    show=False, \n    components='2, 3'\n)\nplt.gca().set_xlabel('DC 1')\nplt.gca().set_ylabel('DC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Diffusion map embedding\n\n\n\n\n\nDespite the imperfection of the diffusion map embedding, we’ll also estimate a diffusion pseudotime value for each cell, as the estimates might be useful later on. We set the root cell to be the ductal cell closest to the minimum of the second diffusion component (as visualized above).\n\n\nCode\nad_panc.uns['iroot'] = np.argmin(ad_panc.obsm['X_diffmap'][:2])\nsc.tl.dpt(ad_panc, n_dcs=10)\n\n\nVisualizing the diffusion pseudotime on our UMAP embedding shows that it seems to recapitulate the underlying biological process fairly well, with ductal cells assigned lower values and endocrine cells assigned higher values.\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='umap', \n    color='dpt_pseudotime',\n    title='', \n    frameon=True, \n    size=20, \n    alpha=0.75, \n    show=False, \n    cmap='plasma'\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Estimated diffusion pseudotime\n\n\n\n\n\n\n\nForce-directed Graph Embedding\nIt’s often worth checking a force-directed graph embedding in addition to the UMAP embedding, as it can sometimes better preserve trajectory structure in the data.\n\n\nCode\nsc.tl.draw_graph(\n    ad_panc, \n    layout='kk',\n    random_state=312,\n    n_jobs=4\n)\n\n\nWhile the overall trajectory structure is better (visually speaking) than the PCA embedding, variation in the ductal & endocrine progenitor celltypes is hidden. We’ll stick with the UMAP embedding going forwards.\n\n\nCode\nsc.pl.draw_graph(\n    ad_panc, \n    color='clusters', \n    title='', \n    alpha=0.75, \n    size=20, \n    show=False\n)\nplt.gca().set_xlabel('Dim 1')\nplt.gca().set_ylabel('Dim 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Force-directed graph embedding"
  },
  {
    "objectID": "tutorials/CellRank_Tutorial.html#rna-velocity-estimation",
    "href": "tutorials/CellRank_Tutorial.html#rna-velocity-estimation",
    "title": "Analyzing Transcriptional Dynamics with CellRank",
    "section": "RNA Velocity Estimation",
    "text": "RNA Velocity Estimation\nNext, we’ll estimate RNA velocity vectors for each cell via scvelo. Details on the methodology can be found in the original paper, and a review of current challenges & limitations of velocity analyses can be read here.\n\nSNN-based Imputation\nWe start by smoothing the spliced & unspliced counts across the NNs we identified earlier. This helps to preserve signal in otherwise very noisy velocity vectors.\n\n\nCode\nscv.pp.moments(\n    ad_panc, \n    n_pcs=None, \n    n_neighbors=20, \n    use_rep='X_pca'\n)\n\n\ncomputing moments based on connectivities\n    finished (0:00:01) --&gt; added \n    'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\n\n\n\n\nDynamical Velocity Computation\nNext, we utilize the dynamical model to estimate velocity vectors, generate a velocity graph, and compute the uncertainty of each cells’ differentiation direction. Lastly, we project the velocity vectors onto our 2D UMAP embedding for visualization purposes.\n\n\nCode\nscv.tl.recover_dynamics(ad_panc, n_jobs=6)\nscv.tl.velocity(\n    ad_panc, \n    mode='dynamical', \n    use_highly_variable=True\n)\nscv.tl.velocity_graph(\n    ad_panc, \n    n_jobs=4, \n    compute_uncertainties=True\n)\nscv.tl.velocity_confidence(ad_panc)\nscv.tl.velocity_embedding(ad_panc, basis='umap')\n\n\nrecovering dynamics (using 6/8 cores)\n  0%|          | 0/1430 [00:00&lt;?, ?gene/s]\n    finished (0:02:57) --&gt; added \n    'fit_pars', fitted parameters for splicing dynamics (adata.var)\ncomputing velocities\n    finished (0:00:05) --&gt; added \n    'velocity', velocity vectors for each individual cell (adata.layers)\ncomputing velocity graph (using 4/8 cores)\n  0%|          | 0/3696 [00:00&lt;?, ?cells/s]\n    finished (0:00:20) --&gt; added \n    'velocity_graph', sparse matrix with cosine correlations (adata.uns)\n--&gt; added 'velocity_length' (adata.obs)\n--&gt; added 'velocity_confidence' (adata.obs)\n--&gt; added 'velocity_confidence_transition' (adata.obs)\ncomputing velocity embedding\n    finished (0:00:00) --&gt; added\n    'velocity_umap', embedded velocity vectors (adata.obsm)\n\n\nVisualizing the projected velocities with a streamline plot allows us to get a rough idea of which direction cells are differentiating towards in each local area. From the endocrine progenitors through pre-endocrine cells we see a smooth flow of differentiation, which is followed by branching trajectories towards each of the mature endocrine celltypes. The streamlines broadly correspond to known biology, which indicates that RNA velocity should provide decent biological insights about this dataset. We also see cycling in the ductal cells, which is expected as this dataset is known to have strong cell-cycle effects in the ductal cell cluster.\n\n\nCode\nscv.pl.velocity_embedding_stream(\n    ad_panc, \n    basis='umap', \n    color='clusters',\n    alpha=0.75, \n    size=20, \n    frameon=True, \n    linewidth=0.5, \n    legend_loc='right',\n    arrow_size=0.8,\n    title='', \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: Velocity embedding streamlines\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nStreamline embedding plots generally provide qualitative insight at best, & their interpretation can be very subjective. Take care when forming biological conclusions or further hypotheses based on them, and verify what you see in a quantitative manner.\n\n\nVisualizing the length of the velocity vectors in each cell gives us an idea of how quickly differentiation is occurring, as velocity vector length is a proxy measurement for differentiation speed. We see high scores in the pre-endocrine cells, as well as in the beta cells. This could be due to the phenomenon of self-duplication that is observed in beta cells, wherein new beta cells are produced in that way instead of through precursor differentiation. More information on beta cell specification can be read in Murtaugh (2007).\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='umap', \n    color='velocity_length',\n    title='', \n    frameon=True, \n    size=20, \n    alpha=0.75, \n    show=False, \n    legend_loc='right margin', \n    cmap='coolwarm'\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Velocity length\n\n\n\n\n\n\n\nInvestigation of Velocity Genes\nWe begin by identifying ranking the velocity genes, then pull the top 6 highest-likelihood genes for each celltype.\n\n\nCode\nscv.tl.rank_dynamical_genes(ad_panc, groupby='clusters')\n\n\nCode\ntop_dyn_genes = scv.get_df(ad_panc, 'rank_dynamical_genes/names').head(6)\n\n\nPlotting the spliced vs. unspliced mRNA estimates along with the learned dynamics per-gene allows us to determine which genes have dynamics specific to mature endocrine celltypes. For example, Pak3 is present in the top 6 genes of every endocrine celltype. Interestingly, this gene is mostly known to be associated in humans with intellectual development disorders, though it’s also present in GO biological processes such as differentiation and system development. Lastly, according to Piccand et al (2013), Pak3 is linked to beta cell specification and has implications for diabetes risk, but their study does not conclude that it is necessary for the development of other endocrine celltypes.\nAnother interesting tidbit is the presence of Meis2 as a top gene for both delta & epsilon cells. This gene is associated with organogenesis and autism, and is known to be a vital part of neural crest development as per Machon et al (2015). It’s role in some endocrine celltypes’ development but not others could indicate that the specification of delta & epsilon cells is more similar than that of alpha and beta cells.\n\nAlpha Cells\n\n\nCode\nscv.pl.scatter(\n  ad_panc, \n  top_dyn_genes['Alpha'], \n  ncols=3, \n  frameon=False, \n  alpha=0.75, \n  size=20, \n  show=False\n)\nplt.gcf().supxlabel('Spliced mRNA')\nplt.gcf().supylabel('Unspliced mRNA')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Alpha cell velocity genes\n\n\n\n\n\n\n\nBeta Cells\n\n\nCode\nscv.pl.scatter(\n  ad_panc, \n  top_dyn_genes['Beta'], \n  ncols=3, \n  frameon=False, \n  alpha=0.75, \n  size=20, \n  show=False\n)\nplt.gcf().supxlabel('Spliced mRNA')\nplt.gcf().supylabel('Unspliced mRNA')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11: Beta cell velocity genes\n\n\n\n\n\n\n\nDelta Cells\n\n\nCode\nscv.pl.scatter(\n  ad_panc, \n  top_dyn_genes['Delta'], \n  ncols=3, \n  frameon=False, \n  alpha=0.75, \n  size=20, \n  show=False\n)\nplt.gcf().supxlabel('Spliced mRNA')\nplt.gcf().supylabel('Unspliced mRNA')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 12: Delta cell velocity genes\n\n\n\n\n\n\n\nEpsilon Cells\n\n\nCode\nscv.pl.scatter(\n  ad_panc, \n  top_dyn_genes['Epsilon'], \n  ncols=3, \n  frameon=False, \n  alpha=0.75, \n  size=20, \n  show=False\n)\nplt.gcf().supxlabel('Spliced mRNA')\nplt.gcf().supylabel('Unspliced mRNA')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 13: Epsilon cell velocity genes"
  },
  {
    "objectID": "tutorials/CellRank_Tutorial.html#cell-fate-identification",
    "href": "tutorials/CellRank_Tutorial.html#cell-fate-identification",
    "title": "Analyzing Transcriptional Dynamics with CellRank",
    "section": "Cell Fate Identification",
    "text": "Cell Fate Identification\nIn this part of the analysis we’ll focus on identifying a set of terminal and initial cell states, along with the probabilities of transitions between them. This is performed via CellRank; the details of the method can be read in the original paper as well as the preprint for the second version.\n\nVelocity Kernel\nWe start by computing a cell-cell transition probability matrix based on our RNA velocity estimates from earlier. Using the model='monte_carlo' option allows us to propagate the velocity vector uncertainty estimates forward to the estimated transition probability matrix.\n\n\nCode\nvk = cr.kernels.VelocityKernel(ad_panc)\nvk.compute_transition_matrix(\n    model='monte_carlo', \n    similarity='cosine', \n    seed=312, \n    n_jobs=4\n)\n\n\nVisualizing the directed transitions on our UMAP embedding shows essentially the same patterns as our velocity vector projections from above, as expected. A drawback of using velocity on this dataset is that the inferred directionality in the ductal cell population is uncertain, so we’ll need another source of information to strengthen our inferred trajectory.\n\n\nCode\nvk.plot_projection(\n    basis='umap', \n    recompute=True, \n    linewidth=0.5, \n    arrow_size=0.8,\n    color='clusters', \n    size=20, \n    alpha=0.75, \n    legend_loc='right margin', \n    title='', \n    frameon=True, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14: Velocity kernel projection\n\n\n\n\n\nVisualizing the velocity confidence for each cell confirms our intuition that the ductal cells aren’t fully interpretable using velocity-based information, as scores are slightly lower for the ductal cells than for e.g., the endocrine precursors or mature endocrine cells.\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='umap', \n    color='velocity_confidence',\n    title='', \n    frameon=True, \n    size=20, \n    alpha=0.75, \n    show=False, \n    legend_loc='right margin', \n    cmap='coolwarm'\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 15: UMAP colored by velocity coherence\n\n\n\n\n\nPlotting the coherence of the velocity estimates per-celltype shows us that the alpha cells also have a relatively low velocity confidence. This could be because they’re located at the end of the trajectory / they’re terminally differentiated, or it could mean that the cells are of lower quality than those in other clusters.\n\n\nCode\nsc.pl.violin(\n  ad_panc, \n  keys='velocity_confidence', \n  groupby='clusters', \n  rotation=30, \n  xlabel='', \n  ylabel='Velocity Confidence',\n  show=False\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 16: Velocity coherence by celltype\n\n\n\n\n\n\n\nCytoTRACE Kernel\nIn addition to the velocity kernel we’ll compute another transition probability matrix based on CytoTRACE scores. The CytoTRACE method essentially uses the number of expressed genes as a proxy for differentiation potential. The details of the method can be found in Gulati et al (2020).\n\n\nCode\nctk = cr.kernels.CytoTRACEKernel(ad_panc).compute_cytotrace()\nctk.compute_transition_matrix(\n    threshold_scheme='soft',\n    nu=0.5, \n    n_jobs=2\n)\n\n\nVisualizing the CytoTRACE score shows what we would expect - ductal cells, which function as the initial cell state in this dataset, express more genes and have a higher differentiation potential than the intermediate and terminal cell states.\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='umap', \n    color='ct_score',\n    title='', \n    frameon=True, \n    size=20, \n    alpha=0.75, \n    show=False, \n    legend_loc='right margin'\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 17: CytoTRACE differentiation scores\n\n\n\n\n\nThe embedding projection based on our CytoTRACE kernel displays a confident differentiation trajectory for the initial and intermediate cell states, but for the mature cell states with fewer expressed genes the differentiation directions are much less certain.\n\n\nCode\nctk.plot_projection(\n    basis='umap', \n    recompute=True, \n    linewidth=0.5, \n    color='clusters', \n    size=20, \n    alpha=0.75, \n    arrow_size=0.8, \n    legend_loc='right margin', \n    title='', \n    frameon=True, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 18: CytoTRACE kernel projection\n\n\n\n\n\n\n\nPseudotime Kernel\nLastly, we can create another kernel based on our estimated diffusion pseudotime from earlier. This should help to smooth out the overall trajectory, since it represents the transitions between celltypes well.\n\n\nCode\npk = cr.kernels.PseudotimeKernel(ad_panc, time_key='dpt_pseudotime')\npk.compute_transition_matrix(threshold_scheme='soft', n_jobs=2)\n\n\nWe visualize the directed transition matrix on our UMAP embedding, and see that the pseudotime kernel does a bit better of a job than the CytoTRACE kernel at identifying directionality in the mature endocrine cells.\n\n\nCode\npk.plot_projection(\n    basis='umap', \n    recompute=True, \n    linewidth=0.5, \n    color='clusters', \n    size=20, \n    alpha=0.75, \n    arrow_size=0.8, \n    legend_loc='right margin', \n    title='', \n    frameon=True, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 19: Pseudotime kernel projection\n\n\n\n\n\n\n\nCombined Kernel\nAs we just saw, individual sources of information about our data can miss critical aspects, like CytoTRACE not detecting much differentiation during the latter half of the trajectory. One of the best features of CellRank is the ability to combine sources of information by combining the kernels themselves. Here we combine the velocity, CytoTRACE, & pseudotime kernels using an equal weighting scheme. This results in a new transition probability matrix, which we can then project onto our embedding.\n\n\nCode\nck = 0.33 * vk + 0.33 * ctk + 0.33 * pk\n\n\nWith all three source of information (velocity, differentiation potential, & pseudotime) combined, we finally have a relatively smooth trajectory that more or less lines up with known biology.\n\n\nCode\nck.plot_projection(\n    basis='umap', \n    recompute=True, \n    linewidth=0.5, \n    arrow_size=0.8, \n    color='clusters', \n    size=20, \n    alpha=0.75, \n    legend_loc='right margin', \n    title='', \n    frameon=True, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 20: Combined kernel projection\n\n\n\n\n\n\n\nMacrostate Estimation\n\nTerminal States\nAfter computing a Schur decomposition of the transition probability matrix, we make the assumption that each component corresponds to a cell macrostate - a somewhat vague concept that can loosely be interpreted as a cell phenotype.\n\n\nCode\ng = cr.estimators.GPCCA(ck)\ng.compute_schur(n_components=15, method='brandts')\n\n\nCode\ng.compute_macrostates(\n    cluster_key='clusters', \n    n_cells=20, \n    n_states=13, \n    method='brandts'\n)\n\n\nOn our UMAP embedding we plot the 20 cells most likely to be assigned to each macrostate.\n\n\nCode\ng.plot_macrostates(\n    which='all', \n    discrete=True, \n    same_plot=True, \n    legend_loc='right margin', \n    title='', \n    basis='umap',\n    frameon=True,\n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 21: Macrostates identified via Schur decomposition\n\n\n\n\n\nBased on the macrostate estimates & known biology, we designate the four mature endocrine celltypes as terminal states. This step can be performed automatically using a heuristic\n\n\nCode\ng.set_terminal_states(\n    states=['Alpha', 'Beta', 'Delta', 'Epsilon'], \n    n_cells=20, \n    cluster_key='clusters'\n)\n\n\n\n\nCode\ng.plot_macrostates(\n    which='terminal', \n    discrete=False, \n    same_plot=True, \n    legend_loc='right margin', \n    title='', \n    frameon=True,\n    basis='umap',\n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 22: Terminal cell fates\n\n\n\n\n\n\n\nInitial States\nBased on our prior knowledge that the ductal cells are the root state, we set the initial state to be the ductal cell macrostate closest to the beginning of the trajectory.\n\n\nCode\ng.set_initial_states(\n    states=['Ductal_5'], \n    n_cells=20, \n    cluster_key='clusters'\n)\n\n\n\n\nCode\ng.plot_macrostates(\n    which='initial', \n    discrete=True,\n    legend_loc='right margin', \n    title='', \n    frameon=True,\n    basis='umap', \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 23: Initial cell state\n\n\n\n\n\n\n\n\nAbsorption Probability Estimation\nNow that we have our terminal cell fates identified, we compute the probability of and time to absorption in each state.\n\n\nCode\ng.compute_absorption_times(n_jobs=2)\n\n\nWe visualize the transition probabilities and initial & stationary distributions for each coarse macrostate.\n\n\nCode\ng.plot_coarse_T(\n    title='', \n    show_initial_dist=True, \n    show_stationary_dist=True\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 24: Coarse transition probability matrix\n\n\n\n\n\nThe next step is to estimate the absorption probability of each terminal state; these probabilities are denoted fate probabilities by CellRank.\n\n\nCode\ng.compute_fate_probabilities()\n\n\nAs expected, fate probabilities are highest near each terminal state. Beta cells seem to have the highest fate probability among endocrine progenitors, but this is most likely due to the higher frequency of beta cells in the dataset along with their position at the “end” of the trajectory on the embedding.\n\n\nCode\ng.plot_fate_probabilities(\n    same_plot=True, \n    title='', \n    legend_loc='right margin', \n    frameon=True, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 25: Cell fate probabilities\n\n\n\n\n\n\n\nLineage Driver Identification\nWe compute lineage drivers for each cell fate by estimating the correlation between spliced mRNA and fate probability, after which we estimate lineage priming. Lineage priming is the degree to which a cell is “committed” towards its cell fate; details of how this is calculated can be read in Velten et al (2017).\n\n\nCode\nlineage_drivers = g.compute_lineage_drivers(\n    seed=312, \n    cluster_key='clusters', \n    confidence_level=0.95, \n    method='fisher'\n)\nlineage_priming = g.compute_lineage_priming()\n\n\nVisualizing lineage priming on our UMAP embedding shows that fate commitment is strongest in the mature endocrine cells, with values being particularly high among epsilon cells.\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='umap', \n    color='priming_degree_fwd', \n    title='', \n    frameon=True, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 26: Lineage priming\n\n\n\n\n\n\n\nLatent Time Estimation\nBy using the initial & terminal cell state probabilities as priors, we obtain a more accurate estimate of per-cell latent time.\n\n\nCode\nscv.tl.latent_time(\n    ad_panc, \n    end_key='term_states_fwd_probs', \n    root_key='init_states_fwd_probs'\n)\n\n\nThe estimated latent time seems to characterize our trajectory well, which is good news for later analysis of trajectory differential expression (TDE). Overall, the beta cells seem to take the longest to reach terminal differentiation.\n\n\nCode\nsc.pl.embedding(\n    ad_panc, \n    basis='umap', \n    color='latent_time', \n    title='', \n    frameon=True,\n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 27: Gene-shared latent time\n\n\n\n\n\n\n\nDirected Graph Abstraction\nUsing the initial & terminal cell state probabilities and gene-shared latent time as priors, we estimated a directed partitioned graph abstraction (PAGA), which shows us the general direction each celltype differentiates towards.\n\n\nCode\nscv.tl.paga(\n    ad_panc, \n    groups='clusters', \n    end_key='term_states_fwd_probs', \n    root_key='init_states_fwd_probs', \n    use_time_prior='latent_time'\n)\n\n\nVisualizing the PAGA on our UMAP embedding shows a transition from ductal cells to endocrine progenitors, from pre-endocrine cells to the mature endocrine celltypes. The one bit that doesn’t agree with known biology is the transition from alpha to epsilon cells, as epsilon cells should be generated from pre-endocrine cells instead. This situation demonstrates the importance of having at least some a priori biological knowledge with which your analysis can be guided, since even with sophisticated methods scRNA-seq studies are still very difficult to get right.\n\n\nCode\nscv.pl.paga(\n    ad_panc, \n    color='clusters',\n    basis='umap', \n    size=20, \n    alpha=.25,\n    min_edge_width=2, \n    frameon=True, \n    title='', \n    node_size_scale=0.75, \n    arrowsize=8,\n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 28: Directed PAGA"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html",
    "href": "tutorials/Integration_Tutorial.html",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "",
    "text": "The need to integrate data from scRNA-seq samples into one harmonized dataset has increased in recent years as single cell sequencing has gotten cheaper, making it easier to collect data from multiple subjects, timepoints, or conditions. The goal of most integration techniques is to create an embedding that is (relatively) free of batch effects and that does not lead to the cells clustering by subject ID. There are many methods available to perform integration in both R & Python, and evaluating which method is “best” for your dataset can be tricky & subjective. In this tutorial we’ll try out several of the integration methods available in the scanpy package and compare their results. We’ll use a SmartSeq2 dataset containing myeloid cells from the developing human fetal liver that was originally analyzed in Popescu et al (2020). As such, our goal will be to determine which integration method results in the best embedding for downstream trajectory analysis. In developmental biology settings, a “good” scRNA-seq integrated embedding is smoothly connected (i.e., no spaced-out, discrete clusters), and places celltypes in roughly the order expected based on known biology."
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#r",
    "href": "tutorials/Integration_Tutorial.html#r",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "R",
    "text": "R\n\n\nCode\nlibrary(dplyr)       # dataframe tools\nlibrary(Matrix)      # sparse matrices\nlibrary(Seurat)      # scRNA-seq tools\nlibrary(ggplot2)     # plots\nlibrary(reticulate)  # Python interface"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#python",
    "href": "tutorials/Integration_Tutorial.html#python",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Python",
    "text": "Python\n\n\nCode\nimport scvi                          # VAE integration\n\n\n/Users/jack/Desktop/PhD/Research/Python_Envs/personal_site/lib/python3.11/site-packages/scvi/_settings.py:63: UserWarning: Since v1.0.0, scvi-tools no longer uses a random seed by default. Run `scvi.settings.seed = 0` to reproduce results from previous versions.\n  self.seed = seed\n/Users/jack/Desktop/PhD/Research/Python_Envs/personal_site/lib/python3.11/site-packages/scvi/_settings.py:70: UserWarning: Setting `dl_pin_memory_gpu_training` is deprecated in v1.0 and will be removed in v1.1. Please pass in `pin_memory` to the data loaders instead.\n  self.dl_pin_memory_gpu_training = (\n\n\nCode\nimport warnings                      # filter out warnings\nimport numpy as np                   # matrix utilities\nimport scanpy as sc                  # scRNA-seq processing\nimport pandas as pd                  # dataframe tools\nimport anndata as ad                 # scRNA-seq data structures\nimport matplotlib.pyplot as plt      # plot utilities\nfrom scipy.sparse import csr_matrix  # sparse matrices\n\n\n\n\nCode\nwarnings.simplefilter('ignore', category=UserWarning)"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#train-scvi-model",
    "href": "tutorials/Integration_Tutorial.html#train-scvi-model",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Train scVI model",
    "text": "Train scVI model\nWe’ll integrate across the ID specifying which fetus the cells came from. We also make scVI aware of our celltypes, and instruct it to regress out variation associated with the percentage of mitochondrial reads.\n\n\nCode\nscvi.settings.verbosity = 0\nscvi.settings.seed = 312\n\n\nCode\nscvi.settings.num_threads = 4\nscvi.model.SCVI.setup_anndata(\n    ad_blood, \n    layer='counts', \n    batch_key='fetal_ID', \n    labels_key='celltype', \n    continuous_covariate_keys=['percent_MT']\n)\n\n\nWe allow gene dispersion estimates to vary by celltype, and specify expression as following a negative-binomial distribution.\n\n\nCode\nint_model = scvi.model.SCVI(\n    ad_blood, \n    n_layers=3, \n    n_hidden=96, \n    n_latent=20, \n    gene_likelihood='nb', \n    dispersion='gene-label'\n)\n\n\nFinally we train a variational autoencoder (VAE) over 250 epochs to embed the cells in 20-dimensional latent space.\n\n\nCode\nint_model.train(\n    early_stopping=True,\n    accelerator='cpu', \n    max_epochs=250, \n    train_size=0.8\n)\n\n\nCode\nad_blood.obsm['X_scVI'] = int_model.get_latent_representation()\n\n\n\n\nCode\nsc.pl.embedding(\n    ad_blood, \n    basis='scVI', \n    color='celltype',\n    title='', \n    frameon=True, \n    size=30, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('scVI 1')\nplt.gca().set_ylabel('scVI 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: scVI embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#snn-graph-estimation",
    "href": "tutorials/Integration_Tutorial.html#snn-graph-estimation",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "SNN graph estimation",
    "text": "SNN graph estimation\nWe use the cosine distance to identify 20 NNs for each cell in the latent scVI space. This neighbor graph will serve as the basis for our nonlinear embeddings.\n\n\nCode\nsc.pp.neighbors(\n    ad_blood, \n    n_neighbors=20,\n    n_pcs=None,  \n    metric='cosine', \n    random_state=312, \n    use_rep='X_scVI'\n)"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#umap-embedding",
    "href": "tutorials/Integration_Tutorial.html#umap-embedding",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "UMAP embedding",
    "text": "UMAP embedding\nUsing default parameters, we fit a 2D UMAP embedding.\n\n\nCode\nsc.tl.umap(ad_blood, random_state=312)\n\n\nThe UMAP embedding is a bit oddly-shaped, but celltypes are well-connected and their progression follows known biology i.e., HSCs form precursor populations which develop into mature monocytic cells.\n\n\nCode\nsc.pl.embedding(\n    ad_blood, \n    basis='umap', \n    color='celltype',\n    title='', \n    frameon=True, \n    size=30, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: scVI-based UMAP embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#force-directed-graph-embedding",
    "href": "tutorials/Integration_Tutorial.html#force-directed-graph-embedding",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Force-directed graph embedding",
    "text": "Force-directed graph embedding\nWith the Fruchterman-Reingold algorithm we generate a force-directed graph embedding in 2 dimensions. In my experience this algorithm often works better than UMAP at preserving trajectory structures.\n\n\nCode\nsc.tl.draw_graph(\n    ad_blood, \n    layout='fr',  \n    random_state=312,\n    n_jobs=2\n)\n\n\nSimilar to the UMAP embedding, the FR embedding is well-connected and the celltypes are arranged correctly. I like this embedding better, but that’s mostly based on aesthetics as well as some intuition about how pseudotime estimation would perform on each.\n\n\nCode\nsc.pl.draw_graph(\n    ad_blood, \n    color='celltype', \n    title='', \n    alpha=0.75, \n    size=30, \n    show=False\n)\nplt.gca().set_xlabel('FR 1')\nplt.gca().set_ylabel('FR 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: scVI-based force-directed graph embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#diffusion-map-embedding",
    "href": "tutorials/Integration_Tutorial.html#diffusion-map-embedding",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Diffusion map embedding",
    "text": "Diffusion map embedding\nLastly, we estimate a diffusion map embedding in 15 dimensions. This algorithm is specifically designed to preserve transitional structures, but in my experience it usually only works well on datasets with very simple trajectories.\n\n\nCode\nsc.tl.diffmap(\n    ad_blood, \n    random_state=312, \n    n_comps=16\n)\nad_blood.obsm['X_diffmap_old'] = ad_blood.obsm['X_diffmap']\nad_blood.obsm['X_diffmap'] = ad_blood.obsm['X_diffmap'][:, 1:] \n\n\nLike the UMAP embedding the diffusion map embedding is pretty angular, but it recapitulates the biology well. Altogether, the scVI integration seems to have worked correctly as all embedding algorithms perform reasonably well.\n\n\nCode\nsc.pl.diffmap(\n    ad_blood, \n    color='celltype', \n    title='', \n    alpha=0.75, \n    size=30, \n    show=False\n)\nplt.gca().set_xlabel('DC 1')\nplt.gca().set_ylabel('DC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: scVI-based diffusion map embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#normalization",
    "href": "tutorials/Integration_Tutorial.html#normalization",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Normalization",
    "text": "Normalization\nThe Harmony algorithm works by “correcting” a principal component embedding for batch effects. As such, we need to first normalize and variance-stabilize the data.\n\n\nCode\nsc.pp.normalize_total(ad_blood, target_sum=1e4)\nsc.pp.log1p(ad_blood)"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#pca-embedding",
    "href": "tutorials/Integration_Tutorial.html#pca-embedding",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "PCA embedding",
    "text": "PCA embedding\nNext we scale the normalized counts and run PCA.\n\n\nCode\nsc.pp.scale(ad_blood)\nsc.tl.pca(\n    ad_blood, \n    n_comps=30, \n    random_state=312, \n    use_highly_variable=True\n)\n\n\nThe PCA embedding on its own is alright, and I don’t imagine the integration procedure will change it much.\n\n\nCode\nsc.pl.embedding(\n    ad_blood, \n    basis='pca', \n    color='celltype', \n    title='', \n    frameon=True, \n    size=30, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('PC 1')\nplt.gca().set_ylabel('PC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: PCA embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#pca-correction-with-harmony",
    "href": "tutorials/Integration_Tutorial.html#pca-correction-with-harmony",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "PCA correction with Harmony",
    "text": "PCA correction with Harmony\n\n\nCode\nsc.external.pp.harmony_integrate(ad_blood, key='fetal_ID')\n\n\n2023-12-08 17:54:12,890 - harmonypy - INFO - Computing initial centroids with sklearn.KMeans...\n/Users/jack/Desktop/PhD/Research/Python_Envs/personal_site/lib/python3.11/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.11 is maybe too old for this OS.\n  warnings.warn(\n2023-12-08 17:54:12,945 - harmonypy - INFO - sklearn.KMeans initialization complete.\n2023-12-08 17:54:12,946 - harmonypy - INFO - Iteration 1 of 10\n2023-12-08 17:54:12,992 - harmonypy - INFO - Iteration 2 of 10\n2023-12-08 17:54:13,037 - harmonypy - INFO - Iteration 3 of 10\n2023-12-08 17:54:13,081 - harmonypy - INFO - Iteration 4 of 10\n2023-12-08 17:54:13,127 - harmonypy - INFO - Iteration 5 of 10\n2023-12-08 17:54:13,171 - harmonypy - INFO - Converged after 5 iterations\n\n\nAs expected, the corrected PCA space is pretty similar to the original one.\n\n\nCode\nsc.pl.embedding(\n    ad_blood, \n    basis='pca_harmony', \n    color='celltype', \n    title='', \n    frameon=True, \n    size=30, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('Harmony 1')\nplt.gca().set_ylabel('Harmony 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Harmony embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#snn-graph-estimation-1",
    "href": "tutorials/Integration_Tutorial.html#snn-graph-estimation-1",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "SNN graph estimation",
    "text": "SNN graph estimation\nWe compute \\(k = 20\\) NNs in the Harmony PCA space.\n\n\nCode\nsc.pp.neighbors(\n    ad_blood, \n    n_neighbors=20,\n    n_pcs=None,  \n    metric='cosine', \n    random_state=312, \n    use_rep='X_pca_harmony'\n)"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#umap-embedding-1",
    "href": "tutorials/Integration_Tutorial.html#umap-embedding-1",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "UMAP embedding",
    "text": "UMAP embedding\n\n\nCode\nsc.tl.umap(ad_blood, random_state=312)\n\n\nUh oh - the UMAP embedding displays disconnected clusters of cells. This indicates that the integration didn’t perform very well, though other dimension reduction algorithms might perform better.\n\n\nCode\nsc.pl.embedding(\n    ad_blood, \n    basis='umap', \n    color='celltype',\n    title='', \n    frameon=True, \n    size=30, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Harmony-based UMAP embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#force-directed-graph-embedding-1",
    "href": "tutorials/Integration_Tutorial.html#force-directed-graph-embedding-1",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Force-directed graph embedding",
    "text": "Force-directed graph embedding\n\n\nCode\nsc.tl.draw_graph(\n    ad_blood, \n    layout='fr',  \n    random_state=312,\n    n_jobs=2\n)\n\n\nThe FR embedding shows a disconnected cluster of monocytes as well. In addition, the progression from HSCs to progenitor / mature states is not well-represented.\n\n\nCode\nsc.pl.draw_graph(\n    ad_blood, \n    color='celltype', \n    title='', \n    alpha=0.75, \n    size=30, \n    show=False\n)\nplt.gca().set_xlabel('FR 1')\nplt.gca().set_ylabel('FR 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: Harmony-based force-directed graph embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#diffusion-map-embedding-1",
    "href": "tutorials/Integration_Tutorial.html#diffusion-map-embedding-1",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Diffusion map embedding",
    "text": "Diffusion map embedding\n\n\nCode\nsc.tl.diffmap(\n    ad_blood, \n    random_state=312, \n    n_comps=16\n)\nad_blood.obsm['X_diffmap_old'] = ad_blood.obsm['X_diffmap']\nad_blood.obsm['X_diffmap'] = ad_blood.obsm['X_diffmap'][:, 1:] \n\n\nThe diffusion map embedding is also fairly disconnected and has a tiny outlier cluster.\n\n\nCode\nsc.pl.diffmap(\n    ad_blood, \n    color='celltype', \n    title='', \n    alpha=0.75, \n    size=30, \n    show=False\n)\nplt.gca().set_xlabel('DC 1')\nplt.gca().set_ylabel('DC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Harmony-based diffusion map embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#batch-specific-snn-estimation",
    "href": "tutorials/Integration_Tutorial.html#batch-specific-snn-estimation",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Batch-specific SNN estimation",
    "text": "Batch-specific SNN estimation\nNext we try the BBKNN algorithm which, instead of producing an integrated embedding (as do the other methods) produces a batch-corrected neighborhood graph. We can then use that graph structure to generate UMAP and other embeddings.\n\n\nCode\nsc.external.pp.bbknn(\n  ad_blood, \n  batch_key='fetal_ID',\n  use_annoy=False,\n  metric='cosine',\n  pynndescent_random_state=312,\n  pynndescent_n_neighbors=20\n)\n\n\nWARNING: consider updating your call to make use of `computation`"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#umap-embedding-2",
    "href": "tutorials/Integration_Tutorial.html#umap-embedding-2",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "UMAP embedding",
    "text": "UMAP embedding\n\n\nCode\nsc.tl.umap(ad_blood, random_state=312)\n\n\nThe UMAP is somewhat smoothly-connected, but the biological sequence of celltypes isn’t correct.\n\n\nCode\nsc.pl.embedding(\n    ad_blood, \n    basis='umap', \n    color='celltype',\n    title='', \n    frameon=True, \n    size=30, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: BBKNN-based UMAP embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#force-directed-graph-embedding-2",
    "href": "tutorials/Integration_Tutorial.html#force-directed-graph-embedding-2",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Force-directed graph embedding",
    "text": "Force-directed graph embedding\n\n\nCode\nsc.tl.draw_graph(\n    ad_blood, \n    layout='fr',  \n    random_state=312,\n    n_jobs=2\n)\n\n\nThe FR embedding suffers from the same issue.\n\n\nCode\nsc.pl.draw_graph(\n    ad_blood, \n    color='celltype', \n    title='', \n    alpha=0.75, \n    size=30, \n    show=False\n)\nplt.gca().set_xlabel('FR 1')\nplt.gca().set_ylabel('FR 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11: BBKNN-based force-directed graph embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#diffusion-map-embedding-2",
    "href": "tutorials/Integration_Tutorial.html#diffusion-map-embedding-2",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Diffusion map embedding",
    "text": "Diffusion map embedding\n\n\nCode\nsc.tl.diffmap(\n    ad_blood, \n    random_state=312, \n    n_comps=16\n)\nad_blood.obsm['X_diffmap_old'] = ad_blood.obsm['X_diffmap']\nad_blood.obsm['X_diffmap'] = ad_blood.obsm['X_diffmap'][:, 1:] \n\n\nThe diffusion map embedding also misplaces the monocyte phenotype.\n\n\nCode\nsc.pl.diffmap(\n    ad_blood, \n    color='celltype', \n    title='', \n    alpha=0.75, \n    size=30, \n    show=False\n)\nplt.gca().set_xlabel('DC 1')\nplt.gca().set_ylabel('DC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 12: BBKNN-based diffusion map embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#batch-correcting-integration",
    "href": "tutorials/Integration_Tutorial.html#batch-correcting-integration",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Batch-correcting integration",
    "text": "Batch-correcting integration\nThe last method we’ll try is Scanorama. Like scVi and Harmony this algorithm produces a corrected low-dimensional embedding.\n\n\nCode\nsc.external.pp.scanorama_integrate(\n  ad_blood, \n  key='fetal_ID', \n  basis='X_pca', \n  knn=20\n)\n\n\n[[0. 1.]\n [0. 0.]]\nProcessing datasets F11_female_12+3PCW &lt;=&gt; F17_male_9+1PCW"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#snn-graph-estimation-2",
    "href": "tutorials/Integration_Tutorial.html#snn-graph-estimation-2",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "SNN graph estimation",
    "text": "SNN graph estimation\nWe identify \\(k = 20\\) NNs in the integrated space.\n\n\nCode\nsc.pp.neighbors(\n    ad_blood, \n    n_neighbors=20,\n    n_pcs=None,  \n    metric='cosine', \n    random_state=312, \n    use_rep='X_scanorama'\n)"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#umap-embedding-3",
    "href": "tutorials/Integration_Tutorial.html#umap-embedding-3",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "UMAP embedding",
    "text": "UMAP embedding\n\n\nCode\nsc.tl.umap(ad_blood, random_state=312)\n\n\nWith UMAP the celltype progression is represented correctly, though the transition from the monocyte-macrophage phenotype to the mature Kupffer cells is a bit wonky.\n\n\nCode\nsc.pl.embedding(\n    ad_blood, \n    basis='umap', \n    color='celltype',\n    title='', \n    frameon=True, \n    size=30, \n    alpha=0.75, \n    show=False\n)\nplt.gca().set_xlabel('UMAP 1')\nplt.gca().set_ylabel('UMAP 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 13: Scanorama-based UMAP embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#force-directed-graph-embedding-3",
    "href": "tutorials/Integration_Tutorial.html#force-directed-graph-embedding-3",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Force-directed graph embedding",
    "text": "Force-directed graph embedding\n\n\nCode\nsc.tl.draw_graph(\n    ad_blood, \n    layout='fr',  \n    random_state=312,\n    n_jobs=2\n)\n\n\nThe FR embedding is a bit worse and isn’t very smoothly connected.\n\n\nCode\nsc.pl.draw_graph(\n    ad_blood, \n    color='celltype', \n    title='', \n    alpha=0.75, \n    size=30, \n    show=False\n)\nplt.gca().set_xlabel('FR 1')\nplt.gca().set_ylabel('FR 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14: Harmony-based force-directed graph embedding"
  },
  {
    "objectID": "tutorials/Integration_Tutorial.html#diffusion-map-embedding-3",
    "href": "tutorials/Integration_Tutorial.html#diffusion-map-embedding-3",
    "title": "Integrating scRNA-seq Datasets with Python",
    "section": "Diffusion map embedding",
    "text": "Diffusion map embedding\n\n\nCode\nsc.tl.diffmap(\n    ad_blood, \n    random_state=312, \n    n_comps=16\n)\nad_blood.obsm['X_diffmap_old'] = ad_blood.obsm['X_diffmap']\nad_blood.obsm['X_diffmap'] = ad_blood.obsm['X_diffmap'][:, 1:] \n\n\nThe diffusion map embedding is mostly OK, but there’s a weird outlier cluster of monocytes located at the minimum of the first component.\n\n\nCode\nsc.pl.diffmap(\n    ad_blood, \n    color='celltype', \n    title='', \n    alpha=0.75, \n    size=30, \n    show=False\n)\nplt.gca().set_xlabel('DC 1')\nplt.gca().set_ylabel('DC 2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 15: Scanorama-based diffusion map embedding"
  },
  {
    "objectID": "tutorials/SCISSORS_Reclustering.html",
    "href": "tutorials/SCISSORS_Reclustering.html",
    "title": "scRNA-seq Reclustering with SCISSORS",
    "section": "",
    "text": "In this tutorial we’ll walk through a basic single cell analysis, with a focus on fine-tuning clustering results using the SCISSORS package, which I wrote during my time at UNC Chapel Hill."
  },
  {
    "objectID": "tutorials/SCISSORS_Reclustering.html#preprocessing",
    "href": "tutorials/SCISSORS_Reclustering.html#preprocessing",
    "title": "scRNA-seq Reclustering with SCISSORS",
    "section": "Preprocessing",
    "text": "Preprocessing\nWe’ll do some minor quality-control checking first by filtering out cells with a high percentage of mitochondrial reads or very low or high numbers of detected genes.\n\n\nCode\npbmc &lt;- PercentageFeatureSet(pbmc, \n                             pattern = \"^MT-\", \n                             col.name = \"percent_MT\")\npbmc &lt;- pbmc[, pbmc$nFeature_RNA &gt;= 200 & pbmc$nFeature_RNA &lt;= 2500 & pbmc$percent_MT &lt;= 10]\n\n\nWe’ll process the raw counts in the usual fashion: QC, normalization, identification of highly variable genes (HVGs), linear & non-linear dimension reduction, and a broad clustering that will (hopefully) capture our major celltypes. When computing the shared nearest-neighbor (SNN) graph, we use the heuristic \\(k = \\sqrt{n}\\) for the number of nearest-neighbors to consider for each cell. This ensures that the clustering will be broad i.e., a smaller number of large clusters will be returned instead of a larger number of small clusters.\n\n\nCode\npbmc &lt;- NormalizeData(pbmc, \n                      normalization.method = \"LogNormalize\", \n                      verbose = FALSE) %&gt;% \n        FindVariableFeatures(selection.method = \"vst\", \n                             nfeatures = 3000, \n                             verbose = FALSE) %&gt;% \n        CellCycleScoring(s.features = cc.genes.updated.2019$s.genes, \n                         g2m.features = cc.genes.updated.2019$g2m.genes, \n                         set.ident = FALSE) %&gt;% \n        AddMetaData(metadata = c(.$S.Score - .$G2M.Score), col.name = \"CC_difference\") %&gt;% \n        ScaleData(vars.to.regress = \"CC_difference\", verbose = FALSE) %&gt;% \n        RunPCA(features = VariableFeatures(.), \n               npcs = 50, \n               verbose = FALSE, \n               seed.use = 312) %&gt;% \n        RunUMAP(reduction = \"pca\",\n                dims = 1:20, \n                n.components = 2, \n                metric = \"cosine\", \n                seed.use = 312, \n                verbose = FALSE) %&gt;% \n        FindNeighbors(reduction = \"pca\", \n                      dims = 1:20, \n                      k.param = sqrt(ncol(.)), \n                      nn.method = \"annoy\", \n                      annoy.metric = \"cosine\", \n                      verbose = FALSE) %&gt;% \n        FindClusters(resolution = 0.3, \n                     random.seed = 312, \n                     verbose = FALSE)\n\n\nLet’s visualize the principal components. Notable genes in PC 1 include MALAT1, high abundance of which is a common artifact of 10X-sequenced data. PC 2 seems to separate NK cells (NKG7, GZMB) and myeloid cells (HLA-DRA, CD79A). PC 3 is composed of variation that could originate from platelets (PPBP). PCs 4-6 look like they separate several types of monocytic, T, NK, and dendritic cells.\n\n\nCode\nDimHeatmap(pbmc, \n           reduction = \"pca\", \n           dims = 1:6, \n           nfeatures = 15, \n           combine = TRUE)\n\n\n\n\n\n\n\n\n\nWe visualize the Louvain clustering via a UMAP plot. We see 5 major clusters, which we’ll annotate next.\n\n\nCode\nDimPlot(pbmc, pt.size = 1) + \n  scale_color_paletteer_d(\"ggsci::nrc_npg\") + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank())"
  },
  {
    "objectID": "tutorials/SCISSORS_Reclustering.html#broad-annotations",
    "href": "tutorials/SCISSORS_Reclustering.html#broad-annotations",
    "title": "scRNA-seq Reclustering with SCISSORS",
    "section": "Broad Annotations",
    "text": "Broad Annotations\nFirst we identify CD8+ T-cells via CD8A, and CD4+ T-cells with IL7R. Lastly, FCGR3A (aka CD16) is specific to CD16+ monocytes. We can combine the plots using the excellent patchwork package.\n\n\nCode\np1 &lt;- FeaturePlot(pbmc, features = \"CD8A\", pt.size = 1) + \n      scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme(axis.ticks = element_blank(), \n            axis.text = element_blank())\np2 &lt;- FeaturePlot(pbmc, features = \"IL7R\", pt.size = 1) + \n      scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme(axis.ticks = element_blank(), \n            axis.text = element_blank())\np1 / p2\n\n\n\n\n\n\n\n\n\nNext, we use HLA-DRA to broadly identify monocytic cells, and FCGR3A (aka CD16) to single out the CD16+ monocytes.\n\n\nCode\np1 &lt;- FeaturePlot(pbmc, features = \"HLA-DRA\", pt.size = 1) + \n      scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme(axis.ticks = element_blank(), \n            axis.text = element_blank())\np2 &lt;- FeaturePlot(pbmc, features = \"FCGR3A\", pt.size = 1) + \n      scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme(axis.ticks = element_blank(), \n            axis.text = element_blank())\np1 / p2\n\n\n\n\n\n\n\n\n\nLastly, abundance of MS4A1 points out a cluster of B cells.\n\n\nCode\nFeaturePlot(pbmc, features = \"MS4A1\", pt.size = 1) + \n  scale_color_gradientn(colours = paletteer_d(\"wesanderson::Zissou1\")) + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank())\n\n\n\n\n\n\n\n\n\nWe’ll add broad celltype labels to our object’s metadata.\n\n\nCode\npbmc@meta.data &lt;- mutate(pbmc@meta.data, \n                         broad_celltype = case_when(seurat_clusters == 0 ~ \"CD4+ T\", \n                                                    seurat_clusters == 1 ~ \"Monocyte\", \n                                                    seurat_clusters == 2 ~ \"CD8+ T\", \n                                                    seurat_clusters == 3 ~ \"B\", \n                                                    seurat_clusters == 4 ~ \"CD16+ Monocyte\", \n                                                    TRUE ~ NA_character_), \n                         broad_celltype = factor(broad_celltype, levels = c(\"CD4+ T\", \n                                                                            \"Monocyte\", \n                                                                            \"CD8+ T\", \n                                                                            \"B\", \n                                                                            \"CD16+ Monocyte\")))\n\n\nAnd visualize the results.\n\n\nCode\nDimPlot(pbmc, pt.size = 1, group.by = \"broad_celltype\") + \n  scale_color_paletteer_d(\"ggsci::nrc_npg\") + \n  labs(x = \"UMAP 1\", \n       y = \"UMAP 2\", \n       color = \"Broad Celltype\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.title = element_blank())"
  },
  {
    "objectID": "tutorials/SCISSORS_Reclustering.html#reclustering",
    "href": "tutorials/SCISSORS_Reclustering.html#reclustering",
    "title": "scRNA-seq Reclustering with SCISSORS",
    "section": "Reclustering",
    "text": "Reclustering\nFrom the plot above, there appears to be some visible subgroups in the monocyte cluster. With that being said - I would generally be very cautious about using UMAPs alone to define heterogeneous groups. In general, I would suggest using something like silhouette score distributions, other clustering statistics, or biological knowledge to determine subclustering targets. We can do this below using ComputeSilhouetteScores(), which returns a silhouette score for each individual cell. Visualizing the results can help us identify which clusters are “poor” fits. For more information, check out the Wikipedia article on clustering scores.\n\n\nCode\nsil_scores &lt;- ComputeSilhouetteScores(pbmc, avg = FALSE)\n\n\nWe can see that the B cell and CD16+ monocyte clusters seem to be well-fit, but the other clusters are less so. We’ll focus on the other monocyte cluster, as it seems to have the highest variance.\n\n\nCode\nsil_scores %&gt;% \n  left_join(distinct(pbmc@meta.data, seurat_clusters, broad_celltype), \n            by = c(\"Cluster\" = \"seurat_clusters\")) %&gt;% \n  ggplot(aes(x = broad_celltype, y = Score, fill = broad_celltype)) + \n  geom_violin(scale = \"width\", \n              color = \"black\", \n              draw_quantiles = 0.5, \n              size = 0.75) + \n  scale_fill_paletteer_d(\"ggsci::nrc_npg\") + \n  labs(y = \"Silhouette Score\", fill = \"Broad Celltype\") + \n  theme_classic(base_size = 14) + \n  theme(panel.grid.major.y = element_line(), \n        axis.title.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nMonocytes\n\n\nCode\nmono_reclust &lt;- ReclusterCells(pbmc, \n                               which.clust = 1, \n                               use.parallel = FALSE, \n                               n.HVG = 3000,\n                               n.PC = 15, \n                               k.vals = c(20, 30, 40), \n                               resolution.vals = c(.2, .3, .4), \n                               random.seed = 312)\n\n\n[1] \"Reclustering cells in cluster 1 using k = 20 & resolution = 0.2; S = 0.419\"\n\n\nLet’s check out the UMAP embedding:\n\n\nCode\nDimPlot(mono_reclust) + \n  scale_color_paletteer_d(\"MetBrewer::Egypt\") + \n  labs(x = \"UMAP 1\", \n       y = \"UMAP 2\", \n       color = \"Subcluster\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.title = element_blank())\n\n\n\n\n\n\n\n\n\nHighly-specific abundance of FCER1A allows us to identify the dendritic cells in cluster 2.\n\n\nCode\ndata.frame(exp = mono_reclust@assays$RNA@data[\"FCER1A\", ], \n           label = mono_reclust$seurat_clusters) %&gt;% \n  ggplot(aes(x = label, y = exp, fill = label)) + \n  geom_violin(scale = \"width\",\n              color = \"black\", \n              draw_quantiles = 0.5, \n              size = 0.75) + \n  scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n  labs(y = \"FCER1A\", fill = \"Subcluster\") + \n  theme_classic(base_size = 14) + \n  theme(panel.grid.major.y = element_line(), \n        axis.title.x = element_blank())\n\n\n\n\n\n\n\n\n\nBoth cluster 0 & cluster 1 seem to be CD14+, and cluster 1 appears to have slightly higher (but still low) abundance of FCGR3A.\n\n\nCode\np1 &lt;- data.frame(exp = mono_reclust@assays$RNA@data[\"CD14\", ], \n                 label = mono_reclust$seurat_clusters) %&gt;% \n      filter(label %in% c(0, 1)) %&gt;% \n      ggplot(aes(x = label, y = exp, fill = label)) + \n      geom_violin(scale = \"width\",\n                  color = \"black\", \n                  draw_quantiles = 0.5, \n                  size = 0.75) + \n      scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n      labs(y = \"CD14\", fill = \"Subcluster\") + \n      theme_classic(base_size = 14) + \n      theme(panel.grid.major.y = element_line(), \n            axis.title.x = element_blank())\np2 &lt;- data.frame(exp = mono_reclust@assays$RNA@data[\"FCGR3A\", ], \n                 label = mono_reclust$seurat_clusters) %&gt;% \n      filter(label %in% c(0, 1)) %&gt;% \n      ggplot(aes(x = label, y = exp, fill = label)) + \n      geom_violin(scale = \"width\",\n                  color = \"black\", \n                  draw_quantiles = 0.5, \n                  size = 0.75) + \n      scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n      labs(y = \"FCGR3A\", fill = \"Subcluster\") + \n      theme_classic(base_size = 14) + \n      theme(panel.grid.major.y = element_line(), \n            axis.title.x = element_blank())\np1 / p2\n\n\n\n\n\n\n\n\n\nFrom Kapellos et al (2019), we know that intermediate monocytes have high abundance of CD14, low but non-zero abundance of CD16 (which again is denoted FCGR3A in this dataset), and can be identified through higher abundance of other markers like HLA-DPB1 and CD74 in comparison to CD14+ monocytes. With all this information, we’ll conclude that cluster 0 is likely composed of CD14+ monocytes and cluster 1 of intermediate monocytes.\n\n\nCode\np1 &lt;- data.frame(exp = mono_reclust@assays$RNA@data[\"HLA-DQB1\", ], \n                 label = mono_reclust$seurat_clusters) %&gt;% \n      filter(label %in% c(0, 1)) %&gt;% \n      ggplot(aes(x = label, y = exp, fill = label)) + \n      geom_violin(scale = \"width\",\n                  color = \"black\", \n                  draw_quantiles = 0.5, \n                  size = 0.75) + \n      scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n      labs(y = \"HLA-DQB1\", fill = \"Subcluster\") + \n      theme_classic(base_size = 14) + \n      theme(panel.grid.major.y = element_line(), \n            axis.title.x = element_blank())\np2 &lt;- data.frame(exp = mono_reclust@assays$RNA@data[\"CD74\", ], \n                 label = mono_reclust$seurat_clusters) %&gt;% \n      filter(label %in% c(0, 1)) %&gt;% \n      ggplot(aes(x = label, y = exp, fill = label)) + \n      geom_violin(scale = \"width\",\n                  color = \"black\", \n                  draw_quantiles = 0.5, \n                  size = 0.75) + \n      scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n      labs(y = \"CD74\", fill = \"Subcluster\") + \n      theme_classic(base_size = 14) + \n      theme(panel.grid.major.y = element_line(), \n            axis.title.x = element_blank())\np1 / p2\n\n\n\n\n\n\n\n\n\nLastly, we can tell that cluster 3 is composed of platelets thanks to high abundance of PPBP.\n\n\nCode\ndata.frame(exp = mono_reclust@assays$RNA@data[\"PPBP\", ], \n           label = mono_reclust$seurat_clusters) %&gt;% \n  ggplot(aes(x = label, y = exp, fill = label)) + \n  geom_violin(scale = \"width\",\n              color = \"black\", \n              draw_quantiles = 0.5, \n              size = 0.75) + \n  scale_fill_paletteer_d(\"MetBrewer::Egypt\") + \n  labs(y = \"PPBP\", fill = \"Subcluster\") + \n  theme_classic(base_size = 14) + \n  theme(panel.grid.major.y = element_line(), \n        axis.title.x = element_blank())\n\n\n\n\n\n\n\n\n\nWe can add the new subcluster labels back in to our original object using IntegrateSubclusters(). We also add labels to the original object reflecting the subcluster annotations.\n\n\nCode\npbmc &lt;- IntegrateSubclusters(pbmc, reclust.results = mono_reclust)\npbmc@meta.data &lt;- mutate(pbmc@meta.data, \n                         celltype = case_when(seurat_clusters == 0 ~ \"CD4+ T\", \n                                              seurat_clusters == 1 ~ \"Platelet\", \n                                              seurat_clusters == 2 ~ \"CD8+ T\", \n                                              seurat_clusters == 3 ~ \"B\", \n                                              seurat_clusters == 4 ~ \"CD16+ Monocyte\", \n                                              seurat_clusters == 5 ~ \"CD14+ Monocyte\", \n                                              seurat_clusters == 6 ~ \"Intermediate Monocyte\", \n                                              seurat_clusters == 7 ~ \"Dendritic Cell\", \n                                              TRUE ~ NA_character_))\n\n\nHere’s the final celltype annotations on our original UMAP embedding.\n\n\nCode\nDimPlot(pbmc, group.by = \"celltype\", pt.size = 1) + \n  scale_color_paletteer_d(\"ggsci::default_nejm\") + \n  labs(x = \"UMAP 1\", \n       y = \"UMAP 2\", \n       color = \"Celltype\") + \n  theme(axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.title = element_blank())"
  },
  {
    "objectID": "tutorials/Trajectory_GRN.html",
    "href": "tutorials/Trajectory_GRN.html",
    "title": "Building a Trajectory Regulatory Network with scRNA-seq Data",
    "section": "",
    "text": "In this analysis we’ll be looking at cells undergoing neurogenesis in the developing human frontal cortex. The data are taken from Nowakowski et al (2017). Our goal is to build a simple method for fitting single cell gene regulatory networks (GRNs) and compare the results across pseudotime lineages, keeping in mind the trajectory structure of the data. If you want to skip straight to the analysis, head to Section 6."
  },
  {
    "objectID": "tutorials/Trajectory_GRN.html#celltype-annotation",
    "href": "tutorials/Trajectory_GRN.html#celltype-annotation",
    "title": "Building a Trajectory Regulatory Network with scRNA-seq Data",
    "section": "6.1 Celltype annotation",
    "text": "6.1 Celltype annotation\nWe begin by running a basic differential expression (DE) testing routine to identify some potential marker genes for each cluster. This will help us assign a celltype identity to each cluster.\n\n\nCode\ncluster_markers &lt;- FindAllMarkers(seu_cortex, \n                                  assay = \"RNA\", \n                                  logfc.threshold = 0.25, \n                                  test.use = \"wilcox\", \n                                  slot = \"data\", \n                                  min.pct = 0.1, \n                                  only.pos = TRUE, \n                                  verbose = FALSE, \n                                  random.seed = 312)\ntop5_cluster_markers &lt;- arrange(cluster_markers, p_val_adj) %&gt;% \n                        with_groups(cluster, \n                                    slice_head, \n                                    n = 5)\n\n\nWe visualize the markers for each cluster with a dotplot, which shows clear differences between clusters. For example, the transcription factor (TF) aristaless related homeobox (ARX) is specifically expressed in cluster 7; this TF’s expression is known to be necessary for healthy neuronal development and helps to regulate differentiation.\n\n\nCode\np6 &lt;- DotPlot(seu_cortex, \n              features = unique(top5_cluster_markers$gene),\n              assay = \"RNA\", \n              group.by = \"seurat_clusters\", \n              dot.scale = 5,\n              cols = paletteer::paletteer_d(\"wesanderson::Zissou1\")[c(1, 5)], \n              scale.by = \"radius\") + \n      coord_flip() +\n      labs(y = \"Leiden Cluster\") +\n      theme_scLANE() + \n      theme(axis.title.y = element_blank(), \n            axis.text.y = element_text(face = \"italic\"))\np6\n\n\n\n\n\n\n\n\nFigure 2: Putative marker genes for each cluster\n\n\n\n\n\nNow we identify markers for each celltype:\n\n\nCode\nIdents(seu_cortex) &lt;- \"celltype_original\"\ncelltype_markers &lt;- FindAllMarkers(seu_cortex, \n                                   assay = \"RNA\", \n                                   logfc.threshold = 0.25, \n                                   test.use = \"wilcox\", \n                                   slot = \"data\", \n                                   min.pct = 0.1, \n                                   only.pos = TRUE, \n                                   verbose = FALSE, \n                                   random.seed = 312)\ntop3_celltype_markers &lt;- arrange(celltype_markers, p_val_adj) %&gt;% \n                         with_groups(cluster, \n                                     slice_head, \n                                     n = 3)\n\n\nThe markers seem consistent with what we’d expect for excitatory neurons; for more information see e.g., Figure 4 and Suppl. Figures 4 & 11 in the original publication.\n\n\nCode\nselect(top3_celltype_markers, \n       celltype = cluster, \n       gene, \n       avg_log2FC, \n       p_val_adj) %&gt;% \n  kableExtra::kbl(digits = 3, \n                  booktabs = TRUE, \n                  col.names = c(\"Celltype\", \"Gene\", \"Mean log2FC\", \"Adj. P-value\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nCelltype\nGene\nMean log2FC\nAdj. P-value\n\n\n\n\nEarly Exc. Neuron 2\nNRP1\n1.124\n0\n\n\nEarly Exc. Neuron 2\nDPY19L1\n1.096\n0\n\n\nEarly Exc. Neuron 2\nSOX11\n0.624\n0\n\n\nLate Exc. Neuron\nZFHX4\n2.034\n0\n\n\nLate Exc. Neuron\nRP11.436D23.1\n1.751\n0\n\n\nLate Exc. Neuron\nLINC00478\n2.164\n0\n\n\nExc. Neuron 2\nDOK5\n2.873\n0\n\n\nExc. Neuron 2\nSATB2\n1.677\n0\n\n\nExc. Neuron 2\nSTMN2\n0.857\n0\n\n\nExc. Neuron 3\nACTN2\n4.731\n0\n\n\nExc. Neuron 3\nLPL\n3.325\n0\n\n\nExc. Neuron 3\nUNC5C\n6.760\n0\n\n\nPFC Exc. Neuron 3\nRP11.98J23.2\n3.008\n0\n\n\nPFC Exc. Neuron 3\nRP11.589F5.4\n3.111\n0\n\n\nPFC Exc. Neuron 3\nRP11.1415C14.3\n3.198\n0\n\n\nExc. Neuron 1\nCRYM\n3.523\n0\n\n\nExc. Neuron 1\nGRIK3\n2.968\n0\n\n\nExc. Neuron 1\nFEZF2\n3.872\n0\n\n\nPFC Exc. Neuron 1\nCDH18\n3.441\n0\n\n\nPFC Exc. Neuron 1\nSORCS1\n4.858\n0\n\n\nPFC Exc. Neuron 1\nCOL19A1\n9.146\n0\n\n\nEarly Exc. Neuron 1\nNDST4\n3.965\n0\n\n\nEarly Exc. Neuron 1\nSTK17B\n3.639\n0\n\n\nEarly Exc. Neuron 1\nCNTNAP2\n2.501\n0\n\n\nPFC Exc. Neuron 2\nCPNE8\n5.170\n0\n\n\nPFC Exc. Neuron 2\nCBLN2\n4.410\n0\n\n\nPFC Exc. Neuron 2\nSLN\n4.456\n0\n\n\n\n\n\n\n\n\nTable 1: The top 3 markers genes for each identified celltype"
  },
  {
    "objectID": "tutorials/Trajectory_GRN.html#pseudotime-estimation",
    "href": "tutorials/Trajectory_GRN.html#pseudotime-estimation",
    "title": "Building a Trajectory Regulatory Network with scRNA-seq Data",
    "section": "6.2 Pseudotime estimation",
    "text": "6.2 Pseudotime estimation\nAfter estimating principal curves across our UMAP embedding with slingshot, we generate per-lineage pseudotime values for each cell. We use our clustering as a source of structure in the data, and assign start and end clusters based on age in PCW. In addition, we mean-aggregate the pseudotime values for each cell in order to get a total pseudotime which represents overall differentiation regardless of lineage.\n\n\nCode\nsling_res &lt;- slingshot(Embeddings(seu_cortex, \"umap\"), \n                       clusterLabels = seu_cortex$seurat_clusters, \n                       start.clus = c(\"3\"), \n                       end.clus = c(\"5\", \"6\", \"7\"), \n                       approx_points = 1e3)\nsling_curves &lt;- slingCurves(sling_res, as.df = TRUE)\nsling_mst &lt;- slingMST(sling_res, as.df = TRUE)\nsling_pt &lt;- as.data.frame(slingPseudotime(sling_res)) %&gt;% \n            rowwise() %&gt;% \n            mutate(PT_Overall = mean(c_across(starts_with(\"Lineage\")), na.rm = TRUE)) %&gt;% \n            ungroup() %&gt;% \n            mutate(across(c(starts_with(\"Lineage\"), PT_Overall), \n                          \\(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))\nseu_cortex &lt;- AddMetaData(seu_cortex, \n                          metadata = sling_pt$PT_Overall, \n                          col.name = \"PT_Overall\")\n\n\nThe graph structure estimate by slingshot is called a minimum spanning tree (MST), and describes the relationships between clusters in an undirected manner.\n\n\nCode\np7 &lt;- data.frame(Embeddings(seu_cortex, \"umap\")) %&gt;% \n      mutate(leiden = seu_cortex$seurat_clusters) %&gt;% \n      ggplot(aes(x = umap_1, y = umap_2, color = leiden)) + \n      geom_point(size = 1.25, alpha = 0.75) + \n      geom_path(data = sling_mst, mapping = aes(x = umap_1, y = umap_2, group = Lineage), \n                linewidth = 1.25, \n                color = \"black\") + \n      geom_point(data = sling_mst, mapping = aes(x = umap_1, y = umap_2, fill = Cluster), \n                color = \"black\", \n                shape = 21, \n                size = 4.5, \n                stroke = 1.25, \n                show.legend = FALSE) + \n      scale_color_manual(values = palette_cluster) + \n      scale_fill_manual(values = palette_cluster) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme_scLANE(umap = TRUE) +  \n      theme(legend.title = element_blank()) + \n      guide_umap()\np7\n\n\n\n\n\n\n\n\nFigure 3: UMAP embedding with MST from Slinghot overlaid\n\n\n\n\n\nSince our data have real-life experimental timepoints, it makes sense to normalize the pseudotime within each lineage to \\([0, 1]\\).\n\n\nCode\np8 &lt;- data.frame(Embeddings(seu_cortex, \"umap\")) %&gt;% \n      bind_cols(sling_pt) %&gt;% \n      tidyr::pivot_longer(starts_with(\"Lineage\"), \n                          names_to = \"lineage\", \n                          values_to = \"pseudotime\") %&gt;% \n      ggplot(aes(x = umap_1, y = umap_2, color = pseudotime)) + \n      facet_wrap(~lineage, nrow = 3) + \n      geom_point(size = 1.25, alpha = 0.75) + \n      labs(x = \"UMAP 1\", \n           y = \"UMAP 2\", \n           color = \"Pseudotime\") + \n      scale_color_gradientn(colors = palette_heatmap, labels = scales::label_number(accuracy = .01)) + \n      theme_scLANE(umap = TRUE)\np8\n\n\n\n\n\n\n\n\nFigure 4: UMAP embedding colored by per-lineage pseudotime\n\n\n\n\n\nThe principal curves show a trifurcating lineage structure.\n\n\nCode\np9 &lt;- data.frame(Embeddings(seu_cortex, \"umap\")) %&gt;% \n      mutate(leiden = seu_cortex$seurat_clusters) %&gt;% \n      ggplot(aes(x = umap_1, y = umap_2, color = leiden)) + \n      geom_point(size = 1.25, alpha = 0.75) + \n      geom_path(data = sling_curves,\n                mapping = aes(x = umap_1, y = umap_2, group = Lineage), \n                color = \"black\", \n                linewidth = 1.5, \n                alpha = 0.75, \n                lineend = \"round\") + \n      scale_color_manual(values = palette_cluster) + \n      labs(x = \"UMAP 1\", y = \"UMAP 2\") + \n      theme_scLANE(umap = TRUE) + \n      theme(legend.title = element_blank()) + \n      guides(color = guide_legend(override.aes = list(size = 4, alpha = 1, stroke = 0.25)))\np9\n\n\n\n\n\n\n\n\nFigure 5: UMAP embedding with principal curves from Slinghot overlaid in black\n\n\n\n\n\nLastly, we plot the distribution of mean-aggregated pseudotime per-timepoint and see that later timepoints have, in general, larger values of overall pseudotime. This indicates that our estimated pseudotime is linked to real experimental time in a meaningful (though fairly noisy) way.\n\n\nCode\np10 &lt;- select(seu_cortex@meta.data, \n              Age_in_Weeks, \n              PT_Overall) %&gt;% \n       mutate(Age_in_Weeks = as.factor(Age_in_Weeks)) %&gt;% \n       ggplot(aes(x = Age_in_Weeks, y = PT_Overall, color = Age_in_Weeks)) + \n       ggbeeswarm::geom_quasirandom(size = 1.25, \n                                    alpha = 0.75, \n                                    show.legend = FALSE) + \n       stat_summary(geom = \"point\", \n                    fun = \"mean\", \n                    color = \"black\",\n                    size = 3) + \n       scale_color_manual(values = palette_timepoint) + \n       labs(x = \"Age (PCW)\", y = \"Mean Psueodtime\") + \n       theme_scLANE()\np10\n\n\n\n\n\n\n\n\nFigure 6: Beeswarm plot displaying the distribution of mean-aggregated pseudotime for each timepoint"
  },
  {
    "objectID": "tutorials/Trajectory_GRN.html#trajectory-de-testing",
    "href": "tutorials/Trajectory_GRN.html#trajectory-de-testing",
    "title": "Building a Trajectory Regulatory Network with scRNA-seq Data",
    "section": "6.3 Trajectory DE testing",
    "text": "6.3 Trajectory DE testing\nUsing scLANE (see the GitHub repository or the preprint for details) we perform trajectory DE significance testing for each HVG across each lineage.\n\n\nCode\ntop3k_hvg &lt;- HVFInfo(seu_cortex) %&gt;% \n             arrange(desc(variance.standardized)) %&gt;% \n             slice_head(n = 3000) %&gt;% \n             rownames(.)\ncell_offset &lt;- createCellOffset(seu_cortex)\npt_df &lt;- select(sling_pt, -PT_Overall)\nscLANE_models &lt;- testDynamic(seu_cortex, \n                             pt = pt_df, \n                             genes = top3k_hvg,\n                             size.factor.offset = cell_offset, \n                             n.cores = 6L, \n                             verbose = FALSE)\n\n\nscLANE testing completed for 3000 genes across 3 lineages in 13.321 mins\n\n\nCode\nscLANE_de_res &lt;- getResultsDE(scLANE_models)\n\n\nWe now have lineage-specific trajectory DE statistics for each gene:\n\n\nCode\nselect(scLANE_de_res, \n       Gene, \n       Lineage, \n       Test_Stat, \n       P_Val, \n       P_Val_Adj,\n       Gene_Dynamic_Lineage, \n       Gene_Dynamic_Overall) %&gt;% \n  mutate(Gene_Dynamic_Lineage = if_else(Gene_Dynamic_Lineage == 1, \"Dynamic\", \"Static\"), \n         Gene_Dynamic_Overall = if_else(Gene_Dynamic_Overall == 1, \"Dynamic\", \"Static\")) %&gt;% \n  slice_head(n = 10) %&gt;% \n  kableExtra::kbl(digits = 4, \n                  booktabs = TRUE, \n                  col.names = c(\"Gene\", \"Lineage\", \"LRT stat.\", \"P-value\", \"Adj. p-value\", \"Gene status (lineage)\", \"Gene status (overall)\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nGene\nLineage\nLRT stat.\nP-value\nAdj. p-value\nGene status (lineage)\nGene status (overall)\n\n\n\n\nLMO3\nB\n4030239\n0\n0\nDynamic\nDynamic\n\n\nLMO3\nA\n3924999\n0\n0\nDynamic\nDynamic\n\n\nMEF2C\nB\n3650115\n0\n0\nDynamic\nDynamic\n\n\nLMO3\nC\n3439387\n0\n0\nDynamic\nDynamic\n\n\nLIMCH1\nB\n3100288\n0\n0\nDynamic\nDynamic\n\n\nLIMCH1\nA\n3097931\n0\n0\nDynamic\nDynamic\n\n\nLINC01102\nB\n2857935\n0\n0\nDynamic\nDynamic\n\n\nLINC01102\nA\n2699231\n0\n0\nDynamic\nDynamic\n\n\nSHISA2\nB\n2086787\n0\n0\nDynamic\nDynamic\n\n\nSHISA2\nA\n2046352\n0\n0\nDynamic\nDynamic\n\n\n\n\n\n\n\n\nTable 2: Top-10 most DE genes from scLANE\n\n\n\n\nWe identify a set of dynamic genes - dynamic meaning DE over any of the three lineages - to be used for downstream analysis.\n\n\nCode\ndyn_genes &lt;- filter(scLANE_de_res, Gene_Dynamic_Overall == 1) %&gt;% \n             distinct(Gene) %&gt;% \n             pull(Gene)\n\n\nLastly, we pull a table of gene dynamics for each lineage.\n\n\nCode\nsmoothed_counts &lt;- smoothedCountsMatrix(scLANE_models, \n                                        size.factor.offset = cell_offset, \n                                        pt = pt_df, \n                                        genes = dyn_genes, \n                                        log1p.norm = TRUE, \n                                        n.cores = 4L)"
  },
  {
    "objectID": "tutorials/Trajectory_GRN.html#constructing-a-grn-from-scratch",
    "href": "tutorials/Trajectory_GRN.html#constructing-a-grn-from-scratch",
    "title": "Building a Trajectory Regulatory Network with scRNA-seq Data",
    "section": "6.4 Constructing a GRN from scratch",
    "text": "6.4 Constructing a GRN from scratch\nBuilding a GRN requires two main inputs - a set of transcription factors, and a set of potential target genes.\n\n\nCode\nhs_ensembl &lt;- useMart(\"ensembl\", dataset = \"hsapiens_gene_ensembl\")\nhs_tf_raw &lt;- readr::read_csv(\"http://humantfs.ccbr.utoronto.ca/download/v_1.01/DatabaseExtract_v_1.01.csv\",\n                             col_select = -1,\n                             num_threads = 2L,\n                             show_col_types = FALSE) %&gt;%\n             janitor::clean_names() %&gt;%\n             filter(is_tf == \"Yes\") %&gt;%\n             select(ensembl_id)\nhs_tfs &lt;- getBM(attributes = c(\"ensembl_gene_id\", \"hgnc_symbol\", \"entrezgene_id\", \"description\", \"gene_biotype\"),\n                filters = \"ensembl_gene_id\",\n                values = hs_tf_raw$ensembl_id,\n                mart = hs_ensembl,\n                uniqueRows = TRUE) %&gt;%\n          rename(ensembl_id = ensembl_gene_id,\n                 entrez_id = entrezgene_id) %&gt;%\n          arrange(ensembl_id) %&gt;%\n          mutate(hgnc_symbol = if_else(hgnc_symbol == \"\", NA_character_, hgnc_symbol),\n                 description = gsub(\"\\\\[Source.*\", \"\", description))\n\n\nAlong with the HGNC symbol, we used BiomaRt to retrieve the Ensembl & Entrez IDs and a description for every TF.\n\n\nCode\nslice_sample(hs_tfs, n = 10) %&gt;% \n  kableExtra::kbl(booktabs = TRUE, \n                  col.names = c(\"Ensembl ID\", \"HGNC symbol\", \"Entrez ID\", \"Description\", \"Biotype\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nEnsembl ID\nHGNC symbol\nEntrez ID\nDescription\nBiotype\n\n\n\n\nENSG00000173041\nZNF680\n340252\nzinc finger protein 680\nprotein_coding\n\n\nENSG00000149922\nTBX6\n6911\nT-box transcription factor 6\nprotein_coding\n\n\nENSG00000085274\nMYNN\n55892\nmyoneurin\nprotein_coding\n\n\nENSG00000169740\nZNF32\n7580\nzinc finger protein 32\nprotein_coding\n\n\nENSG00000214534\nZNF705EP\nNA\nzinc finger protein 705E, pseudogene\ntranscribed_unprocessed_pseudogene\n\n\nENSG00000129071\nMBD4\n8930\nmethyl-CpG binding domain 4, DNA glycosylase\nprotein_coding\n\n\nENSG00000114853\nZBTB47\n92999\nzinc finger and BTB domain containing 47\nprotein_coding\n\n\nENSG00000111206\nFOXM1\n2305\nforkhead box M1\nprotein_coding\n\n\nENSG00000184436\nTHAP7\n80764\nTHAP domain containing 7\nprotein_coding\n\n\nENSG00000196172\nZNF681\n148213\nzinc finger protein 681\nprotein_coding\n\n\n\n\n\n\n\n\nTable 3: Human transcription factors\n\n\n\n\nWe split our set of 295 trajectory DE genes into the two categories using the existing set of human TFs I found online.\n\n\nCode\nvalid_hs_tfs &lt;- hs_tfs$hgnc_symbol[hs_tfs$hgnc_symbol %in% dyn_genes]\ntarget_genes &lt;- dyn_genes[!dyn_genes %in% valid_hs_tfs]\n\n\nThe GRN has the following structure - for every target gene, a penalized model is fit with the dynamics of every possible TF as features (see below). The model’s internal feature selection identifies a subset of TFs whose dynamics are predictive of the target gene’s dynamics. See e.g., the original XGBoost paper for details.\n\\[\n\\text{Dynamics}_{\\text{Target}} = \\begin{bmatrix} \\text{Dynamics}_{\\text{TF}_1} & \\text{Dynamics}_{\\text{TF}_2} & \\dots & \\text{Dynamics}_{\\text{TF}_N} \\end{bmatrix}\n\\tag{1}\\]\nOur GRNs will be lineage-specific, since the matrix of gene dynamics differs by lineage. First we fit the GRN for lineage A; I’ve opted to use the LightGBM package to fit the models as it’s lightweight and in general much faster than other options such as XGBoost. In addition, I prefer this method to alternative penalized methods such as LASSO - boosted trees better capture nonlinearities in the data, though LASSO does offer advantages such as significance testing for coefficients and linear interpretability. Both methods have built-in cross-validation (CV) routines (lightgbm::lgb.cv() and glmnet::cv.glmnet(), respectively); here we use 5-fold CV to select the best set of hyperparameters before training the final model.\n\n\n\n\n\n\nNote\n\n\n\nSince we’re using the foreach and doSNOW packages to loop over the set of target genes in parallel, we need to ensure that our LightGBM model only uses a single thread when fitting the model. This is done by setting the parameters tree_learner = \"serial\" and num_threads = 1L in the model parameters list.\n\n\n\n\nCode\ncl &lt;- parallel::makeCluster(6L)\ndoSNOW::registerDoSNOW(cl)\nlgbm_params &lt;- list(objective = \"gamma\", \n                    tree_learner = \"serial\", \n                    metric = \"l2\", \n                    boosting_type = \"gbdt\", \n                    num_threads = 1L, \n                    seed = 312)\ngrn_res_linA &lt;- foreach(g = seq(target_genes), \n                        .combine = \"list\",\n                        .multicombine = ifelse(length(target_genes) &gt; 1, TRUE, FALSE),\n                        .maxcombine = ifelse(length(target_genes) &gt; 1, length(target_genes), 2),\n                        .packages = c(\"lightgbm\", \"dplyr\"),\n                        .errorhandling = \"pass\",\n                        .inorder = TRUE,\n                        .verbose = FALSE) %dopar% {\n                          feature_mat &lt;- smoothed_counts$Lineage_A[, valid_hs_tfs]\n                          resp_var &lt;- smoothed_counts$Lineage_A[, target_genes[g]]\n                          lgbm_data &lt;- lightgbm::lgb.Dataset(data = feature_mat, label = resp_var)\n                          lgbm_cv &lt;- lightgbm::lgb.cv(params = lgbm_params, \n                                                      data = lgbm_data, \n                                                      nrounds = 100L, \n                                                      nfold = 5L, \n                                                      stratified = FALSE)\n                          lgbm_model &lt;- lightgbm::lgb.train(params = lgbm_params, \n                                                            data = lgbm_data, \n                                                            nrounds = lgbm_cv$best_iter)\n                          imp_table &lt;- as.data.frame(lightgbm::lgb.importance(lgbm_model)) %&gt;% \n                                       dplyr::mutate(Lineage = \"A\", \n                                                     Target_Gene = target_genes[g], \n                                                     .before = 1)\n                          imp_table\n                        }\nnames(grn_res_linA) &lt;- target_genes\ngrn_table_linA &lt;- purrr::imap(grn_res_linA, \\(x, y) {\n  if (!inherits(x, \"data.frame\")) {\n    empty_res &lt;- data.frame(Lineage = \"A\", \n                            Target_Gene = y, \n                            Feature = NA_character_, \n                            Gain = NA_real_, \n                            Cover = NA_real_, \n                            Frequency = NA_real_)\n    return(empty_res)\n  } else {\n    return(x)\n  }\n})\n\n\nWe coerce the model output into a tidy table.\n\n\nCode\ngrn_table_linA &lt;- purrr::reduce(grn_table_linA, rbind) %&gt;% \n                  rename(Tx_Factor = Feature) %&gt;% \n                  filter(!is.na(Target_Gene), \n                         !is.na(Tx_Factor)) %&gt;% \n                  arrange(Tx_Factor, desc(Frequency))\n\n\nNext we add the Spearman correlation between TF and target gene dynamics, as well as the normalized gene expression correlation. We denote the relationship between TF and target gene as repression if the correlation between their dynamics is negative, and activation if the correlation is positive.\n\n\n\n\n\n\nWarning\n\n\n\nUsually the correlations between TF and target dynamics and TF and target expression are similar, or at least have the same directionality (positive or negative). However, when expression patterns are noisy, the correlation directions might differ (see Table 4 for examples). In this case, we’re opting to trust the dynamics more, as they are a smoothed version of expression - though an argument could be made for the opposite stance as well.\n\n\n\n\nCode\ndyn_cormat &lt;- cor(smoothed_counts$Lineage_A, method = \"spearman\")\nexp_cormat &lt;- cor(as.matrix(Matrix::t(seu_cortex@assays$RNA$data[dyn_genes, ])), method = \"spearman\")\ngrn_table_linA &lt;- mutate(grn_table_linA, \n                         Dyn_Cor = NA_real_, \n                         Exp_Cor = NA_real_)\nfor (i in seq(nrow(grn_table_linA))) {\n  grn_table_linA$Dyn_Cor[i] &lt;- dyn_cormat[grn_table_linA$Target_Gene[i], grn_table_linA$Tx_Factor[i]]\n  grn_table_linA$Exp_Cor[i] &lt;- exp_cormat[grn_table_linA$Target_Gene[i], grn_table_linA$Tx_Factor[i]]\n}\ngrn_table_linA &lt;- mutate(grn_table_linA, State = if_else(Dyn_Cor &lt; 0, \"Repression\", \"Activation\"))\n\n\nThe output is shown below. Here we use the frequency with which the TF is included in the final boosted tree model as the feature importance, which can be interpreted as the strength of the predictive relationship between TF & target gene. See e.g., this StackExchange post for more detail.\n\n\nCode\nselect(grn_table_linA, \n       Lineage, \n       Tx_Factor, \n       Target_Gene, \n       Frequency, \n       Dyn_Cor, \n       Exp_Cor, \n       State) %&gt;% \n  slice_sample(n = 10) %&gt;% \n  kableExtra::kbl(digits = 4, \n                  booktabs = TRUE, \n                  col.names = c(\"Lineage\", \"TF\", \"Target\", \"Importance\", \"Dynamics cor.\", \"Expression cor.\", \"State\")) %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, \"hover\")\n\n\n\n\n\n\n\n\nLineage\nTF\nTarget\nImportance\nDynamics cor.\nExpression cor.\nState\n\n\n\n\nA\nTSHZ2\nKLHL14\n0.0333\n0.3740\n0.0356\nActivation\n\n\nA\nCXXC4\nPEX5L\n0.0007\n0.0926\n0.0962\nActivation\n\n\nA\nEOMES\nMDM4\n0.0170\n0.6390\n-0.0034\nActivation\n\n\nA\nCUX2\nRP11.335E14.1\n0.0490\n0.5553\n-0.0111\nActivation\n\n\nA\nFOXP2\nLGI1\n0.0133\n-0.4933\n0.0552\nRepression\n\n\nA\nTGIF2\nPTPRR\n0.0540\n-0.0378\n-0.0173\nRepression\n\n\nA\nFOXP2\nRP11.1112C15.2\n0.0260\n-0.7971\n0.0502\nRepression\n\n\nA\nCREB5\nNTRK2\n0.0566\n-0.6718\n-0.0825\nRepression\n\n\nA\nEOMES\nUNC5D\n0.1240\n0.9615\n0.1371\nActivation\n\n\nA\nDLX5\nAASS\n0.0723\n-0.3497\n-0.0159\nRepression\n\n\n\n\n\n\n\n\nTable 4: GRN for dynamic genes across Lineage A\n\n\n\n\nWe repeat the fitting process for the other two lineages (expand this code block to see the details). Skip to Section 7 if you want a version of this logic that’s been wrapped into a function!\n\n\nCode\ngrn_res_linB &lt;- foreach(g = seq(target_genes), \n                        .combine = \"list\",\n                        .multicombine = ifelse(length(target_genes) &gt; 1, TRUE, FALSE),\n                        .maxcombine = ifelse(length(target_genes) &gt; 1, length(target_genes), 2),\n                        .packages = c(\"lightgbm\", \"dplyr\"),\n                        .errorhandling = \"pass\",\n                        .inorder = TRUE,\n                        .verbose = FALSE) %dopar% {\n                          feature_mat &lt;- smoothed_counts$Lineage_B[, valid_hs_tfs]\n                          resp_var &lt;- smoothed_counts$Lineage_B[, target_genes[g]]\n                          lgbm_data &lt;- lightgbm::lgb.Dataset(data = feature_mat, label = resp_var)\n                          lgbm_cv &lt;- lightgbm::lgb.cv(params = lgbm_params, \n                                                      data = lgbm_data, \n                                                      nrounds = 100L, \n                                                      nfold = 5L, \n                                                      stratified = FALSE)\n                          lgbm_model &lt;- lightgbm::lgb.train(params = lgbm_params, \n                                                            data = lgbm_data, \n                                                            nrounds = lgbm_cv$best_iter)\n                          imp_table &lt;- as.data.frame(lightgbm::lgb.importance(lgbm_model)) %&gt;% \n                                       dplyr::mutate(Lineage = \"B\", \n                                                     Target_Gene = target_genes[g], \n                                                     .before = 1)\n                          imp_table\n                        }\nnames(grn_res_linB) &lt;- target_genes\ngrn_table_linB &lt;- purrr::imap(grn_res_linB, \\(x, y) {\n  if (!inherits(x, \"data.frame\")) {\n    empty_res &lt;- data.frame(Lineage = \"B\", \n                            Target_Gene = y, \n                            Feature = NA_character_, \n                            Gain = NA_real_, \n                            Cover = NA_real_, \n                            Frequency = NA_real_)\n    return(empty_res)\n  } else {\n    return(x)\n  }\n})\ngrn_table_linB &lt;- purrr::reduce(grn_table_linB, rbind) %&gt;% \n                  rename(Tx_Factor = Feature) %&gt;% \n                  filter(!is.na(Target_Gene), \n                         !is.na(Tx_Factor)) %&gt;% \n                  arrange(Tx_Factor, desc(Frequency))\ndyn_cormat &lt;- cor(smoothed_counts$Lineage_B, method = \"spearman\")\ngrn_table_linB &lt;- mutate(grn_table_linB, \n                         Dyn_Cor = NA_real_, \n                         Exp_Cor = NA_real_)\nfor (i in seq(nrow(grn_table_linB))) {\n  grn_table_linB$Dyn_Cor[i] &lt;- dyn_cormat[grn_table_linB$Target_Gene[i], grn_table_linB$Tx_Factor[i]]\n  grn_table_linB$Exp_Cor[i] &lt;- exp_cormat[grn_table_linB$Target_Gene[i], grn_table_linB$Tx_Factor[i]]\n}\ngrn_table_linB &lt;- mutate(grn_table_linB, State = if_else(Dyn_Cor &lt; 0, \"Repression\", \"Activation\"))\ngrn_res_linC &lt;- foreach(g = seq(target_genes), \n                        .combine = \"list\",\n                        .multicombine = ifelse(length(target_genes) &gt; 1, TRUE, FALSE),\n                        .maxcombine = ifelse(length(target_genes) &gt; 1, length(target_genes), 2),\n                        .packages = c(\"lightgbm\", \"dplyr\"),\n                        .errorhandling = \"pass\",\n                        .inorder = TRUE,\n                        .verbose = FALSE) %dopar% {\n                          feature_mat &lt;- smoothed_counts$Lineage_C[, valid_hs_tfs]\n                          resp_var &lt;- smoothed_counts$Lineage_C[, target_genes[g]]\n                          lgbm_data &lt;- lightgbm::lgb.Dataset(data = feature_mat, label = resp_var)\n                          lgbm_cv &lt;- lightgbm::lgb.cv(params = lgbm_params, \n                                                      data = lgbm_data, \n                                                      nrounds = 100L, \n                                                      nfold = 5L, \n                                                      stratified = FALSE)\n                          lgbm_model &lt;- lightgbm::lgb.train(params = lgbm_params, \n                                                            data = lgbm_data, \n                                                            nrounds = lgbm_cv$best_iter)\n                          imp_table &lt;- as.data.frame(lightgbm::lgb.importance(lgbm_model)) %&gt;% \n                                       dplyr::mutate(Lineage = \"C\", \n                                                     Target_Gene = target_genes[g], \n                                                     .before = 1)\n                          imp_table\n                        }\nnames(grn_res_linC) &lt;- target_genes\ngrn_table_linC &lt;- purrr::imap(grn_res_linC, \\(x, y) {\n  if (!inherits(x, \"data.frame\")) {\n    empty_res &lt;- data.frame(Lineage = \"C\", \n                            Target_Gene = y, \n                            Feature = NA_character_, \n                            Gain = NA_real_, \n                            Cover = NA_real_, \n                            Frequency = NA_real_)\n    return(empty_res)\n  } else {\n    return(x)\n  }\n})\ngrn_table_linC &lt;- purrr::reduce(grn_table_linC, rbind) %&gt;% \n                  rename(Tx_Factor = Feature) %&gt;% \n                  filter(!is.na(Target_Gene), \n                         !is.na(Tx_Factor)) %&gt;% \n                  arrange(Tx_Factor, desc(Frequency))\ndyn_cormat &lt;- cor(smoothed_counts$Lineage_C, method = \"spearman\")\ngrn_table_linC &lt;- mutate(grn_table_linC, \n                         Dyn_Cor = NA_real_, \n                         Exp_Cor = NA_real_)\nfor (i in seq(nrow(grn_table_linC))) {\n  grn_table_linC$Dyn_Cor[i] &lt;- dyn_cormat[grn_table_linC$Target_Gene[i], grn_table_linC$Tx_Factor[i]]\n  grn_table_linC$Exp_Cor[i] &lt;- exp_cormat[grn_table_linC$Target_Gene[i], grn_table_linC$Tx_Factor[i]]\n}\ngrn_table_linC &lt;- mutate(grn_table_linC, State = if_else(Dyn_Cor &lt; 0, \"Repression\", \"Activation\"))\nparallel::stopCluster(cl)\n\n\nWe collate all three GRNs into a single overall table:\n\n\nCode\ngrn_table_all &lt;- bind_rows(grn_table_linA, \n                           grn_table_linB, \n                           grn_table_linC) %&gt;% \n                 arrange(Tx_Factor, Target_Gene)\n\n\nFor each lineage we create a directed, weighted graph of the relationships between TFs and target genes using the igraph package.\n\n\nCode\ngraph_df_linA &lt;- select(grn_table_linA, \n                        Tx_Factor, \n                        Target_Gene,\n                        Frequency, \n                        State) %&gt;% \n                 arrange(Tx_Factor, desc(Frequency)) %&gt;% \n                 with_groups(Tx_Factor,\n                             slice_head, \n                             n = 10)\ngraph_obj_linA &lt;- graph.data.frame(graph_df_linA, directed = TRUE)\ngraph_df_linB &lt;- select(grn_table_linB, \n                        Tx_Factor, \n                        Target_Gene,\n                        Frequency, \n                        State) %&gt;% \n                 arrange(Tx_Factor, desc(Frequency)) %&gt;% \n                 with_groups(Tx_Factor,\n                             slice_head, \n                             n = 10)\ngraph_obj_linB &lt;- graph.data.frame(graph_df_linB, directed = TRUE)\ngraph_df_linC &lt;- select(grn_table_linC, \n                        Tx_Factor, \n                        Target_Gene,\n                        Frequency, \n                        State) %&gt;% \n                 arrange(Tx_Factor, desc(Frequency)) %&gt;% \n                 with_groups(Tx_Factor,\n                             slice_head, \n                             n = 10)\ngraph_obj_linC &lt;- graph.data.frame(graph_df_linC, directed = TRUE)\n\n\nOne TF of particular interest is SRY-box transcription factor 4 (SOX4). This TF helps to regulate development and is involved in the specification of cell fate, so a comparison of its behavior across lineages should be pretty neat. We use a Fruchterman-Reingold layout to embed the graphs in 2D space via the layout_with_fr() function from igraph. We see that not only do the top target genes differ widely by lineage, so too do the directions of the relationships (activation vs. repression), and the strengths of those relationships. For example, in lineage A the top target gene is BACE2, whereas for lineage B it’s LINC01268.\n\n\nCode\nset.seed(1)\nfreqs &lt;- purrr::map(list(graph_df_linA, graph_df_linB, graph_df_linC), \\(x) {\n  filter(x, Tx_Factor == \"SOX4\") %&gt;% \n  pull(Frequency)\n})\nfreq_min &lt;- min(purrr::reduce(freqs, c))\nfreq_max &lt;- max(purrr::reduce(freqs, c))\np11 &lt;- fortify(graph_obj_linA, \n               layout = layout_with_fr(graph = graph_obj_linA, \n                                       grid = \"nogrid\", \n                                       weights = E(graph_obj_linA)$weights^3),\n               arrow.gap = .02) %&gt;% \n       mutate(Gene_Type = if_else(name %in% hs_tfs$hgnc_symbol, \"TF\", \"Target Gene\")) %&gt;% \n       filter(name %in% (filter(graph_df_linA, Tx_Factor == \"SOX4\") %&gt;% pull(Target_Gene)) | name == \"SOX4\") %&gt;% \n       ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + \n       facet_wrap(~\"Lineage A\") + \n       geom_edges(aes(linewidth = Frequency, alpha = Frequency, color = State), \n                  arrow = arrow(length = unit(8, \"pt\"), type = \"closed\")) + \n       geom_nodelabel(aes(label = name, fill = Gene_Type), \n                      size = 5) + \n       scale_color_manual(values = c(\"forestgreen\", \"firebrick\")) + \n       scale_fill_manual(values = c(\"darkorange2\", \"dodgerblue3\")) + \n       scale_linewidth_continuous(range = c(0.5, 1.5), limits = c(freq_min, freq_max)) + \n       scale_alpha_continuous(range = c(0.75, 1), limits = c(freq_min, freq_max)) + \n       labs(x = \"FR 1\", \n            y = \"FR 2\", \n            color = \"TF State\", \n            fill = \"Gene Type\") + \n       theme_scLANE(umap = TRUE) + \n       guides(fill = guide_legend(override.aes = list(label = \"\"), order = 1), \n              color = guide_legend(order = 2))\np12 &lt;- fortify(graph_obj_linB, \n               layout = layout_with_fr(graph = graph_obj_linB, \n                                       grid = \"nogrid\", \n                                       weights = E(graph_obj_linB)$weights^3),\n               arrow.gap = .02) %&gt;% \n       mutate(Gene_Type = if_else(name %in% hs_tfs$hgnc_symbol, \"TF\", \"Target Gene\")) %&gt;% \n       filter(name %in% (filter(graph_df_linB, Tx_Factor == \"SOX4\") %&gt;% pull(Target_Gene)) | name == \"SOX4\") %&gt;% \n       ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + \n       facet_wrap(~\"Lineage B\") + \n       geom_edges(aes(linewidth = Frequency, alpha = Frequency, color = State), \n                  arrow = arrow(length = unit(8, \"pt\"), type = \"closed\")) + \n       geom_nodelabel(aes(label = name, fill = Gene_Type), \n                      size = 5) + \n       scale_color_manual(values = c(\"forestgreen\", \"firebrick\")) + \n       scale_fill_manual(values = c(\"darkorange2\", \"dodgerblue3\")) + \n       scale_linewidth_continuous(range = c(0.5, 1.5), limits = c(freq_min, freq_max)) + \n       scale_alpha_continuous(range = c(0.75, 1), limits = c(freq_min, freq_max)) + \n       labs(x = \"FR 1\", \n            y = \"FR 2\", \n            color = \"TF State\", \n            fill = \"Gene Type\") + \n       theme_scLANE(umap = TRUE) + \n       guides(fill = guide_legend(override.aes = list(label = \"\"), order = 1), \n              color = guide_legend(order = 2))\np13 &lt;- fortify(graph_obj_linC, \n               layout = layout_with_fr(graph = graph_obj_linC, \n                                       grid = \"nogrid\", \n                                       weights = E(graph_obj_linC)$weights^3),\n               arrow.gap = .02) %&gt;% \n       mutate(Gene_Type = if_else(name %in% hs_tfs$hgnc_symbol, \"TF\", \"Target Gene\")) %&gt;% \n       filter(name %in% (filter(graph_df_linC, Tx_Factor == \"SOX4\") %&gt;% pull(Target_Gene)) | name == \"SOX4\") %&gt;% \n       ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + \n       facet_wrap(~\"Lineage C\") + \n       geom_edges(aes(linewidth = Frequency, alpha = Frequency, color = State), \n                  arrow = arrow(length = unit(8, \"pt\"), type = \"closed\")) + \n       geom_nodelabel(aes(label = name, fill = Gene_Type), \n                      size = 5) + \n       scale_color_manual(values = c(\"forestgreen\", \"firebrick\")) + \n       scale_fill_manual(values = c(\"darkorange2\", \"dodgerblue3\")) + \n       scale_linewidth_continuous(range = c(0.5, 1.5), limits = c(freq_min, freq_max)) + \n       scale_alpha_continuous(range = c(0.75, 1), limits = c(freq_min, freq_max)) + \n       labs(x = \"FR 1\", \n            y = \"FR 2\", \n            color = \"TF State\", \n            fill = \"Gene Type\") + \n       theme_scLANE(umap = TRUE) + \n       guides(fill = guide_legend(override.aes = list(label = \"\"), order = 1), \n              color = guide_legend(order = 2))\np14 &lt;- (p11 / p12 / p13) + plot_layout(guides = \"collect\")\np14\n\n\n\n\n\n\n\n\nFigure 7: Fruchterman-Reingold embeddings of the relationships between SRY-box transcription factor 4 and lineage-specific target genes"
  }
]