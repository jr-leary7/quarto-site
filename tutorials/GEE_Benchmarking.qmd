---
title: "Benchmarking Negative Binomial GEE Model Backends"
author:
  name: Jack Leary
  email: j.leary@ufl.edu
  affiliations:
    - name: University of Florida
      department: Biostatistics 
      city: Gainesville
      state: FL
date: "`r Sys.Date()`"
format:
  html:
    code-fold: show
    code-copy: true
    code-tools: true
    toc: true
    embed-resources: true
    fig-format: retina
    df-print: kable
    link-external-newwindow: true
execute:
  cache: true
  freeze: auto
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = NA)
```

# Introduction 

In the course of my recent work on [trajectory differential expression](https://jr-leary7.github.io/quarto-site/tutorials/scLANE_Trajectory_DE.html), I've spent a fair amount of time fitting [generalized estimating equation](https://online.stat.psu.edu/stat504/lesson/12/12.1) (GEE) models. These models are like classical GLMs in that they can handle non-normally distributed response variables via some specified link function & iterative estimation of coefficients, but they differ in that they can account for the variation inherent to longitudinal or otherwise clustered datasets in which observations are measured repeatedly from multiple subjects & are thus not independent. Since scRNA-seq experiments are almost always composed of samples from multiple subjects nowadays, GEEs can be of great use when building models of gene expression, as they allow us to be more confident that our standard errors are accurate. Note that unlike (generalized) linear mixed models, the GEE is a *marginal* model and thus provides only a population-level fit - GLMMs are conditional models as they provide subject-specific fits. 

Since [single cell mRNA abundance follows the negative binomial distribution](https://doi.org/10.1186/s13059-021-02584-9), it's necessary that whatever GEE fitting algorithm we use support that distribution. Since the negative binomial is a two-parameter distribution, this does complicate things; the most commonly-used R packages for GEEs do not support NB models. We do have a couple options in R though, and the Python `statsmodels` ecosystem supports NB GEEs as well. Our goal here will be to benchmark the available options in terms of runtime, memory usage, and model accuracy in terms of the fitted values & the estimated coefficients. We'll keep our simulated data simple, with a negative binomial response $Y$ and a single continuously-valued covariate $X$; thus we'll estimate the coefficient vector $\boldsymbol{\beta} = [\beta_0, \beta_1]^T$ using each method for each set of simulations. 

# Libraries 

## R 

```{r, results='hide'}
library(dplyr)       # data manipulation
library(ggplot2)     # plots 
library(reticulate)  # call Python from R
```

## Python

We'll call the Python code from R using `{reticulate}`, but we'll need to make sure to use the virtual environment I set up previously that has the `statsmodels` and `pandas` libraries (and their various dependencies) installed. 

```{r, results='hide'}
use_virtualenv("~/Desktop/Python/science/venv/", required = TRUE)
sm <- import("statsmodels.api")
pd <- import("pandas")
```

# Data 

Let's first simulate an example dataset to show what each further simulation should look like. Our parameters are generated from the following distributions (note that we use the mean & dispersion parameterization of the negative binomial): 

$$
\begin{aligned}
  \boldsymbol{\beta} &\sim \mathcal{N}(\mu = 0,\: \sigma^2 = 0.25) \\
  X &\sim \mathcal{N}(\mu = 0,\: \sigma^2 = 1) \\
  Y &\sim \text{NB}(\mu = e^{\beta_0 + X\beta_1},\: \theta = 3)
\end{aligned}
$$

We'll generate $n = 25$ samples to start, though our actual sample size in the simulations will be larger than this. We add a uniformly-sampled dropout with a rate of 10% to our response $Y$ to mimic the technical variation inherent to scRNA-seq experiments. Lastly, I'll note that we're making this example slightly less complex by not including multiple observations per subject, which we will do in our actual experiment. 

```{r}
set.seed(312)
sim_beta <- rnorm(2, mean = 0, sd = 0.5)
sim_df <- data.frame(X = rnorm(25, mean = 0, sd = 1)) %>% 
          mutate(Beta_X = sim_beta[1] + sim_beta[2] * X, 
                 Exp_Beta_X = exp(Beta_X)) %>% 
          rowwise() %>% 
          mutate(Y = rnbinom(1, mu = Exp_Beta_X, size = 3)) %>% 
          ungroup()
dropout_idx <- sample(seq(nrow(sim_df)), size = round(0.1 * nrow(sim_df)))
sim_df$Y[dropout_idx] <- 0
```

Next, since we're dealing with independent observations we fit an NB GLM via the `{MASS}` package which we'll use to generate fitted values and standard errors. 

```{r}
nb_glm <- MASS::glm.nb(Y ~ X, 
                       data = sim_df, 
                       link = "log")
```

Let's visualize the results. We see that the confidence interval around the fitted mean widens as $X$ and $Y$ increase. This data looks similar enough to what I see in real scRNA-seq studies, so I'm satisfied with using the above simulation scheme for a more rigorous experiment. 

```{r}
sim_df %>% 
  mutate(glm_pred = predict(nb_glm), 
         glm_pred_se = predict(nb_glm, se.fit = TRUE)$se.fit, 
         glm_fit = exp(glm_pred), 
         glm_fit_ci_lo = exp(glm_pred - 1.96 * glm_pred_se), 
         glm_fit_ci_hi = exp(glm_pred + 1.96 * glm_pred_se)) %>% 
  ggplot(aes(x = X, y = Y)) + 
  geom_point(size = 3) + 
  geom_line(aes(x = X, y = glm_fit), 
            color = "forestgreen", 
            size = 1) + 
  geom_ribbon(aes(x = X, ymin = glm_fit_ci_lo, ymax = glm_fit_ci_hi, color = NULL), 
              fill = "forestgreen", 
              alpha = 0.4) + 
  labs(x = "X", 
       y = "Y", 
       title = "Simulated Negative Binomial mRNA Counts", 
       subtitle = "Fitted values from NB GLM with Wald 95% C.I.") + 
  theme_classic(base_size = 14) + 
  theme(plot.subtitle = element_text(face = "italic", size = 12))
```

# Simulations 

## Helper Functions 

Using the same general simulation procedure from the example above, we'll write a function to simulate negative binomial data suitable for using GEEs. We'll generate multiple observations per subject, though the resulting data will be in the same format, albeit with an added subject ID column. We'll keep things simple by allocating our total sample size evenly across subjects & keeping $\boldsymbol{\beta}$ constant across all subjects. 

```{r}
sim_gee_data <- function(n.subjects = NULL, 
                         n.per.subject = NULL, 
                         dropout.rate = 0.1, 
                         theta.y = 3) {
  # simulate true coefficients
  sim_beta <- rnorm(2, mean = 0, sd = 0.5)
  # simulate X & Y per subject
  subject_names <- paste0("S", seq(n.subjects))
  subject_sims <- purrr::map(subject_names, function(s) {
    sim_df <- data.frame(ID = s, 
                         X = rnorm(n.per.subject, mean = 0, sd = 1)) %>% 
              mutate(Beta_X = sim_beta[1] + sim_beta[2] * X, 
                     Exp_Beta_X = exp(Beta_X)) %>% 
              rowwise() %>% 
              mutate(Y = rnbinom(1, mu = Exp_Beta_X, size = theta.y)) %>% 
              ungroup()
  })
  subject_sim_df <- purrr::reduce(subject_sims, rbind) %>% 
                    mutate(ID = as.factor(ID))
  # add stochastic dropout
  dropout_idx <- sample(seq(nrow(subject_sim_df)), size = round(dropout.rate * nrow(subject_sim_df)))
  subject_sim_df$Y[dropout_idx] <- 0
  res_list <- list(beta = sim_beta, 
                   sim_df = subject_sim_df)
  return(res_list)
}
```

Next, we'll write a function to run all of our different model types. If we use the R implementations of the GEE framework, we need to provide an estimate for $\theta$, since the estimation procedure depends on $\theta$ being "known". To approximate this, we provide the value of $\theta$ estimated using the intercept-only model, which in my experience has been a good enough approximation for simpler datasets like this one. 

```{r, results='hide'}
run_model <- function(sim.data = NULL, model.type = NULL) {
  # run correct model framework
  if (model.type == "NB GLM") {
    start_time <- Sys.time()
    model_fit <- MASS::glm.nb(Y ~ X, 
                              data = sim.data, 
                              link = "log")
    diff_time <- Sys.time() - start_time
    model_preds <- predict(model_fit, type = "link")
    model_coef <- coef(model_fit)
    model_est_alpha <- NA_real_
  } else if (model.type == "geeM") {
    start_time <- Sys.time()
    theta_hat <- MASS::theta.mm(y = sim.data$Y,
                                mu = mean(sim.data$Y),
                                dfr = nrow(sim.data) - 1)
    model_fit <- geeM::geem(Y ~ X, 
                            id = sim.data$ID, 
                            data = sim.data, 
                            family = MASS::negative.binomial(theta = theta_hat, link = "log"), 
                            corstr = "exchangeable", 
                            sandwich = TRUE)
    diff_time <- Sys.time() - start_time
    model_preds <- predict(model_fit, type = "link")
    model_coef <- coef(model_fit)
    model_est_alpha <- model_fit$alpha
  } else if (model.type == "mmmgee") {
    theta_hat <- MASS::theta.mm(y = sim.data$Y,
                                mu = mean(sim.data$Y),
                                dfr = nrow(sim.data) - 1)
    start_time <- Sys.time()
    model_fit <- mmmgee::geem2(Y ~ X, 
                               id = sim.data$ID, 
                               data = sim.data, 
                               family = MASS::negative.binomial(theta = theta_hat, link = "log"), 
                               corstr = "exchangeable", 
                               sandwich = TRUE)
    diff_time <- Sys.time() - start_time
    model_preds <- predict(model_fit, type = "link")
    model_coef <- coef(model_fit)
    model_est_alpha <- model_fit$alpha
  } else if (model.type == "statsmodels") {
    start_time <- Sys.time()
    nb_family <- sm$families$NegativeBinomial(link = sm$genmod$families$links$log)
    cor_structure <- sm$cov_struct$Exchangeable()
    py_gee <- sm$GEE$from_formula("Y ~ X", 
                                  "ID", 
                                  data = sim.data, 
                                  cov_struct = cor_structure, 
                                  family = nb_family)
    model_fit <- py_gee$fit()
    diff_time <- Sys.time() - start_time
    model_preds <- model_fit$get_prediction()$linpred$predicted_mean
    model_coef <- model_fit$params
    model_est_alpha <- model_fit$cov_struct$dep_params
  }
  # format results & compute model error
  names(model_coef) <- c("B0", "B1")
  model_rmse <- yardstick::rmse_vec(truth = sim.data$Y, 
                                    estimate = exp(model_preds))
  model_huber_loss <- yardstick::huber_loss_vec(truth = sim.data$Y, 
                                                estimate = exp(model_preds), 
                                                delta = 2)
  res_list <- list(model_type = model.type, 
                   model_runtime = as.numeric(diff_time), 
                   model_runtime_units = attributes(diff_time)$units, 
                   model_rmse = model_rmse, 
                   model_huber_loss = model_huber_loss, 
                   model_beta = model_coef, 
                   model_alpha = model_est_alpha)
  return(res_list)
}
```

## Running the Experiment

We fit each model over 100 iterations, and save the results in a list (`{purrr}` is great for this kind of thing). 

```{r}
sim_list <- purrr::map(seq(100), function(i) {
  set.seed(i)
  sim_data <- sim_gee_data(n.subjects = 3, 
                           n.per.subject = 400, 
                           dropout.rate = 0.1, 
                           theta.y = 5)
  glm_res <- run_model(sim.data = sim_data$sim_df, model.type = "NB GLM")
  geeM_res <- run_model(sim.data = sim_data$sim_df, model.type = "geeM")
  mmmgee_res <- run_model(sim.data = sim_data$sim_df, model.type = "mmmgee")
  statsmodels_res <- run_model(sim.data = sim_data$sim_df, model.type = "statsmodels")
  overall_res_df <- data.frame(Iter = i, 
                               Beta_0 = sim_data$beta[1], 
                               Beta_1 = sim_data$beta[2], 
                               Method = c(glm_res$model_type, geeM_res$model_type, mmmgee_res$model_type, statsmodels_res$model_type), 
                               Est_Beta0 = c(glm_res$model_beta[1], geeM_res$model_beta[1], mmmgee_res$model_beta[1], statsmodels_res$model_beta[1]), 
                               Est_Beta1 = c(glm_res$model_beta[2], geeM_res$model_beta[2], mmmgee_res$model_beta[2], statsmodels_res$model_beta[2]), 
                               Est_Alpha = c(glm_res$model_alpha, geeM_res$model_alpha, mmmgee_res$model_alpha, statsmodels_res$model_alpha), 
                               RMSE = c(glm_res$model_rmse, geeM_res$model_rmse, mmmgee_res$model_rmse, statsmodels_res$model_rmse), 
                               Huber_Loss = c(glm_res$model_huber_loss, geeM_res$model_huber_loss, mmmgee_res$model_huber_loss, statsmodels_res$model_huber_loss), 
                               Runtime = c(glm_res$model_runtime, geeM_res$model_runtime, mmmgee_res$model_runtime, statsmodels_res$model_runtime), 
                               Runtime_Units = c(glm_res$model_runtime_units, geeM_res$model_runtime_units, mmmgee_res$model_runtime_units, statsmodels_res$model_runtime_units))
  return(overall_res_df)
})
```

Let's coerce the results to a dataframe & add a couple more features. 

```{r}
sim_res_df <- purrr::reduce(sim_list, rbind) %>% 
              mutate(Abs_Error_Beta0 = abs(Beta_0 - Est_Beta0), 
                     Abs_Error_Beta1 = abs(Beta_1 - Est_Beta1), 
                     Runtime_Seconds = if_else(Runtime_Units == "secs", Runtime, Runtime * 60))
```

# Analysis 

Now that we have everything simulated & recorded, we can analyze the data. Let's start by just looking at the distribution of runtime per method. While we should have expected that the GLMs would be quicker, I'm pretty surprised by just how much faster the `statsmodels` GEEs were than either R implementation. 

```{r}
ggplot(sim_res_df, aes(x = Method, y = Runtime_Seconds, fill = Method)) + 
  geom_violin(scale = "width", 
              draw_quantiles = 0.5, 
              size = 1) + 
  scale_fill_manual(values = paletteer::paletteer_d("MetBrewer::Egypt")) + 
  labs(y = "Runtime (seconds)", fill = "Method") + 
  theme_classic(base_size = 14) + 
  theme(axis.title.x = element_blank()) + 
  guides(fill = guide_legend(override.aes = list(color = NULL)))
```

Moving onto model accuracy, let's look at the RMSE & Huber loss. All methods have essentially the same distribution. 

```{r}
sim_res_df %>% 
  select(Iter, Method, RMSE, Huber_Loss) %>% 
  tidyr::pivot_longer(cols = c(RMSE, Huber_Loss), names_to = "Metric", values_to = "Metric_Value") %>% 
  mutate(Metric = if_else(Metric == "RMSE", "RMSE", "Huber")) %>% 
  ggplot(aes(x = Method, y = Metric_Value, fill = Method)) + 
  facet_wrap(~Metric, 
             nrow = 2, 
             scales = "free_y") + 
  geom_violin(scale = "width", 
              draw_quantiles = 0.5, 
              size = 1) + 
  scale_fill_manual(values = paletteer::paletteer_d("MetBrewer::Egypt")) + 
  labs(y = "Loss", fill = "Method") + 
  theme_classic(base_size = 14) + 
  theme(axis.title.x = element_blank()) + 
  guides(fill = guide_legend(override.aes = list(color = NULL)))
```

How about the estimated intercepts? It again seems that all methods have essentially the same error distribution. 

```{r}
ggplot(sim_res_df, aes(x = Method, y = Abs_Error_Beta0, fill = Method)) + 
  geom_violin(scale = "width", 
              draw_quantiles = 0.5, 
              size = 1) + 
  scale_fill_manual(values = paletteer::paletteer_d("MetBrewer::Egypt")) + 
  labs(y = latex2exp::TeX(r"(Absolute Error for $\beta_0$)"), fill = "Method") + 
  theme_classic(base_size = 14) + 
  theme(axis.title.x = element_blank()) + 
  guides(fill = guide_legend(override.aes = list(color = NULL)))
```

As with the estimated values for $\beta_0$, there is no clear difference between methods with respect to which method is better at estimating $\beta_1$. 

```{r}
ggplot(sim_res_df, aes(x = Method, y = Abs_Error_Beta1, fill = Method)) + 
  geom_violin(scale = "width", 
              draw_quantiles = 0.5, 
              size = 1) + 
  scale_fill_manual(values = paletteer::paletteer_d("MetBrewer::Egypt")) + 
  labs(y = latex2exp::TeX(r"(Absolute Error for $\beta_1$)"), fill = "Method") + 
  theme_classic(base_size = 14) + 
  theme(axis.title.x = element_blank()) + 
  guides(fill = guide_legend(override.aes = list(color = NULL)))
```

Lastly, let's check to see if any of the GEE methods significantly differs from the others in terms of the estimated within-group correlation parameter $\alpha$. 

```{r}
sim_res_df %>% 
  filter(!is.na(Est_Alpha)) %>% 
  ggplot(aes(x = Est_Alpha, fill = Method, color = Method)) + 
  facet_wrap(~Method, nrow = 3) + 
  geom_density(alpha = 0.6, size = 1) + 
  scale_color_manual(values = paletteer::paletteer_d("MetBrewer::Egypt")[-3]) + 
  scale_fill_manual(values = paletteer::paletteer_d("MetBrewer::Egypt")[-3]) + 
  labs(x = latex2exp::TeX(r"(Estimated Correlation Parameter $\alpha$)"), 
       y = "Density", 
       fill = "Method") + 
  theme_classic(base_size = 14) + 
  guides(fill = guide_legend(override.aes = list(alpha = 1, color = NULL)))
```

# Conclusions 

Since the estimated coefficients and correlation parameters seem to be pretty much the same between methods, and the RMSE / Huber loss don't differ much either, it would make sense to choose the modeling framework with the fastest runtime as the best option. For GEEs this is clearly `statsmodels`, which outperformed the R-based `{geeM}` and `{mmmgee}` quite handily (mean and median values are shown below for each method). In addition, the `statsmodels` implementation varies less in runtime than either R option. This would seem to be an easy decision other than that it might be difficult to release & support R packages utilizing a Python library instead of a native R backend, since not all R users know Python, have it installed, or are capable of debugging its output. This is a tricky decision & I'm not sure of the right course to take yet, but I am impressed with how well `statsmodels` performs and will absolutely be considering using it more in the future. 

```{r}
sim_res_df %>% 
  with_groups(Method, 
              summarise, 
              mu = mean(Runtime_Seconds), 
              med = median(Runtime_Seconds), 
              sd = sd(Runtime_Seconds)) %>% 
  arrange(mu) %>% 
  kableExtra::kbl(digits = 3, 
                  booktabs = TRUE, 
                  caption = "Runtime measured in seconds", 
                  col.names = c("Method", "Mean Runtime", "Median Runtime", "S.D. Runtime")) %>% 
  kableExtra::kable_classic(full_width = FALSE)
```


# Session Info 

```{r}
sessioninfo::session_info()
```
