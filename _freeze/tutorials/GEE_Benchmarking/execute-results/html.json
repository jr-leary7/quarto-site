{
  "hash": "ad21c34dfbf7a6dd4fe7377b50fb1427",
  "result": {
    "markdown": "---\ntitle: \"Benchmarking Negative Binomial GEE Model Backends\"\nauthor:\n  name: Jack Leary\n  email: j.leary@ufl.edu\n  affiliations:\n    - name: University of Florida\n      department: Biostatistics \n      city: Gainesville\n      state: FL\ndate: \"2023-03-23\"\nformat:\n  html:\n    code-fold: show\n    code-copy: true\n    code-tools: true\n    toc: true\n    embed-resources: true\n    fig-format: retina\n    df-print: kable\n    link-external-newwindow: true\nexecute:\n  cache: true\n  freeze: auto\n---\n\n\n\n\n# Introduction \n\nIn the course of my recent work on [trajectory differential expression](https://jr-leary7.github.io/quarto-site/tutorials/scLANE_Trajectory_DE.html), I've spent a fair amount of time fitting [generalized estimating equation](https://online.stat.psu.edu/stat504/lesson/12/12.1) (GEE) models. These models are like classical GLMs in that they can handle non-normally distributed response variables via some specified link function & iterative estimation of coefficients, but they differ in that they can account for the variation inherent to longitudinal or otherwise clustered datasets in which observations are measured repeatedly from multiple subjects & are thus not independent. Since scRNA-seq experiments are almost always composed of samples from multiple subjects nowadays, GEEs can be of great use when building models of gene expression, as they allow us to be more confident that our standard errors are accurate. Note that unlike (generalized) linear mixed models, the GEE is a *marginal* model and thus provides only a population-level fit - GLMMs are conditional models as they provide subject-specific fits. \n\nSince [single cell mRNA abundance follows the negative binomial distribution](https://doi.org/10.1186/s13059-021-02584-9), it's necessary that whatever GEE fitting algorithm we use support that distribution. Since the negative binomial is a two-parameter distribution, this does complicate things; the most commonly-used R packages for GEEs do not support NB models. We do have a couple options in R though, and the Python `statsmodels` ecosystem supports NB GEEs as well. Our goal here will be to benchmark the available options in terms of runtime, memory usage, and model accuracy in terms of the fitted values & the estimated coefficients. We'll keep our simulated data simple, with a negative binomial response $Y$ and a single continuously-valued covariate $X$; thus we'll estimate the coefficient vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ using each method for each set of simulations. \n\n# Libraries \n\n## R \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-1_c62736243ce65672511a992d59ad913e'}\n\n```{.r .cell-code}\nlibrary(dplyr)       # data manipulation\nlibrary(ggplot2)     # plots \nlibrary(reticulate)  # call Python from R\n```\n:::\n\n\n## Python\n\nWe'll call the Python code from R using `{reticulate}`, but we'll need to make sure to use the virtual environment I set up previously that has the `statsmodels` and `pandas` libraries (and their various dependencies) installed. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-2_e5ce6aa592ed1642519d89a8c4c82a69'}\n\n```{.r .cell-code}\nuse_virtualenv(\"~/Desktop/Python/science/venv/\", required = TRUE)\nsm <- import(\"statsmodels.api\")\npd <- import(\"pandas\")\n```\n:::\n\n\n# Data \n\nLet's first simulate an example dataset to show what each further simulation should look like. Our parameters are generated from the following distributions (note that we use the mean & dispersion parameterization of the negative binomial): \n\n$$\n\\begin{aligned}\n  \\boldsymbol{\\beta} &\\sim \\mathcal{N}(\\mu = 0,\\: \\sigma^2 = 0.25) \\\\\n  X &\\sim \\mathcal{N}(\\mu = 0,\\: \\sigma^2 = 1) \\\\\n  Y &\\sim \\text{NB}(\\mu = e^{\\beta_0 + X\\beta_1},\\: \\theta = 3)\n\\end{aligned}\n$$\n\nWe'll generate $n = 25$ samples to start, though our actual sample size in the simulations will be larger than this. We add a uniformly-sampled dropout with a rate of 10% to our response $Y$ to mimic the technical variation inherent to scRNA-seq experiments. Lastly, I'll note that we're making this example slightly less complex by not including multiple observations per subject, which we will do in our actual experiment. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-3_1aa6bece839802b1f47f7a861748a4a3'}\n\n```{.r .cell-code}\nset.seed(312)\nsim_beta <- rnorm(2, mean = 0, sd = 0.5)\nsim_df <- data.frame(X = rnorm(25, mean = 0, sd = 1)) %>% \n          mutate(Beta_X = sim_beta[1] + sim_beta[2] * X, \n                 Exp_Beta_X = exp(Beta_X)) %>% \n          rowwise() %>% \n          mutate(Y = rnbinom(1, mu = Exp_Beta_X, size = 3)) %>% \n          ungroup()\ndropout_idx <- sample(seq(nrow(sim_df)), size = round(0.1 * nrow(sim_df)))\nsim_df$Y[dropout_idx] <- 0\n```\n:::\n\n\nNext, since we're dealing with independent observations we fit an NB GLM via the `{MASS}` package which we'll use to generate fitted values and standard errors. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-4_ea2c3a002bb317d0150555920dd6b415'}\n\n```{.r .cell-code}\nnb_glm <- MASS::glm.nb(Y ~ X, \n                       data = sim_df, \n                       link = \"log\")\n```\n:::\n\n\nLet's visualize the results. We see that the confidence interval around the fitted mean widens as $X$ and $Y$ increase. This data looks similar enough to what I see in real scRNA-seq studies, so I'm satisfied with using the above simulation scheme for a more rigorous experiment. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-5_313ced5a90f0279f9e43ca66731f4b77'}\n\n```{.r .cell-code}\nsim_df %>% \n  mutate(glm_pred = predict(nb_glm), \n         glm_pred_se = predict(nb_glm, se.fit = TRUE)$se.fit, \n         glm_fit = exp(glm_pred), \n         glm_fit_ci_lo = exp(glm_pred - 1.96 * glm_pred_se), \n         glm_fit_ci_hi = exp(glm_pred + 1.96 * glm_pred_se)) %>% \n  ggplot(aes(x = X, y = Y)) + \n  geom_point(size = 3) + \n  geom_line(aes(x = X, y = glm_fit), \n            color = \"forestgreen\", \n            size = 1) + \n  geom_ribbon(aes(x = X, ymin = glm_fit_ci_lo, ymax = glm_fit_ci_hi, color = NULL), \n              fill = \"forestgreen\", \n              alpha = 0.4) + \n  labs(x = \"X\", \n       y = \"Y\", \n       title = \"Simulated Negative Binomial mRNA Counts\", \n       subtitle = \"Fitted values from NB GLM with Wald 95% C.I.\") + \n  theme_classic(base_size = 14) + \n  theme(plot.subtitle = element_text(face = \"italic\", size = 12))\n```\n\n::: {.cell-output-display}\n![](GEE_Benchmarking_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n# Simulations \n\n## Helper Functions \n\nUsing the same general simulation procedure from the example above, we'll write a function to simulate negative binomial data suitable for using GEEs. We'll generate multiple observations per subject, though the resulting data will be in the same format, albeit with an added subject ID column. We'll keep things simple by allocating our total sample size evenly across subjects & keeping $\\boldsymbol{\\beta}$ constant across all subjects. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-6_1ee5ef6aeb1a76a5dca9714df95a55e4'}\n\n```{.r .cell-code}\nsim_gee_data <- function(n.subjects = NULL, \n                         n.per.subject = NULL, \n                         dropout.rate = 0.1, \n                         theta.y = 3) {\n  # simulate true coefficients\n  sim_beta <- rnorm(2, mean = 0, sd = 0.5)\n  # simulate X & Y per subject\n  subject_names <- paste0(\"S\", seq(n.subjects))\n  subject_sims <- purrr::map(subject_names, function(s) {\n    sim_df <- data.frame(ID = s, \n                         X = rnorm(n.per.subject, mean = 0, sd = 1)) %>% \n              mutate(Beta_X = sim_beta[1] + sim_beta[2] * X, \n                     Exp_Beta_X = exp(Beta_X)) %>% \n              rowwise() %>% \n              mutate(Y = rnbinom(1, mu = Exp_Beta_X, size = theta.y)) %>% \n              ungroup()\n  })\n  subject_sim_df <- purrr::reduce(subject_sims, rbind) %>% \n                    mutate(ID = as.factor(ID))\n  # add stochastic dropout\n  dropout_idx <- sample(seq(nrow(subject_sim_df)), size = round(dropout.rate * nrow(subject_sim_df)))\n  subject_sim_df$Y[dropout_idx] <- 0\n  res_list <- list(beta = sim_beta, \n                   sim_df = subject_sim_df)\n  return(res_list)\n}\n```\n:::\n\n\nNext, we'll write a function to run all of our different model types. If we use the R implementations of the GEE framework, we need to provide an estimate for $\\theta$, since the estimation procedure depends on $\\theta$ being \"known\". To approximate this, we provide the value of $\\theta$ estimated using the intercept-only model, which in my experience has been a good enough approximation for simpler datasets like this one. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-7_f32d29720550d3b33608a96bd509e567'}\n\n```{.r .cell-code}\nrun_model <- function(sim.data = NULL, model.type = NULL) {\n  # run correct model framework\n  if (model.type == \"NB GLM\") {\n    start_time <- Sys.time()\n    model_fit <- MASS::glm.nb(Y ~ X, \n                              data = sim.data, \n                              link = \"log\")\n    diff_time <- Sys.time() - start_time\n    model_preds <- predict(model_fit, type = \"link\")\n    model_coef <- coef(model_fit)\n    model_est_alpha <- NA_real_\n  } else if (model.type == \"geeM\") {\n    start_time <- Sys.time()\n    theta_hat <- MASS::theta.mm(y = sim.data$Y,\n                                mu = mean(sim.data$Y),\n                                dfr = nrow(sim.data) - 1)\n    model_fit <- geeM::geem(Y ~ X, \n                            id = sim.data$ID, \n                            data = sim.data, \n                            family = MASS::negative.binomial(theta = theta_hat, link = \"log\"), \n                            corstr = \"exchangeable\", \n                            sandwich = TRUE)\n    diff_time <- Sys.time() - start_time\n    model_preds <- predict(model_fit, type = \"link\")\n    model_coef <- coef(model_fit)\n    model_est_alpha <- model_fit$alpha\n  } else if (model.type == \"mmmgee\") {\n    theta_hat <- MASS::theta.mm(y = sim.data$Y,\n                                mu = mean(sim.data$Y),\n                                dfr = nrow(sim.data) - 1)\n    start_time <- Sys.time()\n    model_fit <- mmmgee::geem2(Y ~ X, \n                               id = sim.data$ID, \n                               data = sim.data, \n                               family = MASS::negative.binomial(theta = theta_hat, link = \"log\"), \n                               corstr = \"exchangeable\", \n                               sandwich = TRUE)\n    diff_time <- Sys.time() - start_time\n    model_preds <- predict(model_fit, type = \"link\")\n    model_coef <- coef(model_fit)\n    model_est_alpha <- model_fit$alpha\n  } else if (model.type == \"statsmodels\") {\n    start_time <- Sys.time()\n    nb_family <- sm$families$NegativeBinomial(link = sm$genmod$families$links$log)\n    cor_structure <- sm$cov_struct$Exchangeable()\n    py_gee <- sm$GEE$from_formula(\"Y ~ X\", \n                                  \"ID\", \n                                  data = sim.data, \n                                  cov_struct = cor_structure, \n                                  family = nb_family)\n    model_fit <- py_gee$fit()\n    diff_time <- Sys.time() - start_time\n    model_preds <- model_fit$get_prediction()$linpred$predicted_mean\n    model_coef <- model_fit$params\n    model_est_alpha <- model_fit$cov_struct$dep_params\n  }\n  # format results & compute model error\n  names(model_coef) <- c(\"B0\", \"B1\")\n  model_rmse <- yardstick::rmse_vec(truth = sim.data$Y, \n                                    estimate = exp(model_preds))\n  model_huber_loss <- yardstick::huber_loss_vec(truth = sim.data$Y, \n                                                estimate = exp(model_preds), \n                                                delta = 2)\n  res_list <- list(model_type = model.type, \n                   model_runtime = as.numeric(diff_time), \n                   model_runtime_units = attributes(diff_time)$units, \n                   model_rmse = model_rmse, \n                   model_huber_loss = model_huber_loss, \n                   model_beta = model_coef, \n                   model_alpha = model_est_alpha)\n  return(res_list)\n}\n```\n:::\n\n\n## Running the Experiment\n\nWe fit each model over 100 iterations, and save the results in a list (`{purrr}` is great for this kind of thing). \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-8_47716e93a788f6061f55b9b77fc36bec'}\n\n```{.r .cell-code}\nsim_list <- purrr::map(seq(100), function(i) {\n  set.seed(i)\n  sim_data <- sim_gee_data(n.subjects = 3, \n                           n.per.subject = 400, \n                           dropout.rate = 0.1, \n                           theta.y = 5)\n  glm_res <- run_model(sim.data = sim_data$sim_df, model.type = \"NB GLM\")\n  geeM_res <- run_model(sim.data = sim_data$sim_df, model.type = \"geeM\")\n  mmmgee_res <- run_model(sim.data = sim_data$sim_df, model.type = \"mmmgee\")\n  statsmodels_res <- run_model(sim.data = sim_data$sim_df, model.type = \"statsmodels\")\n  overall_res_df <- data.frame(Iter = i, \n                               Beta_0 = sim_data$beta[1], \n                               Beta_1 = sim_data$beta[2], \n                               Method = c(glm_res$model_type, geeM_res$model_type, mmmgee_res$model_type, statsmodels_res$model_type), \n                               Est_Beta0 = c(glm_res$model_beta[1], geeM_res$model_beta[1], mmmgee_res$model_beta[1], statsmodels_res$model_beta[1]), \n                               Est_Beta1 = c(glm_res$model_beta[2], geeM_res$model_beta[2], mmmgee_res$model_beta[2], statsmodels_res$model_beta[2]), \n                               Est_Alpha = c(glm_res$model_alpha, geeM_res$model_alpha, mmmgee_res$model_alpha, statsmodels_res$model_alpha), \n                               RMSE = c(glm_res$model_rmse, geeM_res$model_rmse, mmmgee_res$model_rmse, statsmodels_res$model_rmse), \n                               Huber_Loss = c(glm_res$model_huber_loss, geeM_res$model_huber_loss, mmmgee_res$model_huber_loss, statsmodels_res$model_huber_loss), \n                               Runtime = c(glm_res$model_runtime, geeM_res$model_runtime, mmmgee_res$model_runtime, statsmodels_res$model_runtime), \n                               Runtime_Units = c(glm_res$model_runtime_units, geeM_res$model_runtime_units, mmmgee_res$model_runtime_units, statsmodels_res$model_runtime_units))\n  return(overall_res_df)\n})\n```\n:::\n\n\nLet's coerce the results to a dataframe & add a couple more features. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-9_0b6fc8f81b97e5c0012e7d3f46dc999f'}\n\n```{.r .cell-code}\nsim_res_df <- purrr::reduce(sim_list, rbind) %>% \n              mutate(Abs_Error_Beta0 = abs(Beta_0 - Est_Beta0), \n                     Abs_Error_Beta1 = abs(Beta_1 - Est_Beta1), \n                     Runtime_Seconds = if_else(Runtime_Units == \"secs\", Runtime, Runtime * 60))\n```\n:::\n\n\n# Analysis \n\nNow that we have everything simulated & recorded, we can analyze the data. Let's start by just looking at the distribution of runtime per method. While we should have expected that the GLMs would be quicker, I'm pretty surprised by just how much faster the `statsmodels` GEEs were than either R implementation. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-10_316bae83af3ab503ab9e5e11e264895b'}\n\n```{.r .cell-code}\nggplot(sim_res_df, aes(x = Method, y = Runtime_Seconds, fill = Method)) + \n  geom_violin(scale = \"width\", \n              draw_quantiles = 0.5, \n              size = 1) + \n  scale_fill_manual(values = paletteer::paletteer_d(\"MetBrewer::Egypt\")) + \n  labs(y = \"Runtime (seconds)\", fill = \"Method\") + \n  theme_classic(base_size = 14) + \n  theme(axis.title.x = element_blank()) + \n  guides(fill = guide_legend(override.aes = list(color = NULL)))\n```\n\n::: {.cell-output-display}\n![](GEE_Benchmarking_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nMoving onto model accuracy, let's look at the RMSE & Huber loss. All methods have essentially the same distribution. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-11_5cd17895ee6763b73e0c832da838717c'}\n\n```{.r .cell-code}\nsim_res_df %>% \n  select(Iter, Method, RMSE, Huber_Loss) %>% \n  tidyr::pivot_longer(cols = c(RMSE, Huber_Loss), names_to = \"Metric\", values_to = \"Metric_Value\") %>% \n  mutate(Metric = if_else(Metric == \"RMSE\", \"RMSE\", \"Huber\")) %>% \n  ggplot(aes(x = Method, y = Metric_Value, fill = Method)) + \n  facet_wrap(~Metric, \n             nrow = 2, \n             scales = \"free_y\") + \n  geom_violin(scale = \"width\", \n              draw_quantiles = 0.5, \n              size = 1) + \n  scale_fill_manual(values = paletteer::paletteer_d(\"MetBrewer::Egypt\")) + \n  labs(y = \"Loss\", fill = \"Method\") + \n  theme_classic(base_size = 14) + \n  theme(axis.title.x = element_blank()) + \n  guides(fill = guide_legend(override.aes = list(color = NULL)))\n```\n\n::: {.cell-output-display}\n![](GEE_Benchmarking_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nHow about the estimated intercepts? It again seems that all methods have essentially the same error distribution. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-12_1e1dceded7af94092dc888bece81872a'}\n\n```{.r .cell-code}\nggplot(sim_res_df, aes(x = Method, y = Abs_Error_Beta0, fill = Method)) + \n  geom_violin(scale = \"width\", \n              draw_quantiles = 0.5, \n              size = 1) + \n  scale_fill_manual(values = paletteer::paletteer_d(\"MetBrewer::Egypt\")) + \n  labs(y = latex2exp::TeX(r\"(Absolute Error for $\\beta_0$)\"), fill = \"Method\") + \n  theme_classic(base_size = 14) + \n  theme(axis.title.x = element_blank()) + \n  guides(fill = guide_legend(override.aes = list(color = NULL)))\n```\n\n::: {.cell-output-display}\n![](GEE_Benchmarking_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nAs with the estimated values for $\\beta_0$, there is no clear difference between methods with respect to which method is better at estimating $\\beta_1$. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-13_3b67f2fb9a247e9281a9383fe1afd0ea'}\n\n```{.r .cell-code}\nggplot(sim_res_df, aes(x = Method, y = Abs_Error_Beta1, fill = Method)) + \n  geom_violin(scale = \"width\", \n              draw_quantiles = 0.5, \n              size = 1) + \n  scale_fill_manual(values = paletteer::paletteer_d(\"MetBrewer::Egypt\")) + \n  labs(y = latex2exp::TeX(r\"(Absolute Error for $\\beta_1$)\"), fill = \"Method\") + \n  theme_classic(base_size = 14) + \n  theme(axis.title.x = element_blank()) + \n  guides(fill = guide_legend(override.aes = list(color = NULL)))\n```\n\n::: {.cell-output-display}\n![](GEE_Benchmarking_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nLastly, let's check to see if any of the GEE methods significantly differs from the others in terms of the estimated within-group correlation parameter $\\alpha$. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-14_a3ab12214c902b91edce08bdd8fd2ece'}\n\n```{.r .cell-code}\nsim_res_df %>% \n  filter(!is.na(Est_Alpha)) %>% \n  ggplot(aes(x = Est_Alpha, fill = Method, color = Method)) + \n  facet_wrap(~Method, nrow = 3) + \n  geom_density(alpha = 0.6, size = 1) + \n  scale_color_manual(values = paletteer::paletteer_d(\"MetBrewer::Egypt\")[-3]) + \n  scale_fill_manual(values = paletteer::paletteer_d(\"MetBrewer::Egypt\")[-3]) + \n  labs(x = latex2exp::TeX(r\"(Estimated Correlation Parameter $\\alpha$)\"), \n       y = \"Density\", \n       fill = \"Method\") + \n  theme_classic(base_size = 14) + \n  guides(fill = guide_legend(override.aes = list(alpha = 1, color = NULL)))\n```\n\n::: {.cell-output-display}\n![](GEE_Benchmarking_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n# Conclusions \n\nSince the estimated coefficients and correlation parameters seem to be pretty much the same between methods, and the RMSE / Huber loss don't differ much either, it would make sense to choose the modeling framework with the fastest runtime as the best option. For GEEs this is clearly `statsmodels`, which outperformed the R-based `{geeM}` and `{mmmgee}` quite handily (mean and median values are shown below for each method). In addition, the `statsmodels` implementation varies less in runtime than either R option. This would seem to be an easy decision other than that it might be difficult to release & support R packages utilizing a Python library instead of a native R backend, since not all R users know Python, have it installed, or are capable of debugging its output. This is a tricky decision & I'm not sure of the right course to take yet, but I am impressed with how well `statsmodels` performs and will absolutely be considering using it more in the future. \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-15_69702cc8594697b6bcb8ffe0a80e41dd'}\n\n```{.r .cell-code}\nsim_res_df %>% \n  with_groups(Method, \n              summarise, \n              mu = mean(Runtime_Seconds), \n              med = median(Runtime_Seconds), \n              sd = sd(Runtime_Seconds)) %>% \n  arrange(mu) %>% \n  kableExtra::kbl(digits = 3, \n                  booktabs = TRUE, \n                  caption = \"Runtime measured in seconds\", \n                  col.names = c(\"Method\", \"Mean Runtime\", \"Median Runtime\", \"S.D. Runtime\")) %>% \n  kableExtra::kable_classic(full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>Runtime measured in seconds</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Method </th>\n   <th style=\"text-align:right;\"> Mean Runtime </th>\n   <th style=\"text-align:right;\"> Median Runtime </th>\n   <th style=\"text-align:right;\"> S.D. Runtime </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> statsmodels </td>\n   <td style=\"text-align:right;\"> 0.019 </td>\n   <td style=\"text-align:right;\"> 0.019 </td>\n   <td style=\"text-align:right;\"> 0.002 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> NB GLM </td>\n   <td style=\"text-align:right;\"> 0.032 </td>\n   <td style=\"text-align:right;\"> 0.030 </td>\n   <td style=\"text-align:right;\"> 0.010 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mmmgee </td>\n   <td style=\"text-align:right;\"> 2.506 </td>\n   <td style=\"text-align:right;\"> 2.390 </td>\n   <td style=\"text-align:right;\"> 0.442 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> geeM </td>\n   <td style=\"text-align:right;\"> 2.609 </td>\n   <td style=\"text-align:right;\"> 2.510 </td>\n   <td style=\"text-align:right;\"> 0.464 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n# Session Info \n\n\n::: {.cell hash='GEE_Benchmarking_cache/html/unnamed-chunk-16_31ad1e7df317ec6966865ff19f58212e'}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-03-23\n pandoc   2.19.2 @ /usr/local/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version  date (UTC) lib source\n assertthat    0.2.1    2019-03-21 [1] CRAN (R 4.2.0)\n cli           3.3.0    2022-04-25 [1] CRAN (R 4.2.0)\n codetools     0.2-18   2020-11-04 [1] CRAN (R 4.2.1)\n colorspace    2.0-3    2022-02-21 [1] CRAN (R 4.2.0)\n crayon        1.5.1    2022-03-26 [1] CRAN (R 4.2.0)\n DBI           1.1.3    2022-06-18 [1] CRAN (R 4.2.0)\n digest        0.6.29   2021-12-01 [1] CRAN (R 4.2.0)\n dplyr       * 1.0.9    2022-04-28 [1] CRAN (R 4.2.0)\n ellipsis      0.3.2    2021-04-29 [1] CRAN (R 4.2.0)\n evaluate      0.16     2022-08-09 [1] CRAN (R 4.2.0)\n fansi         1.0.3    2022-03-24 [1] CRAN (R 4.2.0)\n farver        2.1.1    2022-07-06 [1] CRAN (R 4.2.0)\n fastmap       1.1.0    2021-01-25 [1] CRAN (R 4.2.0)\n geeM          0.10.1   2018-06-18 [1] CRAN (R 4.2.0)\n generics      0.1.3    2022-07-05 [1] CRAN (R 4.2.0)\n ggplot2     * 3.3.6    2022-05-03 [1] CRAN (R 4.2.0)\n glue          1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n gtable        0.3.0    2019-03-25 [1] CRAN (R 4.2.0)\n here          1.0.1    2020-12-13 [1] CRAN (R 4.2.0)\n highr         0.9      2021-04-16 [1] CRAN (R 4.2.0)\n htmltools     0.5.3    2022-07-18 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4    2021-09-08 [1] CRAN (R 4.2.0)\n httr          1.4.4    2022-08-17 [1] CRAN (R 4.2.0)\n jsonlite      1.8.0    2022-02-22 [1] CRAN (R 4.2.0)\n kableExtra    1.3.4    2021-02-20 [1] CRAN (R 4.2.0)\n knitr         1.40     2022-08-24 [1] CRAN (R 4.2.0)\n labeling      0.4.2    2020-10-20 [1] CRAN (R 4.2.0)\n latex2exp     0.9.4    2022-03-02 [1] CRAN (R 4.2.0)\n lattice       0.20-45  2021-09-22 [1] CRAN (R 4.2.1)\n lifecycle     1.0.1    2021-09-24 [1] CRAN (R 4.2.0)\n magrittr      2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n MASS          7.3-58.1 2022-08-03 [1] CRAN (R 4.2.0)\n Matrix        1.4-1    2022-03-23 [1] CRAN (R 4.2.1)\n mmmgee        1.20     2019-06-21 [1] CRAN (R 4.2.0)\n munsell       0.5.0    2018-06-12 [1] CRAN (R 4.2.0)\n mvtnorm       1.1-3    2021-10-08 [1] CRAN (R 4.2.0)\n paletteer     1.5.0    2022-10-19 [1] CRAN (R 4.2.0)\n pillar        1.8.1    2022-08-19 [1] CRAN (R 4.2.0)\n pkgconfig     2.0.3    2019-09-22 [1] CRAN (R 4.2.0)\n png           0.1-7    2013-12-03 [1] CRAN (R 4.2.0)\n prismatic     1.1.1    2022-08-15 [1] CRAN (R 4.2.0)\n purrr         0.3.4    2020-04-17 [1] CRAN (R 4.2.0)\n R6            2.5.1    2021-08-19 [1] CRAN (R 4.2.0)\n Rcpp          1.0.9    2022-07-08 [1] CRAN (R 4.2.0)\n rematch2      2.1.2    2020-05-01 [1] CRAN (R 4.2.0)\n reticulate  * 1.25     2022-05-11 [1] CRAN (R 4.2.0)\n rlang         1.0.4    2022-07-12 [1] CRAN (R 4.2.0)\n rmarkdown     2.16     2022-08-24 [1] CRAN (R 4.2.0)\n rprojroot     2.0.3    2022-04-02 [1] CRAN (R 4.2.0)\n rstudioapi    0.14     2022-08-22 [1] CRAN (R 4.2.0)\n rvest         1.0.3    2022-08-19 [1] CRAN (R 4.2.0)\n scales        1.2.1    2022-08-20 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2    2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8    2022-07-11 [1] CRAN (R 4.2.0)\n stringr       1.4.1    2022-08-20 [1] CRAN (R 4.2.0)\n svglite       2.1.0    2022-02-03 [1] CRAN (R 4.2.0)\n systemfonts   1.0.4    2022-02-11 [1] CRAN (R 4.2.0)\n tibble        3.1.8    2022-07-22 [1] CRAN (R 4.2.0)\n tidyr         1.2.0    2022-02-01 [1] CRAN (R 4.2.0)\n tidyselect    1.1.2    2022-02-21 [1] CRAN (R 4.2.0)\n utf8          1.2.2    2021-07-24 [1] CRAN (R 4.2.0)\n vctrs         0.4.1    2022-04-13 [1] CRAN (R 4.2.0)\n viridisLite   0.4.1    2022-08-22 [1] CRAN (R 4.2.0)\n webshot       0.5.3    2022-04-14 [1] CRAN (R 4.2.0)\n withr         2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun          0.32     2022-08-10 [1] CRAN (R 4.2.0)\n xml2          1.3.3    2021-11-30 [1] CRAN (R 4.2.0)\n yaml          2.3.5    2022-02-21 [1] CRAN (R 4.2.0)\n yardstick     1.0.0    2022-06-06 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         /Users/jack/Desktop/Python/science/venv/bin/python\n libpython:      /usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/config-3.8-darwin/libpython3.8.dylib\n pythonhome:     /Users/jack/Desktop/Python/science/venv:/Users/jack/Desktop/Python/science/venv\n virtualenv:     /Users/jack/Desktop/Python/science/venv/bin/activate_this.py\n version:        3.8.16 (default, Dec  7 2022, 01:36:11)  [Clang 14.0.0 (clang-1400.0.29.202)]\n numpy:          /Users/jack/Desktop/Python/science/venv/lib/python3.8/site-packages/numpy\n numpy_version:  1.22.4\n statsmodels:    /Users/jack/Desktop/Python/science/venv/lib/python3.8/site-packages/statsmodels\n \n NOTE: Python version was forced by use_python function\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}