{
  "hash": "236ae08148a05e9f78e9d8f690140e2e",
  "result": {
    "markdown": "---\ntitle: \"IN PROGRESS - Interpreting Intercepts in Linear Regression\"\nauthor:\n  name: Jack Leary\n  email: j.leary@ufl.edu\n  affiliations:\n    - name: University of Florida\n      department: Biostatistics \n      city: Gainesville\n      state: FL\ndate: \"2023-01-03\"\nformat:\n  html:\n    code-fold: show\n    code-copy: true\n    code-tools: true\n    toc: true\n    self-contained: true\n    fig-format: retina\n    df-print: kable\n    link-external-newwindow: true\nexecute: \n  cache: true\n  freeze: auto\n---\n\n\n# Introduction \n\nOne concept I struggled with a lot in early statistics courses was what the intercept meant in linear regression models. I tended to just ignore it unless questions specifically pertained to it, and the vast majority of homework questions focused on interpreting the effects of covariates instead. I saw many of my master's-level students struggle with in the SAS computing course I taught during Fall 2022 as well, with confusion about the effect of centering, the difference between centering and standardizing, and intercept interpretation in the different types of (generalized) linear (mixed) models being common pain points on homeworks. As such, I thought it might be useful - for myself and others - to jot down some notes on how the intercept is estimated and what it means under a variety of regression modelling frameworks. \n\n# Matrix Algebra Review \n\nWe're going to start from first principles here with a quick review on matrix algebra. Linear regression is, after, just multiplying matrices in a clever way. \n\n## Multiplication \n\n### Theory \n\nFirst define two matrices $\\mathbf{A}$ and $\\mathbf{B}$, each with 2 rows and 2 columns:\n\n$$\n\\begin{aligned}\n\\mathbf{A} &= \n    \\begin{bmatrix} \n      a_{11} & a_{21} \\\\\n      a_{12} & a_{22} \\\\\n    \\end{bmatrix} \\\\\n\\mathbf{B} &= \n  \\begin{bmatrix} \n    b_{11} & b_{21} \\\\\n    b_{12} & b_{22} \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n$$\n\nTheir product, another matrix $C$, also has 2 rows and 2 columns, and its elements are defined like so, with $i$ specifying the row and $j$ the column of each element. What we're doing is finding the dot product of the $i^{\\text{th}}$ row of $\\mathbf{A}$ and the $j^{\\text{th}}$ column of $\\mathbf{B}$, the expanded definition of which is below. \n\n$$\n\\begin{aligned}\nc_{ij} &= \\mathbf{A}_{i*} \\cdot \\mathbf{B}_{*j} \\\\\nc_{ij} &= \\sum_{k=1}^n a_{ik}b_{kj} \\\\\nc_{ij} &= a_{i1}b_{1j} + \\dots + a_{n1}b_{nj} \\\\\n\\end{aligned}\n$$\n\nAs such, we can define the product of $\\mathbf{A}$ and $\\mathbf{B}$ like so:\n\n$$\n\\begin{aligned}\n\\mathbf{C} &=  \\mathbf{A} \\mathbf{B} \\\\\n\\mathbf{C} &= \n  \\begin{bmatrix} \n    \\mathbf{A}_{1*} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*1} \\\\\n    \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*2} \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{C} &= \n  \\begin{bmatrix} \n    a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n    a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n$$\n\n**Important Note**: To multiply two matrices $\\mathbf{A}$ and $\\mathbf{B}$ together, the number of rows of $\\mathbf{B}$ must be equal to the number of columns in $\\mathbf{A}$. To generalize: \n\n$$\n\\mathbf{A}_{mn} \\cdot \\mathbf{B}_{np} = \\mathbf{C}_{mp}\n$$\n\n### Example \n\nLet's define two matrices:\n\n$$\n\\begin{aligned}\n\\mathbf{A} &= \n  \\begin{bmatrix} \n    3 & 2 \\\\\n    0 & 7 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{B} &= \n  \\begin{bmatrix} \n    1 & 4 \\\\\n    1 & 2 \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n$$\n\nTheir product $\\mathbf{C}$ is defined as:\n\n$$\n\\begin{aligned}\n\\mathbf{C} &= \n  \\begin{bmatrix} \n    3 \\times 1 + 2 \\times 1 & 3 \\times 4 + 2 \\times 2 \\\\\n    0 \\times 1 + 7 \\times 1 & 0 \\times 4 + 7 \\times 2 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{C} &= \n  \\begin{bmatrix} \n    5 & 16 \\\\\n    7 & 14 \\\\\n  \\end{bmatrix} \\\\\n\\end{aligned}\n$$\n\nWe can check this using R:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-1_29d67f3e22fc3112145abd79b3948dd9'}\n\n```{.r .cell-code}\nA_mat <- matrix(c(3, 2, 0, 7), \n                nrow = 2, \n                ncol = 2, \n                byrow = TRUE)\nB_mat <- matrix(c(1, 4, 1, 2), \n                nrow = 2, \n                ncol = 2, \n                byrow = TRUE)\nC_mat <- A_mat %*% B_mat\nC_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    5   16\n[2,]    7   14\n```\n:::\n:::\n\n\n## Transposition \n\n### Theory \n\n### Example \n\n## Inversion \n\n### Theory \n\n### Example \n\n\n## The Identity Matrix \n\n### Theory \n\nThe identity matrix $\\mathbf{I}_{n}$ is a square matrix composed entirely of zeroes *except* along the diagonal, which is composed of ones. This matrix carries some unique properties (which are listed below) that will be helpful to us later on.  \n\n$$\n\\begin{aligned}\n\\mathbf{I}_{n} &= \n  \\begin{bmatrix} \n    1 & 0 & \\cdots & 0 \\\\ \n    0 & 1 & \\cdots & 0 \\\\ \n    \\vdots & \\vdots & \\ddots & 0 \\\\ \n    0 & 0 & 0 & 1 \\\\\n  \\end{bmatrix} \\\\\n\\mathbf{I}_{n}^\\prime &= \\mathbf{I}_{n} \\\\\n\\mathbf{I}_{n}^{-1} &= \\mathbf{I}_{n} \\\\\n\\end{aligned}\n$$\n\n### Example \n\nWe can set up a $3 \\times 3$ identity matrix $\\mathbf{I}_{3}$ in R using the `diag()` function:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-2_748a340cf8fa564302ee2654d69ed604'}\n\n```{.r .cell-code}\nident_mat <- diag(nrow = 3)\nident_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n:::\n:::\n\n\nThe transpose is also equal to $\\mathbf{I}_{3}$:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-3_7c2504c5948f004602835ef7ba39b92b'}\n\n```{.r .cell-code}\nt(ident_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n:::\n:::\n\n\nAs is the inverse: \n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-4_d960eb4a7cfdce9b96e5752666f170da'}\n\n```{.r .cell-code}\nsolve(ident_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n:::\n:::\n\n\n# Linear Model \n\n## Setup \n\nFor now, we'll take it for granted that the solution to a linear regression problem is defined as follows:\n\n$$\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y}\n$$\n\n## The Intercept-only Model \n\nThe intercept-only model (also sometimes called the null model) is defined as linear regression when $\\mathbf{X}$ is simply a column vector of ones:\n\n$$\n\\mathbf{X} = \n  \\begin{bmatrix} \n    1 \\\\\n    \\vdots \\\\\n    1 \\\\\n  \\end{bmatrix}\n$$\n\nWe know the intercept-only model produces the mean as the one predicted value, as the mean minimizes the sum of squared errors in the absence of any other covariates. We can check this using R - we'll first generate a vector $\\mathbf{y}$ consisting of 5 realizations of a random variable, such that $\\mathbf{Y} \\sim \\mathcal{N}(0, 3)$. \n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-5_00ed1b9e872103b8c397054c55e50b8b'}\n\n```{.r .cell-code}\ny <- rnorm(5, mean = 0, sd = 3)\ny <- matrix(y, ncol = 1)\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,]  0.7142100\n[2,] -0.4898113\n[3,]  2.6509190\n[4,]  0.1575862\n[5,]  1.9798764\n```\n:::\n:::\n\n\nThe mean of $\\mathbf{y}$ is:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-6_3e94303b90f4e8bebdc0e56b96597055'}\n\n```{.r .cell-code}\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.002556\n```\n:::\n:::\n\n\nWe can use R to fit an intercept-only model. We can see that the intercept coefficient $\\beta_0$ is equal to the mean of $\\mathbf{y}$. \n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-7_d32d8b2f1dd089bd218a839994e1ffa7'}\n\n```{.r .cell-code}\nnull_mod <- lm(y ~ 1)\ncoef(null_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n   1.002556 \n```\n:::\n:::\n\n\nLet's use linear algebra to figure out why this is true. Once again, we know that the linear regression closed-form solution is given by the following: \n\n$$\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y}\n$$\n\nLet's first define $\\mathbf{X}$:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-8_2a16898c898124ad485b1cbc6875b212'}\n\n```{.r .cell-code}\nX <- c(1, 1, 1, 1, 1)\nX <- matrix(X, ncol = 1)\nX\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    1\n[5,]    1\n```\n:::\n:::\n\n\nThe value of $\\mathbf{X}^\\prime \\mathbf{X}$ is given by the following - note that this is equal to our sample size $n = 5$. \n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-9_89b7c02a79c83564cb6668c012ac3790'}\n\n```{.r .cell-code}\nt(X) %*% X\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    5\n```\n:::\n:::\n\n\nThe inverse of which, $\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1}$, is of course $n^{-1}$:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-10_e9fd790838a2fdc079ca98de3df1fde4'}\n\n```{.r .cell-code}\nsolve(t(X) %*% X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]  0.2\n```\n:::\n:::\n\n\nWe multiply the above by $\\mathbf{X}^\\prime$ again to obtain $\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime$, which gives us a constant vector of length $n$ with all values being equal to $n^{-1}$:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-11_76bedadb868012b9de1d4994182da7da'}\n\n```{.r .cell-code}\nsolve(t(X) %*% X) %*% t(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.2  0.2  0.2  0.2  0.2\n```\n:::\n:::\n\n\nLastly, we multiply the above by $\\mathbf{y}$. Remember how multiplying vectors works - in this case we are multiplying each element of the above vector $\\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime$ with each element of $\\mathbf{y}$ and adding them together. We'll define $\\mathbf{Z} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime$ for convenience of notation: \n\n$$\n\\mathbf{Z} \\mathbf{y} = \\sum_{i=1}^n \\mathbf{Z}_i \\mathbf{y}_i\n$$\n\nSince each element of $\\mathbf{Z}$ is the same, $n^{-1}$, the above quantity is equivalent to:\n\n$$\n\\begin{aligned}\n  \\mathbf{Z} \\mathbf{y} &= \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y} \\\\\n  \\mathbf{Z} \\mathbf{y} &= \\sum_{i=1}^n \\mathbf{Z}_i \\mathbf{y}_i \\\\\n  \\mathbf{Z} \\mathbf{y} &= n^{-1} \\sum_{i=1}^n \\mathbf{y}_i \\\\\n\\end{aligned}\n$$\n\nThis is simply the sum of all the element of $\\mathbf{y}$ divided by $n$ - the mean! We can verify this with R by using linear algebra to compute the OLS solution:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-12_0c2c14414ffab1f6d74e339ac5c0e6c5'}\n\n```{.r .cell-code}\nsolve(t(X) %*% X) %*% t(X) %*% y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]\n[1,] 1.002556\n```\n:::\n:::\n\n\nThis is equal to simply taking the mean of $\\mathbf{y}$: \n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-13_2a32a88f44f60dea3114ed48399d26d8'}\n\n```{.r .cell-code}\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.002556\n```\n:::\n:::\n\n\n# Generalized Linear Model\n\nWe'll next move to the more complicated case of the generalized linear model. \n\n# Linear Mixed Model\n\n# Generalized Linear Mixed Model\n\n# References \n\n1. Faraway, J. [Extending the Linear Model with R, 2nd Edition](https://doi.org/10.1201/9781315382722). *Chapman and Hall*. 2016. \n\n2. Hastie, T. *et al*. [The Elements of Statistical Learning, 2nd Edition](https://doi.org/10.1007/b94608). *Springer*. 2009. \n\n# Session Info \n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-14_774c0faac178f05055554540b251432b'}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-01-01\n pandoc   2.19.2 @ /usr/local/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.3.0   2022-04-25 [1] CRAN (R 4.2.0)\n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.2.1)\n digest        0.6.29  2021-12-01 [1] CRAN (R 4.2.0)\n evaluate      0.16    2022-08-09 [1] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n htmltools     0.5.3   2022-07-18 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.0)\n jsonlite      1.8.0   2022-02-22 [1] CRAN (R 4.2.0)\n knitr         1.40    2022-08-24 [1] CRAN (R 4.2.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n rlang         1.0.4   2022-07-12 [1] CRAN (R 4.2.0)\n rmarkdown     2.16    2022-08-24 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr       1.4.1   2022-08-20 [1] CRAN (R 4.2.0)\n xfun          0.32    2022-08-10 [1] CRAN (R 4.2.0)\n yaml          2.3.5   2022-02-21 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}