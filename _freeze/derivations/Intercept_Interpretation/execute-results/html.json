{
  "hash": "c4fc9c2cde4e5962348c28e5da8ab0ca",
  "result": {
    "markdown": "---\ntitle: \"IN PROGRESS - Interpreting Intercepts in Linear Regression\"\nauthor:\n  name: Jack Leary\n  email: j.leary@ufl.edu\n  affiliations:\n    - name: University of Florida\n      department: Biostatistics \n      city: Gainesville\n      state: FL\ndate: \"2022-12-29\"\nformat:\n  html:\n    code-fold: show\n    code-copy: true\n    code-tools: true\n    toc: true\n    self-contained: true\n    fig-format: retina\n    df-print: kable\n    link-external-newwindow: true\nexecute: \n  cache: true\n  freeze: auto\n---\n\n\n# Introduction \n\nOne concept I struggled with a lot in early statistics courses was what the intercept meant in linear regression models. I tended to just ignore it unless questions specifically pertained to it, and the vast majority of homework questions focused on interpreting the effects of covariates instead. I saw many of my master's-level students struggle with in the SAS computing course I taught during Fall 2022 as well, with confusion about the effect of centering, the difference between centering and standardizing, and intercept interpretation in mixed models being common pain points on homeworks. As such, I thought it might be useful - for myself and others - to jot down some notes on how the intercept is estimated and what it means under a variety of regression modelling frameworks. \n\n# Matrix Algebra Review \n\nWe're going to start from first principles here with a quick review on matrix algebra. Linear regression is, after, just multiplying matrices in a clever way. \n\n## Multiplication \n\n### Theory \n\nFirst define two matrices $A$ and $B$, each with 2 rows and 2 columns:\n\n$$\n\\begin{aligned}\n\n\\mathbf{A} &= \n  \\begin{bmatrix} \n    a_{11} & a_{21} \\\\\n    a_{12} & a_{22} \\\\\n  \\end{bmatrix} \\\\\n  \n& \\\\\n  \n\\mathbf{B} &= \n  \\begin{bmatrix} \n    b_{11} & b_{21} \\\\\n    b_{12} & b_{22} \\\\\n  \\end{bmatrix}\n\n\\end{aligned}\n$$\n\nTheir product, another matrix $C$, also has 2 rows and 2 columns, and its elements are defined like so, with $i$ specifying the row and $j$ the column of each element. What we're doing is finding the dot product of the $i^{\\text{th}}$ row of $\\mathbf{A}$ and the $i^{\\text{th}}$ column of $\\mathbf{B}$, the expanded definition of which is below. \n\n$$\n\\begin{aligned}\n\nc_{i,j} &= \\mathbf{A}_{i*} \\cdot \\mathbf{B}_{*j} \\\\\nc_{i,j} &= \\sum_{k=1}^n a_{ik}b_{kj} \\\\\nc_{i,j} &= a_{i1}b_{1j} + \\dots + a_{n1}b_{nj} \\\\\n\n\\end{aligned}\n$$\n\nAs such, we can define the product of $\\mathbf{A}$ and $\\mathbf{B}$ like so:\n\n$$\n\\begin{aligned}\n\n\\mathbf{C} &=  \\mathbf{A} \\mathbf{B} \\\\\n\n& \\\\\n\n\\mathbf{C} &= \n  \\begin{bmatrix} \n    \\mathbf{A}_{1*} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*1} \\\\\n    \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2*} \\cdot \\mathbf{B}_{*2} \\\\\n  \\end{bmatrix} \\\\\n\n& \\\\\n\n\\mathbf{C} &= \n  \\begin{bmatrix} \n    a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n    a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\\\\n  \\end{bmatrix} \\\\\n\n\\end{aligned}\n$$\n\n**Important Note**: To multiply two matrices $\\mathbf{A}$ and $\\mathbf{B}$ together, the number of rows of $\\mathbf{B}$ must be equal to the number of columns in $\\mathbf{A}$. To generalize: \n\n$$\n\\mathbf{A}_{mn} \\cdot \\mathbf{B}_{np} = \\mathbf{C}_{mp}\n$$\n\n### Example \n\nLet's define two matrices:\n\n$$\n\\begin{aligned}\n\n\\mathbf{A} &= \n  \\begin{bmatrix} \n    3 & 2 \\\\\n    0 & 7 \\\\\n  \\end{bmatrix} \\\\\n  \n& \\\\\n  \n\\mathbf{B} &= \n  \\begin{bmatrix} \n    1 & 4 \\\\\n    1 & 2 \\\\\n  \\end{bmatrix}\n\n\\end{aligned}\n$$\n\nTheir product $\\mathbf{C}$ is defined as:\n\n$$\n\\begin{aligned}\n\n\\mathbf{C} &= \n  \\begin{bmatrix} \n    3 \\times 1 + 2 \\times 1 & 3 \\times 4 + 2 \\times 2 \\\\\n    0 \\times 1 + 7 \\times 1 & 0 \\times 4 + 7 \\times 2 \\\\\n  \\end{bmatrix} \\\\\n\n& \\\\\n\n\\mathbf{C} &= \n  \\begin{bmatrix} \n    5 & 16 \\\\\n    7 & 14 \\\\\n  \\end{bmatrix} \\\\\n\n\\end{aligned}\n$$\n\nWe can check this using R:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-1_29d67f3e22fc3112145abd79b3948dd9'}\n\n```{.r .cell-code}\nA_mat <- matrix(c(3, 2, 0, 7), \n                nrow = 2, \n                ncol = 2, \n                byrow = TRUE)\nB_mat <- matrix(c(1, 4, 1, 2), \n                nrow = 2, \n                ncol = 2, \n                byrow = TRUE)\nC_mat <- A_mat %*% B_mat\nC_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    5   16\n[2,]    7   14\n```\n:::\n:::\n\n\n## Transposition \n\n### Theory \n\n### Example \n\n## Inversion \n\n### Theory \n\n### Example \n\n\n## The Identity Matrix \n\n### Theory \n\nThe identity matrix $\\mathbf{I}_{n}$ is a square matrix composed entirely of zeroes *except* along the diagonal, which is composed of ones. This matrix carries some unique properties (which are listed below) that will be helpful to us later on.  \n\n$$\n\\begin{aligned}\n\n\\mathbf{I}_{n} &= \n  \\begin{bmatrix} \n    1 & 0 & \\cdots & 0 \\\\ \n    0 & 1 & \\cdots & 0 \\\\ \n    \\vdots & \\vdots & \\ddots & 0 \\\\ \n    0 & 0 & 0 & 1 \\\\\n  \\end{bmatrix}\n  \n& \\\\\n\n\\mathbf{I}_{n}^\\prime &= \\mathbf{I}_{n} \\\\\n\n& \\\\\n\n\\mathbf{I}_{n}^{-1} &= \\mathbf{I}_{n} \\\\\n\n\\end{aligned}\n$$\n\n### Example \n\nWe can set up a $3 \\times 3$ identity matrix $\\mathbf{I}_{3}$ in R using the `diag()` function:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-2_748a340cf8fa564302ee2654d69ed604'}\n\n```{.r .cell-code}\nident_mat <- diag(nrow = 3)\nident_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n:::\n:::\n\n\nThe transpose is also equal to $\\mathbf{I}_{3}$:\n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-3_7c2504c5948f004602835ef7ba39b92b'}\n\n```{.r .cell-code}\nt(ident_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n:::\n:::\n\n\nAs is the inverse: \n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-4_d960eb4a7cfdce9b96e5752666f170da'}\n\n```{.r .cell-code}\nsolve(ident_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n:::\n:::\n\n\n# Linear Regression \n\n## Setup \n\nFor now, we'll take it for granted that the solution to a linear regression problem is defined as follows:\n\n$$\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^\\prime \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\prime \\mathbf{y}\n$$\n\n## The Intercept-only Model \n\nThe intercept-only model (also sometimes called the null model) is defined as linear regression when $\\mathbf{X}$ is simply a column vector of ones:\n\n$$\n\\mathbf{X} = \n  \\begin{bmatrix} \n    1 \\\\\n    \\vdots \\\\\n    1 \\\\\n  \\end{bmatrix}\n$$\n\n\n# References \n\n\n\n# Session Info \n\n\n::: {.cell hash='Intercept_Interpretation_cache/html/unnamed-chunk-5_3d967de584cc6b9bc2908b1df750b742'}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2022-12-29\n pandoc   2.19.2 @ /usr/local/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.3.0   2022-04-25 [1] CRAN (R 4.2.0)\n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.2.1)\n digest        0.6.29  2021-12-01 [1] CRAN (R 4.2.0)\n evaluate      0.16    2022-08-09 [1] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n htmltools     0.5.3   2022-07-18 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.0)\n jsonlite      1.8.0   2022-02-22 [1] CRAN (R 4.2.0)\n knitr         1.40    2022-08-24 [1] CRAN (R 4.2.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n rlang         1.0.4   2022-07-12 [1] CRAN (R 4.2.0)\n rmarkdown     2.16    2022-08-24 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr       1.4.1   2022-08-20 [1] CRAN (R 4.2.0)\n xfun          0.32    2022-08-10 [1] CRAN (R 4.2.0)\n yaml          2.3.5   2022-02-21 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}