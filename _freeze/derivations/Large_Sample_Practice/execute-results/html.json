{
  "hash": "c42bccb64352de9fda6d3040bb6d477a",
  "result": {
    "markdown": "---\ntitle: \"Assorted Large Sample Theory Practice Problems\"\nauthor:\n  name: Jack Leary\n  email: j.leary@ufl.edu\n  affiliations:\n    - name: University of Florida\n      department: Biostatistics \n      city: Gainesville\n      state: FL\ndate: \"2023-05-11\"\nformat:\n  html:\n    code-fold: show\n    code-copy: true\n    code-tools: true\n    toc: true\n    embed-resources: true\n    fig-format: retina\n    df-print: kable\n    link-external-newwindow: true\nexecute: \n  cache: true\n  freeze: auto\n  warning: false\n---\n\n\n# Introduction \n\nHere are a couple practice problems I wrote up while studying for a course in statistical asymptotics. The main focus is on the application of influence functions & U-statistics, though there are some simulation examples of statistical properties with code as well. Sources for the problems are listed throughout; some are taken directly from the original source while others I have modified slightly.  \n\n# Libraries\n\n\n::: {.cell hash='Large_Sample_Practice_cache/html/unnamed-chunk-1_99dd5e325de9eb18ae86f5286ca47193'}\n\n```{.r .cell-code}\nlibrary(dplyr)    # data manipulation\nlibrary(ggplot2)  # plots\n```\n:::\n\n\n# Delta Method \n\n## Exercise 3.1 - *Asymptotic Statistics*\n\nThis is exercise 3.1 from [van der Vaart's textbook](https://doi.org/10.1017/CBO9780511802256). The setup is as follows:\n\n$$\n\\begin{aligned}\n  & X_1, ..., X_n \\overset{\\small{\\text{IID}}}{\\sim} F \\\\\n  &\\bar{X}_n = n^{-1} \\sum_{i=1}^n X_i \\\\\n  & S^2_n = n^{-1} \\sum_{i=1}^n \\left( X_i - \\bar{X}_n \\right)^2 \\\\\n  & \\mu_4 = \\mathbb{E} \\left[ (X - \\mu)^4 \\right];\\: \\mu_4 \\in \\mathbb{R} \\\\\n\\end{aligned}\n$$\n\nWe're interested in finding the joint asymptotic distribution of the following, and in determining what assumptions are necessary for the two quantities to be considered asymptotically independent:\n\n$$\n\\begin{pmatrix}\n  \\sqrt{n}(\\bar{X}_n - \\mu) \\\\\n  \\sqrt{n}(S^2_n - \\sigma^2)\n\\end{pmatrix}\n$$\n\nWe'll start by defining the basics; we know that the sample mean converges in expectation to the population mean, and thus converges in probability as well. \n\n$$\n\\mathbb{E} \\left[ \\bar{X}_n \\right]\\mu \\implies \\bar{X}_n \\overset{p}{\\to} \\mu\n$$\n\nOn the other hand, $S^2_n$ is not an unbiased estimator, with expectation:\n\n$$\n\\mathbb{E} \\left[ S^2_n \\right] = \\frac{n-1}{n} \\sigma^2\n$$\n\nHowever, we can show that it converges to $\\sigma^2$ in probability. **Note**: going forwards, raw moments will be denoted $m_k$, and central moments $\\mu_k$. \n\n$$\n\\begin{aligned}\n  S^2_n \n    &= n^{-1} \\sum_{i=1}^n \\left( X_i - \\bar{X}_n \\right)^2 \\\\\n    &= n^{-1} \\sum_{i=1}^n X_i^2 - 2X_i\\bar{X}_n + \\bar{X}_n^2 \\\\\n    &= \\bar{X^2_n} - \\bar{X}^2_n \\\\\n  \\underset{n \\to \\infty}{\\text{lim}} S^2_n \n    &= m_2 - m_1^2 \\\\\n    &= \\sigma^2 \\\\\n  \\implies S^2_n &\\overset{p}{\\to} \\sigma^2 \\\\\n\\end{aligned}\n$$\n\nSince both estimators converge in probability to their population parameters, they converge in distribution as well thanks to the following property of stochastic convergence:\n\n$$\nX_n \\overset{p}{\\to} X \\implies X_n \\overset{d}{\\to} X\n$$\n\nThe asymptotic variance of the sample mean is derived like so:\n\n$$\n\\begin{aligned}\n  \\text{Var}(\\bar{X}_n)\n    &= \\text{Var} \\left( n^{-1} \\sum_{i=1}^n X_i \\right) \\\\\n    &= n^{-2} \\sum_{i=1}^n \\text{Var}(X_i) \\\\\n    &= n^{-1}\\sigma^2 \\\\\n\\end{aligned}\n$$\n\nThus, the asymptotic distribution of the sample mean is:\n\n$$\n\\sqrt{n} \\left( \\bar{X}_n - \\mu \\right) \\overset{d}{\\to} \\mathcal{N}\\left( 0, \\sigma^2 \\right)\n$$\n\nDeriving the asymptotic variance for $S^2_n$ is slightly trickier. Going forward, we'll assume that the observations $X_i$ have been centered around $\\mu$, and thus have zero mean. This will simplify the following derivation somewhat:\n\n$$\n\\begin{aligned}\n\\text{Var} \\left( S^2_n \\right)\n  &= \\text{Var} \\left( n^{-1} \\sum_{i=1}^n X_i^2 - \\bar{X}_n^2  \\right) \\\\ \n  &= \\text{Var} \\left( n^{-1} \\sum_{i=1}^n X_i^2 \\right) \\\\ \n  &= n^{-2} \\sum_{i=1}^n \\text{Var} \\left( X_i^2 \\right) \\\\\n  &= n^{-2} \\sum_{i=1}^n \\mathbb{E}\\left[ X_i^4 \\right] - \\left( \\mathbb{E}[X_i^2] \\right)^2 \\\\\n  &= n^{-2} \\sum_{i=1}^n m_4 - m_2^2 \\\\\n  &= n^{-1} \\left( m_4 - m_2^2 \\right) \\\\\n\\end{aligned}\n$$\n\nThus, the asymptotic distribution for the sample variance is:\n\n$$\n\\sqrt{n} \\left( S^2_n - \\sigma^2 \\right) \\overset{d}{\\to} \\mathcal{N} \\left( 0, m_4 - m_2^2 \\right)\n$$\n\nNow all we need is the covariance between the two estimators. We'll derive this using the multivariate Delta Method. First, we'll need to define the sample mean and variance as a functional. Then, we'll derive the asymptotic distribution of that quantity, the general form of which will be:\n\n$$\n\\sqrt{n} \\left( \\hat{\\theta}_n - \\theta \\right) \\overset{d}{\\to} \\mathcal{N}_2(\\mathbf{0}, \\boldsymbol{\\Sigma})\n$$\n\nThe multivariate Delta Method then allows us to formulate the joint asymptotic distribution of the original estimators like so:\n\n$$\n\\sqrt{n} \\left( \\phi(\\hat{\\theta}_n) - \\phi(\\theta) \\right) \\overset{d}{\\to} \\mathcal{N}_2 \\left( \\mathbf{0},\\: \\phi^\\prime(\\theta) \\boldsymbol{\\Sigma} \\phi^\\prime(\\theta)^T \\right)\n$$\n\nWe can derive the covariance between the sample mean & sample variance in the following fashion, making use of some properties of summation:\n\n$$\n\\begin{aligned}\n  \\text{Cov} \\left( \\bar{X}_n, S^2_n \\right)\n    &= \\mathbb{E} \\left[ \\left( \\bar{X}_n  - \\mathbb{E} \\left[ \\bar{X}_n \\right] \\right) \\left( S^2_n  - \\mathbb{E} \\left[ S^2_n \\right] \\right) \\right] \\\\\n    &= \\mathbb{E} \\left[ \\left( \\bar{X}_n  - m_1 \\right) \\left( S^2_n  - m_2 \\right) \\right] \\\\\n    &= \\mathbb{E} \\left[ \\bar{X}_n S^2_n -m_2\\bar{X}_n -m_1S^2_n + m_1m_2 \\right] \\\\\n    &= \\mathbb{E} \\left[ \\bar{X}_nS^2_n \\right] - m_2m_1 - m_1m_2 +m_1m_2 \\\\\n    &= \\mathbb{E} \\left[ \\bar{X}_nS^2_n \\right] - m_1m_2 \\\\\n    &= \\mathbb{E} \\left[ \\left( n^{-1} \\sum_{i=1}^n X_i \\right) \\left( n^{-1} \\sum_{i=1}^n X_i^2 \\right)  \\right] - m_1m_2 \\\\\n    &= \\mathbb{E} \\left[ n^{-2} \\sum_{i=1}^n X_i \\sum_{i=1}^n X_i^2 \\right] - m_1m_2 \\\\\n    &= \\mathbb{E} \\left[ n^{-2} \\sum_{i=1}^n \\sum_{j=1}^n X_i X_j^2 \\right] - m_1m_2 \\\\\n    &= n^{-2} \\sum_{i=1}^n \\sum_{j=1}^n \\mathbb{E} \\left[ X_j^3 \\right] -m_1m_2 \\\\\n    &= n^{-2} (n^2m_3) -m_1m_2 \\\\\n    &= m_3 - m_1m_2 \\\\\n\\end{aligned}\n$$\n\nThus we have the asymptotic distribution for the first and second moments (this can be verified by checking p.27 of the van der Vaart textbook). For expediency's sake, we'll refer to the variance-covariance matrix of the asymptotic distribution as $\\boldsymbol{\\Sigma^*}$ going forwards. \n\n$$\n\\sqrt{n} \\left( \\begin{pmatrix} \\bar{X}_n \\\\ \\bar{X^2}_n \\end{pmatrix} - \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix} \\right) \n\\overset{d}{\\to}\n\\mathcal{N}_2 \\left( \\mathbf{0},\\: \\begin{pmatrix} m_2 - m_1^2 & m_3 - m_1m_2 \\\\ m_3 - m_1m_2 & m_4 - m_2^2 \\end{pmatrix} \\right)\n$$\n\nNext we need to set up our function, which we do like so:\n\n$$\n\\begin{aligned}\n  \\phi(x, y) &= \\begin{pmatrix} x \\\\ y - x^2 \\end{pmatrix} \\\\\n  \\phi \\left( m_1, m_2 \\right) &= \\begin{pmatrix} \\bar{X}_n \\\\ S^2_n \\end{pmatrix} \\\\\n\\end{aligned}\n$$\n\nThe gradient of $\\phi$ is then:\n\n$$\n\\begin{aligned}\n  \\phi^\\prime_{m_1, m_2} \n    &= \\begin{pmatrix}\n         \\frac{\\partial}{\\partial m_1} \\phi_1(m_1, m_2) & \\frac{\\partial}{\\partial m_1} \\phi_2(m_1, m_2) \\\\\n         \\frac{\\partial}{\\partial m_2} \\phi_1(m_1, m_2) & \\frac{\\partial}{\\partial m_2} \\phi_2(m_1, m_2) \\\\\n       \\end{pmatrix} \\\\\n    &= \\begin{pmatrix}\n         1 & -2m_1 \\\\\n         0 & 1 \\\\\n       \\end{pmatrix}\n\\end{aligned}\n$$\n\nUsing the multivariate Delta Method, we have thus arrived at the joint asymptotic distribution of the sample mean & sample variance:\n\n$$\n\\sqrt{n} \\left( \\begin{pmatrix} \\bar{X}_n \\\\ S^2_n \\end{pmatrix} - \\begin{pmatrix} \\mu \\\\ \\sigma^2 \\end{pmatrix} \\right)\n\\overset{d}{\\to}\n\\mathcal{N}_2 \\left( \\mathbf{0},\\: \\phi^\\prime_{m_1, m_2} \\boldsymbol{\\Sigma^*} \\left( \\phi^\\prime_{m_1, m_2} \\right)^T \\right)\n$$\n\nIn deriving the above quantity, we've also figured out what conditions are necessary for $\\bar{X}_n$ and $S^2_n$ to be independent. We defined the covariance between the two estimators above:\n\n$$\n\\text{Cov} \\left( \\bar{X}_n, S^2_n \\right) = m_3 - m_1m_2\n$$\n\nKeeping in mind that we have centered our data,  we know that our raw moments are equivalent to central moments. For symmetric distributions, the expectations of the odd moments are all equal to zero, and thus the covariance as defined above will go to zero if the distribution function $F$ is symmetric. In order for us to strictly say that $\\bar{X}_n$ and $S^2_n$ are independent and not just uncorrelated, the two quantities must be jointly normally distributed, which we have shown above. Thus, as long as $F$ is symmetric, the sample mean and sample variance are independent. \n\n# U-Statistics\n\n## Exercise 12.3 - *Asymptotic Statistics*\n\nThis problem is also pulled from the van der Vaart book; the goal is simply to determine an appropriate kernel for a U-statistic for the third central moment. \n\n$$\n\\mu_3 = \\mathbb{E} \\left[ (X - \\mathbb{E}[X])^3 \\right]\n$$\n\nWe could attempt to define the kernel as we would when creating a U-statistic for the sample variance, as shown below:\n\n$$\nh(X_i, X_j) = (X_i - X_j)^3\n$$\n\nHowever, this kernel is no symmetric, which is a highly desirable property in U-statistic kernels i.e., $h(x_i, x_j) \\neq h(x_j, x_i)$ as shown below:\n\n$$\n\\begin{aligned}\n  h(X_i, X_j) \n    &= (X_i - X_j)^3 \\\\\n    &= X_i^3 -3X_jX_i^2 + 3X_j^2X_i - X_j^3 \\\\\n  h(X_j, X_i)\n    &= (X_j - X_i)^3 \\\\\n    &= X_j^3 -3X_iX_j^2 + 3X_i^2X_j - X_i^3 \\\\\n\\implies h(X_i, X_j) &= -h(X_j, X_i) \\\\\n\\end{aligned}\n$$\n\nHowever, any asymmetric U-statistic of degree $r$ may be symmetrized by averaging over all possible input permutations using the following technique:\n\n$$\ng(X_1, ..., X_r) = (r!)^{-1} \\sum_{i_1, ..., i_r} h(X_{i_1}, ..., X_{i_r})\n$$\n\nWe'll thus use the following symmetric kernel of degree 3 as detailed in [Locke & Spurrier (1978)](https://doi.org/10.2307/2335095):\n\n$$\nh(X_1, X_2, X_3) = \\frac{1}{3} \\sum_{i=1}^3 \\left( X_i - \\frac{1}{3} (X_1 + X_2 + X_3) \\right)^3\n$$\n\nThis in turn leads to the following U-statistic for $\\mu_k$:\n\n$$\n\\begin{aligned}\n  U_n \n    &= \\binom{n}{3}^{-1} \\sum_{i=1}^n \\sum_{i<j} \\sum_{j<k} h(X_i, X_j, X_k) \\\\\n    &= \\binom{n}{3}^{-1} \\sum_{i=1}^n \\sum_{i<j} \\sum_{j<k} \\frac{1}{3} \\left( \\left( X_i - \\frac{1}{3}(X_i + X_j + X_k) \\right)^3 + \\left( X_j - \\frac{1}{3}(X_i + X_j + X_k) \\right)^3 + \\left( X_k - \\frac{1}{3}(X_i + X_j + X_k) \\right)^3\\right)\n\\end{aligned}\n$$\n\nWe can show this empirically by testing it via simulation. Here we simulate $n = 1000$ realizations from $X_1, \\dots, X_n \\overset{\\small{\\text{IID}}}{\\sim} \\mathcal{N}(3,\\: 1)$. Since we're sampling from a normal distribution, which is symmetric about $\\mu$, the expectation of the third central moment is equal to zero. In the interest of showing how the U-statistic converges towards $\\mu_3$ as sample size grows, we'll compute the statistic for several values of $n$ and then visualize the results. First, we'll need to define a helper function for our kernel:\n\n\n::: {.cell hash='Large_Sample_Practice_cache/html/unnamed-chunk-2_ae8609efe4606a80bced6846def845b6'}\n\n```{.r .cell-code}\nh_ijk <- function(x_i, x_j, x_k) {\n  sum_ijk <- x_i + x_j + x_k\n  mean_ijk <- 1/3 * ((x_i - 1/3 * sum_ijk)^3 + (x_j - 1/3 * sum_ijk)^3 + (x_k - 1/3 * sum_ijk)^3)\n  return(mean_ijk)\n}\n```\n:::\n\n\nNow we iterate over sample sizes & save the results. We'll perform a few replications per sample size value for reproducibility reasons. \n\n\n::: {.cell hash='Large_Sample_Practice_cache/html/unnamed-chunk-3_429700984404f089e3a59d898d470c08'}\n\n```{.r .cell-code}\nn_vals <- purrr::map(c(5, 10, 15, 25, 40, 50, 75, 100, 150, 200, 250, 300, 400, 500), \\(x) rep(x, 3)) %>% \n          purrr::reduce(c)\nU_n_vals <- numeric(length = length(n_vals)) \nfor (n in seq(n_vals)) {\n  set.seed(n)\n  sample_n <- n_vals[n]\n  mu <- 3\n  sigma <- 1\n  x <- rnorm(sample_n, mean = mu, sd = sigma)\n  i <- 1\n  U_sum <- 0\n  while (i <= sample_n) {\n    j <- i + 1\n    while (j <= sample_n) {\n      k <- j + 1\n      while (k <= sample_n) {\n        h_val <- h_ijk(x_i = x[i], x_j = x[j], x_k = x[k])\n        U_sum <- U_sum + h_val\n        k <- k + 1\n      }\n      j <- j + 1\n    }\n    i <- i + 1\n  }\n  U_n <- choose(sample_n, 3)^(-1) * U_sum\n  U_n_vals[n] <- U_n\n}\n```\n:::\n\n\nWe can see that the U-statistic grows very close to the true value of zero as $n$ increases. The drawback of this approach is the computational cost; even for this relatively small sample size the runtime is several minutes, and grows on the order of $O(n^3)$. \n\n\n::: {.cell hash='Large_Sample_Practice_cache/html/unnamed-chunk-4_629cd8bf0a52270e980da8ee932526b7'}\n\n```{.r .cell-code}\ndata.frame(U = U_n_vals, \n           N = n_vals) %>% \n  ggplot(aes(x = N, y = abs(U))) + \n  geom_point() + \n  geom_smooth(color = \"forestgreen\", se = FALSE) + \n  labs(x = latex2exp::TeX(r\"($\\textit{n}$)\"), \n       y = latex2exp::TeX(r\"($|\\textit{U_n} - \\theta|$)\")) + \n  theme_classic(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](Large_Sample_Practice_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Example 1.3.2 - *U-Statistics: Theory and Practice*\n\nThis problem is a slight modification of one of the examples from [Lee (1990)](https://doi.org/10.1201/9780203734520) seen on page 13. Instead of deriving the asymptotic distribution of the U-statistic for the sample variance, we'll do so for the second raw moment $m_2$. \n\n$$\n\\theta = E[X^2] = m_2\n$$\n\nFrom the definition of variance, we know that $m_2 = \\mu_2 + m_1^2$. Our kernel, which is symmetric, will thus be a combination of the kernels for the sample variance and for the expectation squared:\n\n$$\nh(X_1, X_2) = \\frac{(X_1 - X_2)^2}{2} + X_1X_2\n$$\n\nhaving expectation:\n\n$$\n\\begin{aligned}\n  \\mathbb{E}[h(X_1, X_2)] \n    &= \\mathbb{E} \\left[ \\frac{(X_1 - X_2)^2}{2} + X_1X_2 \\right] \\\\\n    &= \\frac{1}{2}(\\mathbb{E} \\left[ (X_1 - X_2)^2 \\right]) - \\mathbb{E}[X_1X_2] \\\\\n    &= \\frac{1}{2}(m_2 - 2\\mu^2 + m_2) + \\mu^2 \\\\\n    &= m_2 \\\\\n\\end{aligned}\n$$\n\nThis leads us to the following U-statistic:\n\n$$\nU_n = \\binom{n}{2}^{-1} \\sum_{i=1}^n \\sum_{i<j} \\frac{(X_i - X_j)^2}{2} + X_iX_j\n$$\n\nWe define the following:\n\n$$\n\\begin{aligned}\n  h_1(X_1, X_2) &= \\mathbb{E}[h(X_1, X_2) | X_1] \\\\ \n  h_1^c(X_1, X_2) \n    &= \\mathbb{E}[h(X_1, X_2) | X_1] - \\theta \\\\ \n    &= \\mathbb{E}[h(X_1, X_2) | X_1] - m_2 \\\\ \n  \\zeta_1 \n    &= \\mathbb{E} \\left[ (h_1^c(X_1, X_2))^2 \\right] \\\\\n    &= \\mathbb{E} \\left[ (\\mathbb{E}[h(X_1, X_2) | X_1] - m_2)^2 \\right] \\\\\n    &= \\text{Var} \\left( \\mathbb{E}[h(X_1, X_2) | X_1] \\right) \\\\\n  \\text{Var}(U_n) &\\overset{p}{\\to} \\frac{r^2}{n} \\zeta_1 \\\\\n  \\sqrt{n}(U_n - m_2) &\\overset{d}{\\to} \\mathcal{N}(0,\\: r^2\\zeta_1) \\\\\n\\end{aligned}\n$$\n\nWe begin with the expectation of the kernel conditional on $X_1$:\n\n$$\n\\begin{aligned}\n  h_1(X_1, X_2) \n    &= \\mathbb{E} \\left[ \\frac{(X_1 - X_2)^2}{2} + X_1X_2 | X_1 \\right] \\\\\n    &= \\frac{1}{2} \\mathbb{E}[X_1^2 -2X_1X_2 + X_2^2 | X_1] + \\mathbb{E}[X_1X_2 | X_1] \\\\\n    &= \\frac{1}{2}(X_1^2 - 2X_1\\mu + m_2) + X_1\\mu \\\\\n    &= \\frac{X_1^2 + m_2}{2}\n\\end{aligned}\n$$\n\nFrom which we can derive $\\zeta_1$:\n\n$$\n\\begin{aligned}\n  \\zeta_1\n    &= \\text{Var} \\left( \\frac{X_1^2 + m_2}{2} \\right) \\\\\n    &= \\frac{1}{4} \\text{Var}(X_1^2 + m_2) \\\\\n    &= \\frac{1}{4} \\left( \\text{Var}(X_1^2) + \\text{Var}(m_2) \\right) \\\\\n    &= \\frac{1}{4} \\text{Var}(X_1^2 \\\\\n    &= \\frac{1}{4} \\mathbb{E} \\left[ (X_1^2 - m_2)^2 \\right] \\\\\n    &= \\frac{1}{4}(m_4 - 2m_2m_2 + m_2^2) \\\\\n    &= \\frac{m_4 - m_2^2}{4} \\\\\n\\end{aligned}\n$$\n\nFinally, since our kernel has degree $r = 2$:\n\n$$\n\\sqrt{n}(U_n - m_2) \\overset{d}{\\to} \\mathcal{N}(0,\\: m_4 - m_2^2)\n$$\n\nWe can confirm this using simulation as well. First, we define a new function that computes our kernel:\n\n\n::: {.cell hash='Large_Sample_Practice_cache/html/unnamed-chunk-5_f78bf691253cf4469ffdee8159590929'}\n\n```{.r .cell-code}\nh_ij <- function(x_i, x_j) {\n  m2_ij <- ((x_i - x_j)^2) / 2 + x_i * x_j\n  return(m2_ij)\n}\n```\n:::\n\n\nWe'll simulate observations from the following distribution. From the definition of variance, we know that $m_2 = \\text{Var}(X) + \\left( \\mathbb{E}(X) \\right)^2$, which in our case is: $m_2 = 1 + 2^2 = 5$. This is the value against which we'll compare our U-statistics, whose expectation is $m_2$. \n\n$$\nX_1, \\dots, X_n \\overset{\\small{\\text{IID}}}{\\sim} \\mathcal{N}(2,\\: 1)\n$$\n\nNow we iterate over a range of possible values for $n$, running the simulation 3x per value, and recording the U-statistics that we estimate. \n\n\n::: {.cell hash='Large_Sample_Practice_cache/html/unnamed-chunk-6_f77cb2e5d8b62d6d4e69d9c8b9fdc32a'}\n\n```{.r .cell-code}\nn_vals <- purrr::map(c(5, 10, 15, 25, 40, 50, 75, 100, 150, 200, 250, 300, 400, 500, 750, 1000, 1250), \\(x) rep(x, 3)) %>% \n          purrr::reduce(c)\nU_n_vals <- numeric(length = length(n_vals)) \nfor (n in seq(n_vals)) {\n  set.seed(n)\n  sample_n <- n_vals[n]\n  mu <- 2\n  sigma <- 1\n  x <- rnorm(sample_n, mean = mu, sd = sigma)\n  i <- 1\n  U_sum <- 0\n  while (i <= sample_n) {\n    j <- i + 1\n    while (j <= sample_n) {\n      h_val <- h_ij(x_i = x[i], x_j = x[j])\n      U_sum <- U_sum + h_val\n      j <- j + 1\n    }\n    i <- i + 1\n  }\n  U_n <- choose(sample_n, 2)^(-1) * U_sum\n  U_n_vals[n] <- U_n\n}\n```\n:::\n\n\nPlotting the results, we see a monotonically decreasing trend of the absolute error of $U_n$ when compared to the true value $m_2 = 5$. \n\n\n::: {.cell hash='Large_Sample_Practice_cache/html/unnamed-chunk-7_fca8e52fcca476754fd11ff7117096f2'}\n\n```{.r .cell-code}\ndata.frame(U = U_n_vals, \n           N = n_vals) %>% \n  ggplot(aes(x = N, y = abs(U - 5))) + \n  geom_point() + \n  geom_smooth(color = \"forestgreen\", se = FALSE) + \n  labs(x = latex2exp::TeX(r\"($\\textit{n}$)\"), \n       y = latex2exp::TeX(r\"($|\\textit{U_n} - \\theta|$)\")) + \n  theme_classic(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](Large_Sample_Practice_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n# Influence Functions \n\n## Example 3.2 - *Zepeda-Tello, R. et al*\n\nThis example is pulled from [The delta-method and influence function in medical statistics: A reproducible tutorial](https://doi.org/10.48550/arXiv.2206.15310), a preprint from June 2022. Section 3 includes several examples of asymptotic distributions of estimators derived using the functional delta method. Example 3.2 shows the derivation of the asymptotic distribution of the sample mean using the influence function instead of the Central Limit Theorem; they use a discrete distribution for simplicity, but here we'll use a continuous distribution instead. \n\nWe assume that observations are generated from the following distribution with mean $\\mu$ and variance $\\sigma^2$:\n\n$$\nX_1, \\dots, X_n \\overset{\\small{\\text{IID}}}{\\sim} F\n$$\n\nThe estimator can be formulated as a functional, with $\\phi(\\cdot)$ simply being the identity function:\n\n$$\n\\begin{aligned}\n  \\Psi \n    &= \\phi(\\mathbb{P}_X) \\\\\n    &= \\phi(\\theta) \\\\\n    &= \\mu \\\\\n  \\widehat{\\Psi}_n\n    &= \\phi(\\widehat{P}_X) \\\\\n    &= \\phi(\\hat{\\theta}_n) \\\\\n    &= \\bar{X}_n \\\\ \n\\end{aligned}\n$$\n\nVia a Taylor expansion, we have:\n\n$$\n\\begin{aligned}\n\\widehat{\\Psi}_n &\\approx \\Psi + IF(X) \\\\\n\\implies IF(X) &= \\bar{X}_n - \\mu + o_p(1) \\\\\n\\end{aligned}\n$$\n\nThe asymptotic distribution is as follows; we note that the expectation of the influence function is always equal to zero, and thus its variance is equal to its second raw moment. \n\n$$\n\\phi(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} \\mathcal{N} \\left( 0,\\: \\text{Var} \\left( IF(X) \\right) \\right)\n$$\n\nWe derive the variance of the influence function, and obtain the same result as we would have in using the CLT:\n\n$$\n\\begin{aligned}\n  \\text{Var} \\left( IF(X) \\right)\n    &= \\text{Var} \\left( \\bar{X}_n - \\mu \\right) \\\\\n    &= \\text{Var} \\left( n^{-1} \\sum_{i=1}^n X_i \\right) \\\\\n    &= n^{-2} \\sum_{i=1}^n \\text{Var}(X_i) \\\\\n    &= n^{-1}\\sigma^2 \\\\\n\\end{aligned}\n$$\n\nThus the asymptotic distribution of the sample mean is:\n\n$$\n\\sqrt{n} \\left( \\bar{X}_n - \\mu \\right) \\overset{d}{\\to} \\mathcal{N}(0,\\: \\sigma^2)\n$$\n\n## Exercise 12.7 - *Asymptotic Statistics*\n\nI'm modifying this question slightly; the original asks for the asymptotic distribution of the U-statistic for $\\mu^2$, and in addition we'll derive its joint distribution with the U-statistic for $m_2$ that we found previously using influence functions. \n\nFirst, we define the given quantities:\n\n$$\n\\begin{aligned}\n  X_1, \\dots, X_n &\\overset{\\small{\\text{IID}}}{\\sim} F \\\\\n  \\mathbb{E} \\left[ X^2_1 \\right] &< \\infty \\\\\n\\end{aligned}\n$$\n\nWe define the following symmetric kernel to use in the U-statistic for $\\mu^2$:\n\n$$\nh(X_1, X_2) = X_1X_2\n$$\n\nThe expectation of that kernel is given by:\n\n$$\n\\begin{aligned}\n  \\mathbb{E} \\left[ h(X_1, X_2) \\right]\n    &= \\mathbb{E}[X_1X_2] \\\\\n    &= \\mathbb{E}[X_1]\\mathbb{E}[X_2] \\\\\n    &= \\mu^2 \\\\\n\\end{aligned}\n$$\n\nThe U-statistic is thus:\n\n$$\nU_n = \\binom{n}{2}^{-1} \\sum_{i=1}^n \\sum_{i<j} X_iX_j\n$$\n\nNext we derive the asymptotic distribution of the U-statistic:\n\n$$\n\\begin{aligned}\n  h_1(X_1, X_2)\n    &= \\mathbb{E} \\left[ h(X_1, X_2) | X_1 \\right] \\\\\n    &= \\mathbb{E}[X_1X_2 | X_1] \\\\\n    &= X_1\\mu \\\\\n  \\implies \\zeta_1 \n    &= \\mathbb{E} \\left[ (h_1^c(X_1, X_2))^2 \\right] \\\\\n    &= \\mathbb{E} \\left[(h_1(X_1, X_2) - \\mu^2)^2 \\right] \\\\\n    &= \\text{Var}(h_1(X_1, X_2)) \\\\\n    &= \\text{Var}(X_1\\mu) \\\\\n    &= \\mu^2\\text{Var}(X_1) \\\\\n    &= \\mu^2\\sigma^2 \\\\\n  \\implies \\sqrt{n}(U_n - \\mu^2) &\\overset{d}{\\to} \\mathcal{N}(0,\\: 4\\mu^2\\sigma^2)\n\\end{aligned}\n$$\n\nThe influence function of a U-statistic is given by:\n\n$$\nIF_U(X) = r h_1^c(X_1, \\dots, X_r)\n$$\n\nThus the influence function for our U-statistic is:\n\n$$\nIF_U(X) = 2\\mu(X_1 - \\mu)\n$$\n\nRemembering the U-statistic we derived earlier for $m_2$, which we'll now refer to as $U^*$ in order to distinguish it from the other statistic:\n\n$$\nU_n^* = \\binom{n}{2}^{-1} \\sum_{i=1}^n \\sum_{i<j} \\frac{(X_i - X_j)^2}{2} + X_iX_j\n$$\n\nWe now define its influence function as:\n\n$$\n\\begin{aligned}\n  IF_{U^*}(X) \n    &= 2 \\left( \\frac{X_1^2 + m_2}{2} - m_2 \\right) \\\\\n    &= X_1^2 - m_2 \\\\\n\\end{aligned}\n$$\n\nThus, the joint distribution of the two U-statistics is:\n\n$$\n\\sqrt{n} \\begin{pmatrix} U_n - \\mu^2 \\\\ U_n^* - m_2 \\end{pmatrix} \n\\overset{d}{\\to} \n\\boldsymbol{\\mathcal{N}}_2 \\left(\\mathbf{0},\\: \n  \\begin{pmatrix} \n    \\text{Var} \\left( IF_U \\right) & \\text{Cov} \\left( IF_U, IF_{U^*} \\right) \\\\\n    \\text{Cov} \\left( IF_U, IF_{U^*} \\right) & \\text{Var} \\left( IF_{U^*} \\right) \\\\\n  \\end{pmatrix} \\right)\n$$\n\nThe variance of an influence function is equal to its second raw moment, as its first raw moment is always equal to zero. Ergo, the variances of the two U-statistics are as follows (they should match the asymptotic variances from earlier): \n\n$$\n\\begin{aligned}\n  \\text{Var} \\left( IF_U(X) \\right)\n    &= \\mathbb{E} \\left[ IF_U(X)^2 \\right] \\\\\n    &= \\mathbb{E} \\left[ (2\\mu(X_1 - \\mu))^2 \\right] \\\\\n    &= \\mathbb{E} \\left[ 4\\mu^2(X_1 - \\mu)^2 \\right] \\\\\n    &= 4\\mu^2\\sigma^2 \\\\\n  \\implies \\text{Var} \\left( IF_{U^*}(X) \\right)\n    &= \\mathbb{E} \\left[ (X_1^2 - m_2)^2 \\right] \\\\\n    &= \\text{Var} \\left( X_1^2 \\right) \\\\\n    &= \\mathbb{E} \\left[ (X_1^2)^2 \\right] - \\left( \\mathbb{E} \\left[ X_1^2 \\right] \\right)^2 \\\\\n    &= m_4 - m_2^2 \\\\\n\\end{aligned}\n$$\n\nLastly, the covariance term:\n\n$$\n\\begin{aligned}\n  \\text{Cov} \\left( IF_U, IF_{U^*} \\right)\n    &= \\mathbb{E} \\left[ IF_U IF_{U^*} \\right] - \\mathbb{E}[IF_U]\\mathbb{E}[IF_{U^*}] \\\\\n    &= \\mathbb{E} \\left[ 2\\mu(X_1 - \\mu)(X_1 - m_2) \\right] - \\mathbb{E}[2\\mu(X_1 - \\mu)] \\mathbb{E}[X_1 - m_2] \\\\\n    &= \\mathbb{E}[2\\mu(X_1^2 - X_1m_2 - \\mu X_1 + \\mu m_2)] - 2\\mu \\left( \\mathbb{E}[X_1] - \\mu \\right)(\\mu - m_2) \\\\\n    &= 2\\mu(m_2 - \\mu m_2 - \\mu^2 + \\mu m_2) \\\\\n    &= 2\\mu(m_2 - \\mu^2) \\\\\n\\end{aligned}\n$$\n\nThus we have arrived at the join asymptotic distribution:\n\n$$\n\\sqrt{n} \\begin{pmatrix} U_n - \\mu^2 \\\\ U_n^* - m_2 \\end{pmatrix} \n\\overset{d}{\\to} \n\\boldsymbol{\\mathcal{N}}_2 \\left(\\mathbf{0},\\: \n  \\begin{pmatrix} \n    4\\mu^2\\sigma^2 & 2\\mu(m_2 - \\mu^2) \\\\\n    2\\mu(m_2 - \\mu^2) & m_4 - m_2^2 \\\\\n  \\end{pmatrix} \\right)\n$$\n\n# Session Info \n\n\n::: {.cell hash='Large_Sample_Practice_cache/html/unnamed-chunk-8_118dddde3c65bdb4fd916e93506ca8a5'}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-05-11\n pandoc   2.19.2 @ /usr/local/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.2.0)\n cli           3.3.0   2022-04-25 [1] CRAN (R 4.2.0)\n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.2.1)\n colorspace    2.0-3   2022-02-21 [1] CRAN (R 4.2.0)\n DBI           1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n digest        0.6.29  2021-12-01 [1] CRAN (R 4.2.0)\n dplyr       * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n evaluate      0.16    2022-08-09 [1] CRAN (R 4.2.0)\n fansi         1.0.3   2022-03-24 [1] CRAN (R 4.2.0)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.0)\n ggplot2     * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.2.0)\n htmltools     0.5.3   2022-07-18 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.0)\n jsonlite      1.8.0   2022-02-22 [1] CRAN (R 4.2.0)\n knitr         1.40    2022-08-24 [1] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.2.0)\n latex2exp     0.9.4   2022-03-02 [1] CRAN (R 4.2.0)\n lattice       0.20-45 2021-09-22 [1] CRAN (R 4.2.1)\n lifecycle     1.0.1   2021-09-24 [1] CRAN (R 4.2.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n Matrix        1.4-1   2022-03-23 [1] CRAN (R 4.2.1)\n mgcv          1.8-40  2022-03-29 [1] CRAN (R 4.2.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.2.0)\n nlme          3.1-159 2022-08-09 [1] CRAN (R 4.2.0)\n pillar        1.8.1   2022-08-19 [1] CRAN (R 4.2.0)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.0)\n purrr         0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.2.0)\n rlang         1.0.4   2022-07-12 [1] CRAN (R 4.2.0)\n rmarkdown     2.16    2022-08-24 [1] CRAN (R 4.2.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr       1.4.1   2022-08-20 [1] CRAN (R 4.2.0)\n tibble        3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyselect    1.1.2   2022-02-21 [1] CRAN (R 4.2.0)\n utf8          1.2.2   2021-07-24 [1] CRAN (R 4.2.0)\n vctrs         0.4.1   2022-04-13 [1] CRAN (R 4.2.0)\n withr         2.5.0   2022-03-03 [1] CRAN (R 4.2.0)\n xfun          0.32    2022-08-10 [1] CRAN (R 4.2.0)\n yaml          2.3.5   2022-02-21 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}